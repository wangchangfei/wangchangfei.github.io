<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark：11、Spark SQL 数据源]]></title>
    <url>%2F2018%2F02%2F18%2Fspark-11-spark-sql-datasource.html</url>
    <content type="text"><![CDATA[通用加载/保存方法1) 手动指定选项 Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。 Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。 scala&gt; val df = spark.read.load(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;) df: org.apache.spark.sql.DataFrame = [age: bigint, name: string] scala&gt; df.select(&quot;name&quot;).write.save(&quot;names.parquet&quot;) 当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。 可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。 scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;hdfs://hadoop001:9000/people.json&quot;) peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string] scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;) scala&gt; 除此之外，可以直接运行SQL在文件上： val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&quot;) sqlDF.show() 2) 文件保存选项 可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表： Scala/JavaAny Language Meaning SaveMode.ErrorIfExists(default) “error”(default) 如果文件存在，则报错 SaveMode.Append “append” 追加 SaveMode.Overwrite “overwrite” 覆写 SaveMode.Ignore “ignore” 数据存在，则忽略 Parquet文件1) Parquet读写 Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 // Encoders for most common types are automatically provided by importing spark.implicits._ import spark.implicits._ val peopleDF = spark.read.json(&quot;examples/src/main/resources/people.json&quot;) // DataFrames can be saved as Parquet files, maintaining the schema information peopleDF.write.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;) // Read in the parquet file created above // Parquet files are self-describing so the schema is preserved // The result of loading a Parquet file is also a DataFrame val parquetFileDF = spark.read.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;) // Parquet files can also be used to create a temporary view and then used in SQL statements parquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;) val namesDF = spark.sql(&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;) namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show() // +------------+ // | value| // +------------+ // |Name: Justin| // +------------+ 2) 解析分区信息 对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构： path └── to └── table ├── gender=male │ ├── ... │ │ │ ├── country=US │ │ └── data.parquet │ ├── country=CN │ │ └── data.parquet │ └── ... └── gender=female ├── ... │ ├── country=US │ └── data.parquet ├── country=CN │ └── data.parquet └── ... 通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下： root |-- name: string (nullable = true) |-- age: long (nullable = true) |-- gender: string (nullable = true) |-- country: string (nullable = true) 需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为： spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。 如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。 3) Schema合并 像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0 开始默认关闭了该功能。可以通过下面两种方式开启该功能： 当数据源为Parquet文件时，将数据源选项mergeSchema设置为true。 设置全局SQL选项spark.sql.parquet.mergeSchema为true。 // sqlContext from the previous example is used in this example. // This is used to implicitly convert an RDD to a DataFrame. import spark.implicits._ // Create a simple DataFrame, stored into a partition directory val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;) df1.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=1&quot;) // Create another DataFrame in a new partition directory, // adding a new column and dropping an existing column val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;) df2.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=2&quot;) // Read the partitioned table val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;hdfs://hadoop001:9000/data/test_table&quot;) df3.printSchema() // The final schema consists of all 3 columns in the Parquet files together // with the partitioning column appeared in the partition directory paths. // root // |-- single: int (nullable = true) // |-- double: int (nullable = true) // |-- triple: int (nullable = true) // |-- key : int (nullable = true) Hive数据源Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。 若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。 import java.io.File import org.apache.spark.sql.Row import org.apache.spark.sql.SparkSession case class Record(key: Int, value: String) // warehouseLocation points to the default location for managed databases and tables val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath val spark = SparkSession .builder() .appName(&quot;Spark Hive Example&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .enableHiveSupport() .getOrCreate() import spark.implicits._ import spark.sql sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;) sql(&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;) // Queries are expressed in HiveQL sql(&quot;SELECT * FROM src&quot;).show() // +---+-------+ // |key| value| // +---+-------+ // |238|val_238| // | 86| val_86| // |311|val_311| // ... // Aggregation queries are also supported. sql(&quot;SELECT COUNT(*) FROM src&quot;).show() // +--------+ // |count(1)| // +--------+ // | 500 | // +--------+ // The results of SQL queries are themselves DataFrames and support all normal functions. val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;) // The items in DataFrames are of type Row, which allows you to access each column by ordinal. val stringsDS = sqlDF.map { case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot; } stringsDS.show() // +--------------------+ // | value| // +--------------------+ // |Key: 0, Value: val_0| // |Key: 0, Value: val_0| // |Key: 0, Value: val_0| // ... // You can also use DataFrames to create temporary views within a SparkSession. val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;))) recordsDF.createOrReplaceTempView(&quot;records&quot;) // Queries can then join DataFrame data with data stored in Hive. sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show() // +---+------+---+------+ // |key| value|key| value| // +---+------+---+------+ // | 2| val_2| 2| val_2| // | 4| val_4| 4| val_4| // | 5| val_5| 5| val_5| // ... 1) 内嵌Hive应用 如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf : spark.sql.warehouse.dir= 注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。 2) 外部Hive应用 如果想连接外部已经部署好的Hive，需要通过以下几个步骤。 a 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。 b 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。 $ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar JSON数据集Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。 {&quot;name&quot;:&quot;Michael&quot;} {&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30} {&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19} // Primitive types (Int, String, etc) and Product types (case classes) encoders are // supported by importing this when creating a Dataset. import spark.implicits._ // A JSON dataset is pointed to by path. // The path can be either a single text file or a directory storing text files val path = &quot;examples/src/main/resources/people.json&quot; val peopleDF = spark.read.json(path) // The inferred schema can be visualized using the printSchema() method peopleDF.printSchema() // root // |-- age: long (nullable = true) // |-- name: string (nullable = true) // Creates a temporary view using the DataFrame peopleDF.createOrReplaceTempView(&quot;people&quot;) // SQL statements can be run by using the sql methods provided by spark val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;) teenagerNamesDF.show() // +------+ // | name| // +------+ // |Justin| // +------+ // Alternatively, a DataFrame can be created for a JSON dataset represented by // a Dataset[String] storing one JSON object per string val otherPeopleDataset = spark.createDataset( &quot;&quot;&quot;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot; :: Nil) val otherPeople = spark.read.json(otherPeopleDataset) otherPeople.show() // +---------------+----+ // | address|name| // +---------------+----+ // |[Columbus,Ohio]| Yin| // +---------------+----+ JDBCSpark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。 注意，需要将相关的数据库驱动放到spark的类路径下。 $ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar // Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods // Loading data from a JDBC source val jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot; rddtable&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).load() val connectionProperties = new Properties() connectionProperties.put(&quot;user&quot;, &quot;root&quot;) connectionProperties.put(&quot;password&quot;, &quot;hive&quot;) val jdbcDF2 = spark.read .jdbc(&quot;jdbc:mysql://hadoop001:3306/rdd&quot;, &quot;rddtable&quot;, connectionProperties) // Saving data to a JDBC source jdbcDF.write .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;) .option(&quot;dbtable&quot;, &quot;rddtable2&quot;) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;hive&quot;) .save() jdbcDF2.write .jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties) // Specifying create table column data types on write jdbcDF.write .option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;) .jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：10、Spark SQL 解析]]></title>
    <url>%2F2018%2F02%2F16%2Fspark-10-spark-sql-analysis.html</url>
    <content type="text"><![CDATA[SparkSession的概念在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(&quot;Spark SQL basic example&quot;) .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) .getOrCreate() // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ SparkSession.builder 用于创建一个SparkSession。 import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。 如果需要Hive支持，则需要以下创建语句： import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(&quot;Spark SQL basic example&quot;) .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) .enableHiveSupport() .getOrCreate() // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ DataFrames的创建在Spark SQL中SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。 1) 从Spark数据源进行创建： val df = spark.read.json(&quot;examples/src/main/resources/people.json&quot;) // Displays the content of the DataFrame to stdout df.show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ 2) 将RDD转换为DataFrame： /** Michael Andy, 30 Justin, 19 **/ scala&gt; val peopleRdd = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;) peopleRdd: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[18] at textFile at &lt;console&gt;:24 scala&gt; val peopleDF3 = peopleRdd.map(_.split(&quot;,&quot;)).filter(arr =&gt; arr.length == 2).map(paras =&gt; (paras(0),paras(1).trim().toInt)).toDF(&quot;name&quot;,&quot;age&quot;) peopleDF3: org.apache.spark.sql.DataFrame = [name: string, age: int] scala&gt; peopleDF3.show() +-------+---+ | name|age| +-------+---+ |Michael| 29| | Andy| 30| | Justin| 19| +-------+---+ DataFrame的常用操作1) DSL风格语法 // This import is needed to use the $-notation import spark.implicits._ // Print the schema in a tree format df.printSchema() // root // |-- age: long (nullable = true) // |-- name: string (nullable = true) // Select only the &quot;name&quot; column df.select(&quot;name&quot;).show() // +-------+ // | name| // +-------+ // |Michael| // |Andy| // | Justin| // +-------+ // Select everybody, but increment the age by 1 df.select($&quot;name&quot;, $&quot;age&quot; + 1).show() // +-------+---------+ // | name|(age + 1)| // +-------+---------+ // |Michael| null| // | Andy| 31| // | Justin| 20| // +-------+---------+ // Select people older than 21 df.filter($&quot;age&quot; &gt; 21).show() // +---+----+ // |age|name| // +---+----+ // | 30|Andy| // +---+----+ // Count people by age df.groupBy(&quot;age&quot;).count().show() // +----+-----+ // | age|count| // +----+-----+ // | 19| 1| // |null| 1| // | 30| 1| // +----+-----+ 2) SQL风格语法 // Register the DataFrame as a SQL temporary view df.createOrReplaceTempView(&quot;people&quot;) val sqlDF = spark.sql(&quot;SELECT * FROM people&quot;) sqlDF.show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ // Register the DataFrame as a global temporary view df.createGlobalTempView(&quot;people&quot;) // Global temporary view is tied to a system preserved database `global_temp` spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ // Global temporary view is cross-session spark.newSession().sql(&quot;SELECT * FROM global_temp.people&quot;).show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ 临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people。 DataSet的创建Dataset是具有强类型的数据集合，需要提供对应的类型信息。 // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit, // you can use custom classes that implement the Product interface case class Person(name: String, age: Long) // Encoders are created for case classes val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS() caseClassDS.show() // +----+---+ // |name|age| // +----+---+ // |Andy| 32| // +----+---+ // Encoders for most common types are automatically provided by importing spark.implicits._ val primitiveDS = Seq(1, 2, 3).toDS() primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4) // DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name val path = &quot;hdfs://hadoop001:9000/people.json&quot; val peopleDS = spark.read.json(path).as[Person] peopleDS.show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ DataSet与RDD的转换Spark SQL支持通过两种方式将存在的RDD转换为Dataset，转换的过程中需要让Dataset获取RDD中的Schema信息，主要有两种方式，一种是通过反射来获取RDD中的Schema信息。这种方式适合于列名已知的情况下。第二种是通过编程接口的方式将Schema信息应用于RDD，这种方式可以处理那种在运行时才能知道列的方式。 1) 通过反射获取Schema SparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。 // For implicit conversions from RDDs to DataFrames import spark.implicits._ // Create an RDD of Person objects from a text file, convert it to a Dataframe val peopleDF = val peopleDF = spark.sparkContext.textFile(&quot;hdfs://hadoop001:9000/people2.txt&quot;).map(_.split(&quot;,&quot;)).filter(arr =&gt; arr.length == 2).map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)).toDF() // Register the DataFrame as a temporary view peopleDF.createOrReplaceTempView(&quot;people&quot;) // SQL statements can be run by using the sql methods provided by Spark val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;) // The columns of a row in the result can be accessed by field index ROW object teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show() // +------------+ // | value| // +------------+ // |Name: Justin| // +------------+ // or by field name teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;)).show() // +------------+ // | value| // +------------+ // |Name: Justin| // +------------+ // No pre-defined encoders for Dataset[Map[K,V]], define explicitly implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]] // Primitive types and case classes can be also defined as // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder() // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T] teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect() // Array(Map(&quot;name&quot; -&gt; &quot;Justin&quot;, &quot;age&quot; -&gt; 19)) 2) 通过编程设置Schema 如果case类不能够提前定义，可以通过下面三个步骤定义一个DataFrame 创建一个多行结构的RDD; 创建用StructType来表示的行结构信息。 通过SparkSession提供的createDataFrame方法来应用Schema。 import org.apache.spark.sql.types._ // Create an RDD val peopleRDD = spark.sparkContext.textFile(&quot;hdfs://hadoop001:9000/people2.txt&quot;) // The schema is encoded in a string,应该是动态通过程序生成的 val schemaString = &quot;name age&quot; // Generate the schema based on the string of schema Array[StructFiled] val fields = schemaString.split(&quot; &quot;).map(fieldName =&gt; StructField(fieldName, StringType, nullable = true)) val filed = schemaString.split(&quot; &quot;).map(filename=&gt; filename match{ case &quot;name&quot; =&gt; StructField(filename,StringType,nullable = true); case &quot;age&quot;=&gt;StructField(filename, IntegerType,nullable = true)}) val schema = StructType(fields) // Convert records of the RDD (people) to Rows import org.apache.spark.sql._ val rowRDD = peopleRDD.map(_.split(&quot;,&quot;)).filter(arr =&gt; arr.length == 2).map(attributes =&gt; Row(attributes(0), attributes(1).trim)) // Apply the schema to the RDD val peopleDF = spark.createDataFrame(rowRDD, schema) // Creates a temporary view using the DataFrame peopleDF.createOrReplaceTempView(&quot;people&quot;) // SQL can be run over a temporary view created using DataFrames val results = spark.sql(&quot;SELECT name FROM people&quot;) // The results of SQL queries are DataFrames and support all the normal RDD operations // The columns of a row in the result can be accessed by field index or by field name results.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show() // +-------------+ // | value| // +-------------+ // |Name: Michael| // | Name: Andy| // | Name: Justin| // +-------------+ 类型之间的转换总结RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换。 1) DataFrame/Dataset转RDD： val rdd1=testDF.rdd val rdd2=testDS.rdd 2) RDD转DataFrame： import spark.implicits._ val testDF = rdd.map {line=&gt; (line._1,line._2) }.toDF(&quot;col1&quot;,&quot;col2&quot;) 一般用元组把一行的数据写在一起，然后在toDF中指定字段名。 3) RDD转Dataset： import spark.implicits._ case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型 val testDS = rdd.map {line=&gt; Coltest(line._1,line._2) }.toDS 可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可 4）Dataset转DataFrame： 这个也很简单，因为只是把case class封装成Row： import spark.implicits._ val testDF = testDS.toDF DataFrame转Dataset： import spark.implicits._ case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型 val testDS = testDF.as[Coltest] 这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。 在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。 UDF函数1) UDF函数的创建 scala&gt; val df = spark.read.json(&quot;hdfs://hadoop001:9000/people.json&quot;) df: org.apache.spark.sql.DataFrame = [age: bigint, name: string] scala&gt; df.show() +----+-------+ | age| name| +----+-------+ |null|Michael| | 30| Andy| | 19| Justin| +----+-------+ scala&gt; spark.udf.register(&quot;addName&quot;, (x:String)=&gt; &quot;Name:&quot;+x) res5: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType))) scala&gt; df.createOrReplaceTempView(&quot;people&quot;) scala&gt; spark.sql(&quot;Select addName(name), age from people&quot;).show() +-----------------+----+ |UDF:addName(name)| age| +-----------------+----+ | Name:Michael|null| | Name:Andy| 30| | Name:Justin| 19| +-----------------+----+ 2) UDAF函数的创建 强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。 1) 弱类型用户自定义聚合函数 通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。 import org.apache.spark.sql.expressions.MutableAggregationBuffer import org.apache.spark.sql.expressions.UserDefinedAggregateFunction import org.apache.spark.sql.types._ import org.apache.spark.sql.Row import org.apache.spark.sql.SparkSession object MyAverage extends UserDefinedAggregateFunction { // 聚合函数输入参数的数据类型 def inputSchema: StructType = StructType(StructField(&quot;inputColumn&quot;, LongType) :: Nil) // 聚合缓冲区中值得数据类型 def bufferSchema: StructType = { StructType(StructField(&quot;sum&quot;, LongType) :: StructField(&quot;count&quot;, LongType) :: Nil) } // 返回值的数据类型 def dataType: DataType = DoubleType // 对于相同的输入是否一直返回相同的输出。 def deterministic: Boolean = true // 初始化 def initialize(buffer: MutableAggregationBuffer): Unit = { // 存工资的总额 buffer(0) = 0L // 存工资的个数 buffer(1) = 0L } // 相同Execute间的数据合并。 def update(buffer: MutableAggregationBuffer, input: Row): Unit = { if (!input.isNullAt(0)) { buffer(0) = buffer.getLong(0) + input.getLong(0) buffer(1) = buffer.getLong(1) + 1 } } // 不同Execute间的数据合并 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = { buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) } // 计算最终结果 def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1) } // 注册函数 spark.udf.register(&quot;myAverage&quot;, MyAverage) val df = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;) df.createOrReplaceTempView(&quot;employees&quot;) df.show() // +-------+------+ // | name|salary| // +-------+------+ // |Michael| 3000| // | Andy| 4500| // | Justin| 3500| // | Berta| 4000| // +-------+------+ val result = spark.sql(&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;) result.show() // +--------------+ // |average_salary| // +--------------+ // | 3750.0| // +--------------+ 2) 强类型用户自定义函数 通过继承Aggregator来实现强类型自定义聚合函数，同样是求平均工资。 import org.apache.spark.sql.expressions.Aggregator import org.apache.spark.sql.Encoder import org.apache.spark.sql.Encoders import org.apache.spark.sql.SparkSession // 既然是强类型，可能有case类 case class Employee(name: String, salary: Long) case class Average(var sum: Long, var count: Long) object MyAverage extends Aggregator[Employee, Average, Double] { // 定义一个数据结构，保存工资总数和工资总个数，初始都为0 def zero: Average = Average(0L, 0L) // Combine two values to produce a new value. For performance, the function may modify `buffer` // and return it instead of constructing a new object def reduce(buffer: Average, employee: Employee): Average = { buffer.sum += employee.salary buffer.count += 1 buffer } // 聚合不同execute的结果 def merge(b1: Average, b2: Average): Average = { b1.sum += b2.sum b1.count += b2.count b1 } // 计算输出 def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count // 设定之间值类型的编码器，要转换成case类 // Encoders.product是进行scala元组和case类转换的编码器 def bufferEncoder: Encoder[Average] = Encoders.product // 设定最终输出值的编码器 def outputEncoder: Encoder[Double] = Encoders.scalaDouble } import spark.implicits._ val ds = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;).as[Employee] ds.show() // +-------+------+ // | name|salary| // +-------+------+ // |Michael| 3000| // | Andy| 4500| // | Justin| 3500| // | Berta| 4000| // +-------+------+ // Convert the function to a `TypedColumn` and give it a name val averageSalary = MyAverage.toColumn.name(&quot;average_salary&quot;) val result = ds.select(averageSalary) result.show() // +--------------+ // |average_salary| // +--------------+ // | 3750.0| // +--------------+]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：9、Spark SQL 使用]]></title>
    <url>%2F2018%2F02%2F15%2Fspark-9-spark-sql-use.html</url>
    <content type="text"><![CDATA[Spark SQL 查询命令行查询打开Spark shell 创建如下JSON文件，注意JSON的格式： {&quot;name&quot;:&quot;Michael&quot;} {&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30} {&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19} scala&gt; import org.apache.spark.sql.SparkSession import org.apache.spark.sql.SparkSession scala&gt; val spark = SparkSession.builder().config(sparkconf).getOrCreate() 18/05/11 11:23:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect. spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@128d397c scala&gt; val rdd=spark.sparkContext.parallelize(Seq((&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1))) rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:29 scala&gt; import spark.implicits._ import spark.implicits._ scala&gt; val df = spark.read.json(&quot;hdfs://hadoop001:9000/people.json&quot;) df: org.apache.spark.sql.DataFrame = [age: bigint, name: string] scala&gt; df.show +----+-------+ | age| name| +----+-------+ |null|Michael| | 30| Andy| | 19| Justin| +----+-------+ scala&gt; df.filter($&quot;age&quot; &gt; 21).show +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ scala&gt; df.createOrReplaceTempView(&quot;persons&quot;) scala&gt; spark.sql(&quot;select * from persons&quot;) res3: org.apache.spark.sql.DataFrame = [age: bigint, name: string] scala&gt; spark.sql(&quot;select * from persons&quot;).show +----+-------+ | age| name| +----+-------+ |null|Michael| | 30| Andy| | 19| Justin| +----+-------+ scala&gt; spark.sql(&quot;select * from persons where age &gt; 21&quot;).show +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ scala&gt; IDEA创建SparkSQL程序IDEA中程序的打包和运行方式都和SparkCore类似，Maven依赖中需要添加新的依赖项： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.coderfei&lt;/groupId&gt; &lt;artifactId&gt;CoreWordCount&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.11.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*.scala&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 代码如下： package main import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession object DataFrameSyllabus { def main(args: Array[String]): Unit = { //创建SparkConf()并设置App名称 val sparkconf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;test&quot;).set(&quot;spark.port.maxRetries&quot;,&quot;1000&quot;) val spark = SparkSession.builder().config(sparkconf).getOrCreate() // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ val df = spark.read.json(&quot;hdfs://hadoop001:9000/people.json&quot;) // Displays the content of the DataFrame to stdout df.show() df.filter($&quot;age&quot; &gt; 21).show() df.createOrReplaceTempView(&quot;persons&quot;) spark.sql(&quot;SELECT * FROM persons where age &gt; 21&quot;).show() spark.stop() } }]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：8、Spark SQL 概述]]></title>
    <url>%2F2018%2F02%2F13%2Fspark-8-spark-sql-summary.html</url>
    <content type="text"><![CDATA[SparkSQL是什么 SparkSQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。 我们已经学习了Hive，它是将HiveSQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有SparkSQL的应运而生，它是将SparkSQL转换成RDD，然后提交到集群执行，执行效率非常快！ 1) 易整合 2) 统一的数据访问方式 3) 兼容Hive 4) 标准的数据连接 SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。 RDD、DataFrames、DataSet 在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看： RDD(Spark1.0)—&gt;Dataframe(Spark1.3)—&gt;Dataset(Spark1.6) 如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。 在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。 三者的详细说明1) RDD a RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。 b RDD的最大好处就是简单，API的人性化程度很高。 c RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。 RDD 例子如下: 2) DataFame 与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrameAPI提供的是一套高层的关系操作，比函数式的RDDAPI要更加友好，门槛更低。 由于与R和Pandas的DataFrame类似，SparkDataFrame很好地继承了传统单机数据分析的开发体验。 上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得SparkSQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。 DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待 DataFrame也是懒执行的。 性能上比RDD要高，主要有两方面原因： 定制化内存管理： 数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。 优化的执行计划： 查询计划通过Spark catalyst optimiser进行优化。 比如这个例子： 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而SparkSQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。 得到的优化执行计划在转换成物理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。 对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。 Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错。 3) DataSet 1) 是DataframeAPI的一个扩展，是Spark最新的数据抽象 2) 用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。 3) Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。 4) 样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。 5) Dataframe是Dataset的特列，DataFrame=Dataset[Row]，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。 6) DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。 RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。 三者的共性1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利 2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。 import org.apache.spark.sql.SparkSession import org.apache.spark.{SparkConf, SparkContext} val sparkconf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;test&quot;).set(&quot;spark.port.maxRetries&quot;,&quot;1000&quot;) val spark = SparkSession.builder().config(sparkconf).getOrCreate() val rdd=spark.sparkContext.parallelize(Seq((&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1))) // map不运行 rdd.map{line=&gt; println(&quot;运行&quot;) line._1 } 3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出 4、三者都有partition的概念 5、三者有许多共同的函数，如filter，排序等 6、在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。 import spark.implicits._ 7、DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型 DataFrame: testDF.map{ case Row(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; &quot;&quot; } Dataset: case class Coltest(col1:String,col2:Int)extends Serializable testDS.map{ case Coltest(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; &quot;&quot; } 三者的区别RDD: 1、RDD一般和sparkmlib同时使用 2、RDD不支持sparksql操作 DataFrame: 1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如： testDF.foreach{ line =&gt; val col1=line.getAs[String](&quot;col1&quot;) val col2=line.getAs[String](&quot;col2&quot;) } 每一列的值没法直接访问 2、DataFrame与Dataset一般不与spark ml同时使用 3、DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如： dataDF.createOrReplaceTempView(&quot;tmp&quot;) spark.sql(&quot;select ROW,DATE from tmp where DATE is not null order by DATE&quot;).show(100,false) 4、DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然 //保存 val saveoptions = Map(&quot;header&quot; -&gt; &quot;true&quot;, &quot;delimiter&quot; -&gt; &quot;\t&quot;, &quot;path&quot; -&gt; &quot;hdfs://master01:9000/test&quot;) datawDF.write.format(&quot;com.coderf.spark.csv&quot;).mode(SaveMode.Overwrite).options(saveoptions).save() //读取 val options = Map(&quot;header&quot; -&gt; &quot;true&quot;, &quot;delimiter&quot; -&gt; &quot;\t&quot;, &quot;path&quot; -&gt; &quot;hdfs://master01:9000/test&quot;) val datarDF= spark.read.options(options).format(&quot;com.coderf.spark.csv&quot;).load() 利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。 Dataset: Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段 而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。 case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型 /** rdd (&quot;a&quot;, 1) (&quot;b&quot;, 1) (&quot;a&quot;, 1) **/ val test: Dataset[Coltest]=rdd.map{line=&gt; Coltest(line._1,line._2) }.toDS test.map{ line=&gt; println(line.col1) println(line.col2) } 可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：7、Spark RDD依赖与DAG]]></title>
    <url>%2F2018%2F02%2F12%2Fspark-7-spark-rdd-dependency-dag.html</url>
    <content type="text"><![CDATA[RDD的依赖关系RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。 1) 窄依赖 窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用 2) 宽依赖 宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle 3) Lineage RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 下面例子说明了“窄”，“宽”依赖的不同现象： scala&gt; val text = sc.textFile(&quot;README.md&quot;) text: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24 scala&gt; val words = text.flatMap(_.split(&quot; &quot;)) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at &lt;console&gt;:26 scala&gt; words.map((_,1)) res0: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:29 scala&gt; res0.reduceByKey(_+_) res1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:31 scala&gt; res1.dependencies res2: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@6cfe48a4) scala&gt; res0.dependencies res3: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@6c9e24c4) DAG的生成DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 RDD的周边对应关系 输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。 随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。 随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。 1) 每个节点可以起一个或多个Executor。 2) 每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。 3) 每个Task执行的结果就是生成了目标RDD的一个partiton。 注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。 而 Task被执行的并发度 = Executor数目 * 每个Executor核数。 至于partition的数目： 1) 对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。 2) 在Map阶段partition数目保持不变。 3) 在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。 比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。 如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：6、Spark RDD持久化与检查点机制]]></title>
    <url>%2F2018%2F02%2F11%2Fspark-6-spark-rdd-cache-checkpoint.html</url>
    <content type="text"><![CDATA[RDD的持久化1) RDD的缓存 Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。 2) RDD缓存方式 RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。 通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。 scala&gt; val rdd = sc.makeRDD(1 to 10) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at makeRDD at &lt;console&gt;:25 scala&gt; val nocache = rdd.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;) nocache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at map at &lt;console&gt;:27 scala&gt; val cache = rdd.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;) cache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at map at &lt;console&gt;:27 scala&gt; cache.cache res24: cache.type = MapPartitionsRDD[21] at map at &lt;console&gt;:27 scala&gt; nocache.collect res25: Array[String] = Array(1[1505479375155], 2[1505479374674], 3[1505479374674], 4[1505479375153], 5[1505479375153], 6[1505479374675], 7[1505479375154], 8[1505479375154], 9[1505479374676], 10[1505479374676]) scala&gt; nocache.collect res26: Array[String] = Array(1[1505479375679], 2[1505479376157], 3[1505479376157], 4[1505479375680], 5[1505479375680], 6[1505479376159], 7[1505479375680], 8[1505479375680], 9[1505479376158], 10[1505479376158]) scala&gt; nocache.collect res27: Array[String] = Array(1[1505479376743], 2[1505479377218], 3[1505479377218], 4[1505479376745], 5[1505479376745], 6[1505479377219], 7[1505479376747], 8[1505479376747], 9[1505479377218], 10[1505479377218]) scala&gt; cache.collect res28: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253]) scala&gt; cache.collect res29: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253]) scala&gt; cache.collect res30: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253]) cache.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY) 在存储级别的末尾加上“_2”来把持久化数据存为两份。 缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。 注意：使用 Tachyon可以实现堆外缓存。 RDD检查点机制Spark中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。 cache 和 checkpoint 是有显著区别的， 缓存把 RDD 计算出来然后放在内存中，但是RDD 的依赖链（相当于数据库中的redo 日志），也不能丢掉，当某个点某个 executor 宕了，上面cache 的RDD就会丢掉，需要通过依赖链重放计算出来，不同的是，checkpoint是把 RDD 保存在 HDFS中， 是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链， 是通过复制实现的高容错。 如果存在以下场景，则比较适合使用检查点机制： 1) DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。 2) 在宽依赖上做Checkpoint获得的收益更大。 为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。 scala&gt; val data = sc.parallelize(1 to 100 , 5) data: org.apache.spark.rdd.RDD[Int] =ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:12 scala&gt; sc.setCheckpointDir(&quot;hdfs://hadoop001:9000/checkpoint&quot;) scala&gt; data.checkpoint scala&gt; data.count scala&gt; val ch1 = sc.parallelize(1 to 2) ch1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:25 scala&gt; val ch2 = ch1.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;) ch2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at map at &lt;console&gt;:27 scala&gt; val ch3 = ch1.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;) ch3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[37] at map at &lt;console&gt;:27 scala&gt; ch3.checkpoint scala&gt; ch2.collect res62: Array[String] = Array(1[1505480940726], 2[1505480940243]) scala&gt; ch2.collect res63: Array[String] = Array(1[1505480941957], 2[1505480941480]) scala&gt; ch2.collect res64: Array[String] = Array(1[1505480942736], 2[1505480942257]) scala&gt; ch3.collect res65: Array[String] = Array(1[1505480949080], 2[1505480948603]) scala&gt; ch3.collect res66: Array[String] = Array(1[1505480948683], 2[1505480949161]) scala&gt; ch3.collect res67: Array[String] = Array(1[1505480948683], 2[1505480949161]) checkpoint写流程RDD checkpoint 过程中会经过以下几个状态： [ Initialized → marked for checkpointing → checkpointing in progress → checkpointed ] 转换流程如下： 1) data.checkpoint 这个函数调用中， 设置的目录中， 所有依赖的 RDD 都会被删除， 函数必须在 job 运行之前调用执行， 强烈建议 RDD 缓存在内存中（又提到一次，千万要注意，即checkpoint()之前，先cache()），否则保存到文件的时候需要从头计算。初始化RDD的 checkpointData 变量为 ReliableRDDCheckpointData。 这时候标记为 Initialized 状态 2) 在所有 job action 的时候，runJob 方法中都会调用 rdd.doCheckpoint , 这个会向前递归调用所有的依赖的RDD，看看需不需要 checkpoint。如果需要，然后调用 checkpointData.get.checkpoint()，里面标记状态为CheckpointingInProgress，里面调用具体实现类的 ReliableRDDCheckpointData的doCheckpoint 方法。 3) doCheckpoint -&gt; writeRDDToCheckpointDirectory， 注意这里会把 job 再运行一次， 如果已经cache 了，就可以直接使用缓存中的 RDD 了， 就不需要重头计算一遍了（怎么又说了一遍），这时候直接把RDD，输出到 hdfs，每个分区一个文件，会先写到一个临时文件，如果全部输出完，进行rename，如果输出失败，就回滚delete。 4) 标记 状态为 Checkpointed， markCheckpointed方法中清除所有的依赖， 怎么清除依赖的呢， 就是把RDD 变量的强引用设置为 null，垃圾回收了，会触发 ContextCleaner 里面的监听，清除实际 BlockManager 缓存中的数据。 checkpoint读流程如果一个RDD我们已经checkpoint了那么是什么时候用呢，checkpoint将RDD持久化到HDFS或本地文件夹，如果不被手动remove掉，是一直存在的，也就是说可以被下一个driverprogram使用。比如sparkstreaming挂掉了，重启后就可以使用之前checkpoint的数据进行recover,当然在同一个driverprogram也可以使用。我们讲下在同一个driverprogram中是怎么使用checkpoint数据的。 具体细节如下： 如果一个RDD被checkpoint了，那么这个RDD中对分区和依赖的处理都是使用的RDD内部的checkpointRDD变量，具体实现是ReliableCheckpointRDD类型。这个是在checkpoint写流程中创建的。依赖和获取分区方法中先判断是否已经checkpoint，如果已经checkpoint了，就斩断依赖，使用ReliableCheckpointRDD，来处理依赖和获取分区。如果没有，才往前回溯依赖。依赖就是没有依赖，因为已经斩断了依赖，获取分区数据就是读取checkpoint到hdfs目录中不同分区保存下来的文件。 如果在 Scala 中出现了 NotSerializableException，通常问题就在于我们传递了一个不可序列 化的类中的函数或字段。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：5、Spark RDD 编程（Action）]]></title>
    <url>%2F2018%2F02%2F10%2Fspark-5-spark-rdd-program-action.html</url>
    <content type="text"><![CDATA[常用算子：Action1) reduce(func) 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。 scala&gt; val rdd1 = sc.makeRDD(1 to 10,2) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at makeRDD at &lt;console&gt;:24 scala&gt; rdd1.reduce(_+_) res50: Int = 55 scala&gt; val rdd2 = sc.makeRDD(Array((&quot;a&quot;,1),(&quot;a&quot;,3),(&quot;c&quot;,3),(&quot;d&quot;,5))) rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[86] at makeRDD at &lt;console&gt;:24 scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2)) res51: (String, Int) = (adca,12)] 2) collect() 在驱动程序中，以数组的形式返回数据集的所有元素。 scala&gt; var rdd1 = sc.makeRDD(1 to 10, 2) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:24 scala&gt; rdd1.collect() res0: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) 3) count() 返回RDD的元素个数。 scala&gt; var rdd2 = sc.makeRDD(1 to 10, 2) rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:24 scala&gt; rdd2.count() res1: Long = 10 4) first() 返回RDD的第一个元素(类似于take(1))。 scala&gt; var rdd3 = sc.makeRDD(1 to 10, 2) rdd3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at makeRDD at &lt;console&gt;:24 scala&gt; rdd3.first() res2: Int = 1 5) take(n) 返回一个由数据集的前n个元素组成的数组。 scala&gt; var rdd4 = sc.makeRDD(1 to 10, 2) rdd4: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at makeRDD at &lt;console&gt;:24 scala&gt; rdd4.take(5) res3: Array[Int] = Array(1, 2, 3, 4, 5) 6) takeSample(withReplacement,num, [seed]) 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。 scala&gt; var rdd5 = sc.parallelize(1 to 10, 2) rdd5: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at &lt;console&gt;:24 scala&gt; rdd5.collect res4: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) scala&gt; rdd5.takeSample(true, 5, 3) res5: Array[Int] = Array(3, 5, 5, 9, 7) 7) takeOrdered(n) 返回前几个的排序。 scala&gt; val rdd6 = sc.makeRDD(Seq(10, 4, 2, 12, 3)) rdd6: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at makeRDD at &lt;console&gt;:24 scala&gt; rdd6.top(2) res6: Array[Int] = Array(12, 10) scala&gt; rdd6.takeOrdered(2) res7: Array[Int] = Array(2, 3) scala&gt; rdd6.takeOrdered(4) res8: Array[Int] = Array(2, 3, 4, 10) 8) aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U) aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。 scala&gt; var rdd7 = sc.makeRDD(1 to 10, 2) rdd7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at makeRDD at &lt;console&gt;:24 scala&gt; rdd7.aggregate(1)( | {(x: Int, y: Int) =&gt; x + y}, | {(a: Int, b: Int) =&gt; a + b}) res9: Int = 58 scala&gt; rdd7.aggregate(1)( | {(x: Int, y: Int) =&gt; x * y}, | {(a: Int, b: Int) =&gt; a + b}) res10: Int = 30361 9) fold(num)(func) 折叠操作，aggregate的简化操作，seqop和combop一样。 scala&gt; var rdd8 = sc.makeRDD(1 to 4,2) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[90] at makeRDD at &lt;console&gt;:24 scala&gt; rdd8.aggregate(1)( | {(x : Int,y : Int) =&gt; x + y}, | {(a : Int,b : Int) =&gt; a + b} | ) res59: Int = 13 scala&gt; rdd8.fold(1)(_+_) res60: Int = 13 10) saveAsTextFile(path) 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。 scala&gt; val rdd8 = sc.parallelize(1 to 10, 2) rdd8: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at parallelize at &lt;console&gt;:24 scala&gt; rdd8.saveAs saveAsObjectFile saveAsTextFile scala&gt; rdd8.saveAsTextFile(&quot;hdfs://hadoop001:9000/my_rdd&quot;) 11) saveAsObjectFile(path) 用于将RDD中的元素序列化成对象，存储到文件中。 scala&gt; val rdd10 = sc.parallelize(List((1, 3), (1, 2), (1, 4), (2, 3), (3, 6), (3, 8))) rdd10: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24 scala&gt; rdd10.saveAsObjectFile(&quot;hdfs://hadoop001:9000/my_obj&quot;) 12) countByKey() 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3) rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at parallelize at &lt;console&gt;:24 scala&gt; rdd.countByKey() res63: scala.collection.Map[Int,Long] = Map(3 -&gt; 2, 1 -&gt; 3, 2 -&gt; 1) 13) foreach(func) 在数据集的每一个元素上，运行函数func进行更新。 scala&gt; var rdd = sc.makeRDD(1 to 10,2) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[107] at makeRDD at &lt;console&gt;:24 scala&gt; var sum = sc.accumulator(0) warning: there were two deprecation warnings; re-run with -deprecation for details sum: org.apache.spark.Accumulator[Int] = 0 scala&gt; rdd.foreach(sum+=_) scala&gt; sum.value res68: Int = 55 scala&gt; rdd.collect().foreach(println) 1 2 3 4 5 6 7 8 9 10 RDD中数值元素的统计操作Spark 对包含数值数据的 RDD 提供了一些描述性的统计操作。Spark 的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用 stats() 时通过一次遍历数据计算出来，并以StatsCounter对象返回。 scala&gt; var rdd1 = sc.makeRDD(1 to 100) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[42] at makeRDD at &lt;console&gt;:32 scala&gt; rdd1.sum() res34: Double = 5050.0 scala&gt; rdd1.max() res35: Int = 100 RDD算子中接受的函数注意事项Spark 的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。 在 Scala 中，我们可以把定义的内联函数、方法的引用或静态方法传递给 Spark，就像 Scala 的其他函数式 API 一样。我们还要考虑其他一些细节，比如所传递的函数及其引用 的数据需要是可序列化的(实现了 Java 的 Serializable 接口)。 传递一个对象的方法或者字段时，会包含对整个对象的引用。 class SearchFunctions(val query: String) extends java.io.Serializable{ def isMatch(s: String): Boolean = { s.contains(query) } def getMatchesFunctionReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = { // 问题:&quot;isMatch&quot;表示&quot;this.isMatch&quot;，因此我们要传递整个&quot;this&quot; rdd.filter(isMatch) } def getMatchesFieldReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = { // 问题:&quot;query&quot;表示&quot;this.query&quot;，因此我们要传递整个&quot;this&quot; rdd.filter(x =&gt; x.contains(query)) } def getMatchesNoReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = { // 安全:只把我们需要的字段拿出来放入局部变量中 val query_ = this.query rdd.filter(x =&gt; x.contains(query_)) } } 如果在 Scala 中出现了 NotSerializableException，通常问题就在于我们传递了一个不可序列 化的类中的函数或字段。 不同RDD类型的转换有些函数只能用于特定类型的 RDD，比如 mean() 和 variance() 只能用在数值 RDD 上， 而 join() 只能用在键值对 RDD 上。在 Scala 和 Java 中，这些函数都没有定义在标准的 RDD 类中，所以要访问这些附加功能，必须要确保获得了正确的专用 RDD 类。在 Scala 中，将 RDD 转为有特定函数的 RDD(比如在 RDD[Double] 上进行数值操作)是 由隐式转换来自动处理的。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：4、Spark RDD 编程（Transformation）]]></title>
    <url>%2F2018%2F02%2F09%2Fspark-4-spark-rdd-program-transformation.html</url>
    <content type="text"><![CDATA[编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。 要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。 创建RDD在Spark中创建RDD的创建方式大概可以分为三种： （1）从集合中创建RDD； （2）从外部存储创建RDD； （3）从其他RDD创建。 1) 由一个已经存在的Scala集合创建，集合并行化。 例如 val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8)) 而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明： def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] 我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的： def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } 我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]. Spark文档的说明是： Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item. 原来，这个函数还为数据提供了位置信息，来看看我们怎么使用： scala&gt; val test1= sc.parallelize(List(1,2,3)) test1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21 scala&gt; val test2 = sc.makeRDD(List(1,2,3)) test2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21 scala&gt; val seq = List((1, List(&quot;slave01&quot;)), (2, List(&quot;slave02&quot;))) seq: List[(Int, List[String])] = List((1,List(slave01)), (2,List(slave02))) scala&gt; val test3 = sc.makeRDD(seq) test3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23 scala&gt; test3.preferredLocations(test3.partitions(1)) res26: Seq[String] = List(slave02) scala&gt; test3.preferredLocations(test3.partitions(0)) res27: Seq[String] = List(slave01) scala&gt; test1.preferredLocations(test1.partitions(0)) res28: Seq[String] = List() 我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下 def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs) } 都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。 2) 由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等 scala&gt; val test = sc.textFile(&quot;hdfs://hadoop001:9000/RELEASE&quot;) test: org.apache.spark.rdd.RDD[String] = hdfs://hadoop001:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24 RDD算子操作RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。 常用算子：Transformation1) map(func) 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成。 scala&gt; var source = sc.parallelize(1 to 10) source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24 scala&gt; source.collect() res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) scala&gt; val mapadd = source.map(_ * 2) mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26 scala&gt; mapadd.collect() res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20) 2) filter(func) 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。 scala&gt; var sourceFilter = sc.parallelize(Array(&quot;xiaoming&quot;,&quot;xiaojiang&quot;,&quot;xiaohe&quot;,&quot;dazhi&quot;)) sourceFilter: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24 scala&gt; val filter = sourceFilter.filter(_.contains(&quot;xiao&quot;)) filter: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at &lt;console&gt;:26 scala&gt; sourceFilter.collect() res9: Array[String] = Array(xiaoming, xiaojiang, xiaohe, dazhi) scala&gt; filter.collect() res10: Array[String] = Array(xiaoming, xiaojiang, xiaohe) 3) flatMap(func) 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。 scala&gt; val sourceFlat = sc.parallelize(1 to 5) sourceFlat: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24 scala&gt; sourceFlat.collect() res11: Array[Int] = Array(1, 2, 3, 4, 5) scala&gt; val flatMap = sourceFlat.flatMap(1 to _) flatMap: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26 scala&gt; flatMap.collect() res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5) 4) mapPartitions(func) 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。 scala&gt; val rdd = sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;),(&quot;lucy&quot;,&quot;female&quot;))) rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24 scala&gt; :paste // Entering paste mode (ctrl-D to finish) def partitionsFun(iter : Iterator[(String,String)]) : Iterator[String] = { var woman = List[String]() while (iter.hasNext){ val next = iter.next() next match { case (_,&quot;female&quot;) =&gt; woman = next._1 :: woman case _ =&gt; } } woman.iterator } // Exiting paste mode, now interpreting. partitionsFun: (iter: Iterator[(String, String)])Iterator[String] scala&gt; val result = rdd.mapPartitions(partitionsFun) result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at mapPartitions at &lt;console&gt;:28 scala&gt; result.collect() res13: Array[String] = Array(kpop, lucy) 5) mapPartitionsWithIndex(func) 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。 scala&gt; val rdd = sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;),(&quot;lucy&quot;,&quot;female&quot;))) rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24 scala&gt; :paste // Entering paste mode (ctrl-D to finish) def partitionsFun(index : Int, iter : Iterator[(String,String)]) : Iterator[String] = { var woman = List[String]() while (iter.hasNext){ val next = iter.next() next match { case (_,&quot;female&quot;) =&gt; woman = &quot;[&quot;+index+&quot;]&quot;+next._1 :: woman case _ =&gt; } } woman.iterator } // Exiting paste mode, now interpreting. partitionsFun: (index: Int, iter: Iterator[(String, String)])Iterator[String] scala&gt; val result = rdd.mapPartitionsWithIndex(partitionsFun) result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at mapPartitionsWithIndex at &lt;console&gt;:28 scala&gt; result.collect() res14: Array[String] = Array([0]kpop, [3]lucy) 6) sample(withReplacement, fraction, seed) 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。 scala&gt; val rdd = sc.parallelize(1 to 10) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24 scala&gt; rdd.collect() res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) scala&gt; var sample1 = rdd.sample(true,0.4,2) sample1: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[21] at sample at &lt;console&gt;:26 scala&gt; sample1.collect() res16: Array[Int] = Array(1, 2, 2, 7, 7, 8, 9) scala&gt; var sample2 = rdd.sample(false,0.2,3) sample2: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[22] at sample at &lt;console&gt;:26 scala&gt; sample2.collect() res17: Array[Int] = Array(1, 9) 7) takeSample 和Sample的区别是：takeSample返回的是最终的结果集合。 8) union(otherDataset) 对源RDD和参数RDD求并集后返回一个新的RDD。 scala&gt; val rdd1 = sc.parallelize(1 to 5) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24 scala&gt; val rdd2 = sc.parallelize(5 to 10) rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24 scala&gt; val rdd3 = rdd1.union(rdd2) rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[25] at union at &lt;console&gt;:28 scala&gt; rdd3.collect() res18: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10) 9) intersection(otherDataset) 对源RDD和参数RDD求交集后返回一个新的RDD。 scala&gt; val rdd1 = sc.parallelize(1 to 7) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24 scala&gt; val rdd2 = sc.parallelize(5 to 10) rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:24 scala&gt; val rdd3 = rdd1.intersection(rdd2) rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at intersection at &lt;console&gt;:28 scala&gt; rdd3.collect() [Stage 15:=============================&gt; (2 + 2) res19: Array[Int] = Array(5, 6, 7) 10) distinct([numTasks])) 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。 scala&gt; val distinctRdd = sc.parallelize(List(1,2,1,5,2,9,6,1)) distinctRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24 scala&gt; val unionRDD = distinctRdd.distinct() unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[37] at distinct at &lt;console&gt;:26 scala&gt; unionRDD.collect() [Stage 16:&gt; (0 + 4) [Stage 16:=============================&gt; (2 + 2) res20: Array[Int] = Array(1, 9, 5, 6, 2) scala&gt; val unionRDD = distinctRdd.distinct(2) unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at distinct at &lt;console&gt;:26 scala&gt; unionRDD.collect() res21: Array[Int] = Array(6, 2, 1, 9, 5) 11) partitionBy 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区，否则会生成ShuffleRDD。 scala&gt; val rdd = sc.parallelize(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;),(4,&quot;ddd&quot;)),4) rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24 scala&gt; rdd.partitions.size res24: Int = 4 scala&gt; var rdd2 = rdd.partitionBy(new org.apache.spark.HashPartitioner(2)) rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[45] at partitionBy at &lt;console&gt;:26 scala&gt; rdd2.partitions.size res25: Int = 2 12) reduceByKey(func, [numTasks]) 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。 scala&gt; val rdd = sc.parallelize(List((&quot;female&quot;,1),(&quot;male&quot;,5),(&quot;female&quot;,5),(&quot;male&quot;,2))) rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[46] at parallelize at &lt;console&gt;:24 scala&gt; val reduce = rdd.reduceByKey((x,y) =&gt; x+y) reduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[47] at reduceByKey at &lt;console&gt;:26 scala&gt; reduce.collect() res29: Array[(String, Int)] = Array((female,6), (male,7)) 13) groupByKey groupByKey也是对每个key进行操作，但只生成一个sequence。 scala&gt; val words = Array(&quot;one&quot;, &quot;two&quot;, &quot;two&quot;, &quot;three&quot;, &quot;three&quot;, &quot;three&quot;) words: Array[String] = Array(one, two, two, three, three, three) scala&gt; val wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1)) wordPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at &lt;console&gt;:26 scala&gt; val group = wordPairsRDD.groupByKey() group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[5] at groupByKey at &lt;console&gt;:28 scala&gt; group.collect() res1: Array[(String, Iterable[Int])] = Array((two,CompactBuffer(1, 1)), (one,CompactBuffer(1)), (three,CompactBuffer(1, 1, 1))) scala&gt; group.map(t =&gt; (t._1, t._2.sum)) res2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at &lt;console&gt;:31 scala&gt; res2.collect() res3: Array[(String, Int)] = Array((two,2), (one,1), (three,3)) scala&gt; val map = group.map(t =&gt; (t._1, t._2.sum)) map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at map at &lt;console&gt;:30 scala&gt; map.collect() res4: Array[(String, Int)] = Array((two,2), (one,1), (three,3)) 14) combineByKeyC =&gt; C, mergeCombiners: (C, C) =&gt; C) 对相同K，把V合并成一个集合. createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建那个键对应的累加器的初始值 mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并 mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。 scala&gt; val scores = Array((&quot;Fred&quot;, 88), (&quot;Fred&quot;, 95), (&quot;Fred&quot;, 91), (&quot;Wilma&quot;, 93), (&quot;Wilma&quot;, 95), (&quot;Wilma&quot;, 98)) scores: Array[(String, Int)] = Array((Fred,88), (Fred,95), (Fred,91), (Wilma,93), (Wilma,95), (Wilma,98)) scala&gt; val input = sc.parallelize(scores) input: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26 scala&gt; val combine = input.combineByKey( | (v)=&gt;(v,1), | (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1), | (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2)) combine: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[53] at combineByKey at &lt;console&gt;:28 scala&gt; val result = combine.map{ | case (key,value) =&gt; (key,value._1/value._2.toDouble)} result: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[54] at map at &lt;console&gt;:30 scala&gt; result.collect() res33: Array[(String, Double)] = Array((Wilma,95.33333333333333), (Fred,91.33333333333333)) 15) aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U) 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。 例如：List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8))，分一个分区，以key为1的分区为例，0先和3比较得3，3在和2比较得3，3在和4比较得4，所以整个key为1的组最终结果为（1，4），同理，key为2的最终结果为（2，3），key为3的为（3，8）.如果分三个分区，前两个是一个分区，中间两个是一个分区，最后两个是一个分区，第一个分区的最终结果为（1，3），第二个分区为（1，4）（2，3），最后一个分区为（3，8），combine后为 (3,8), (1,7), (2,3) scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8))) rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24 scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_) agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[13] at aggregateByKey at &lt;console&gt;:26 scala&gt; agg.collect() res7: Array[(Int, Int)] = Array((3,8), (1,7), (2,3)) scala&gt; agg.partitions.size res8: Int = 3 scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1) rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24 scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect() agg: Array[(Int, Int)] = Array((1,4), (3,8), (2,3)) 16) foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]aggregateByKey的简化操作，seqop和combop相同。 scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8))) rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[91] at parallelize at &lt;console&gt;:24 scala&gt; val agg = rdd.foldByKey(0)(_+_) agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[92] at foldByKey at &lt;console&gt;:26 scala&gt; agg.collect() res61: Array[(Int, Int)] = Array((3,14), (1,9), (2,3)) 17) sortByKey([ascending], [numTasks])在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。 scala&gt; val rdd = sc.parallelize(Array((3,&quot;aa&quot;),(6,&quot;cc&quot;),(2,&quot;bb&quot;),(1,&quot;dd&quot;))) rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24 scala&gt; rdd.sortByKey(true).collect() res9: Array[(Int, String)] = Array((1,dd), (2,bb), (3,aa), (6,cc)) scala&gt; rdd.sortByKey(false).collect() res10: Array[(Int, String)] = Array((6,cc), (3,aa), (2,bb), (1,dd)) 18) sortBy(func,[ascending], [numTasks]) 与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。 scala&gt; val rdd = sc.parallelize(List(1,2,3,4)) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24 scala&gt; rdd.sortBy(x =&gt; x).collect() res11: Array[Int] = Array(1, 2, 3, 4) scala&gt; rdd.sortBy(x =&gt; x%3).collect() res12: Array[Int] = Array(3, 4, 1, 2) 19) join(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。 scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;))) rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24 scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6))) rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24 scala&gt; rdd.join(rdd1).collect() res13: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6))) 20) cogroup(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD。 scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;))) rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24 scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6))) rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24 scala&gt; rdd.cogroup(rdd1).collect() res14: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6)))) scala&gt; val rdd2 = sc.parallelize(Array((4,4),(2,5),(3,6))) rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[41] at parallelize at &lt;console&gt;:24 scala&gt; rdd.cogroup(rdd2).collect() res15: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6)))) scala&gt; val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;))) rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24 scala&gt; rdd3.cogroup(rdd2).collect() [Stage 36:&gt; (0 + 0) res16: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(d, a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6)))) 21) cartesian(otherDataset) 笛卡尔积 scala&gt; val rdd1 = sc.parallelize(1 to 3) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24 scala&gt; val rdd2 = sc.parallelize(2 to 5) rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at &lt;console&gt;:24 scala&gt; rdd1.cartesian(rdd2).collect() res17: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (2,2), (2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5)) 22) pipe(command, [envVars]) 对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。 Shell脚本 #!/bin/sh echo &quot;AA&quot; while read LINE; do echo &quot;&gt;&gt;&gt;&quot;${LINE} done shell脚本需要集群中的所有节点都能访问到。 scala&gt; val rdd = sc.parallelize(List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;),1) rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[50] at parallelize at &lt;console&gt;:24 scala&gt; rdd.pipe(&quot;/home/bigdata/pipe.sh&quot;).collect() res18: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you) scala&gt; val rdd = sc.parallelize(List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;),2) rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:24 scala&gt; rdd.pipe(&quot;/home/bigdata/pipe.sh&quot;).collect() res19: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, AA, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you) pipe.sh: #!/bin/sh echo &quot;AA&quot; while read LINE; do echo &quot;&gt;&gt;&gt;&quot;${LINE} done 23) coalesce(numPartitions) 缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。 scala&gt; val rdd = sc.parallelize(1 to 16,4) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[54] at parallelize at &lt;console&gt;:24 scala&gt; rdd.partitions.size res20: Int = 4 scala&gt; val coalesceRDD = rdd.coalesce(3) coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[55] at coalesce at &lt;console&gt;:26 scala&gt; coalesceRDD.partitions.size res21: Int = 3 24) repartition(numPartitions) 根据分区数，从新通过网络随机洗牌所有数据。 scala&gt; val rdd = sc.parallelize(1 to 16,4) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at &lt;console&gt;:24 scala&gt; rdd.partitions.size res22: Int = 4 scala&gt; val rerdd = rdd.repartition(2) rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[60] at repartition at &lt;console&gt;:26 scala&gt; rerdd.partitions.size res23: Int = 2 scala&gt; val rerdd = rdd.repartition(4) rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[64] at repartition at &lt;console&gt;:26 scala&gt; rerdd.partitions.size res24: Int = 4 25) repartitionAndSortWithinPartitions(partitioner) repartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高。 26) glom 将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。 scala&gt; val rdd = sc.parallelize(1 to 16,4) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[65] at parallelize at &lt;console&gt;:24 scala&gt; rdd.glom().collect() res25: Array[Array[Int]] = Array(Array(1, 2, 3, 4), Array(5, 6, 7, 8), Array(9, 10, 11, 12), Array(13, 14, 15, 16)) 27) mapValues 针对于(K,V)形式的类型只对V进行操作。 scala&gt; val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;))) rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[67] at parallelize at &lt;console&gt;:24 scala&gt; rdd3.mapValues(_+&quot;|||&quot;).collect() res26: Array[(Int, String)] = Array((1,a|||), (1,d|||), (2,b|||), (3,c|||)) 28) subtract 计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来。 scala&gt; val rdd = sc.parallelize(3 to 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[70] at parallelize at &lt;console&gt;:24 scala&gt; val rdd1 = sc.parallelize(1 to 5) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[71] at parallelize at &lt;console&gt;:24 scala&gt; rdd.subtract(rdd1).collect() res27: Array[Int] = Array(8, 6, 7)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：3、Spark RDD]]></title>
    <url>%2F2018%2F02%2F08%2Fspark-3-spark-rdd.html</url>
    <content type="text"><![CDATA[RDD存在的原因RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。 MR中的迭代 Spark中的迭代 我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。 RDD是什么RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。 在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。 RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。 RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。 Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。 默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。 RDD的基本属性首先来看一下官方的描述 1) 一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 2) 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 3) RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 4) 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 5) 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 小结：RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。 RDD的弹性理解1) 自动进行内存和磁盘数据存储的切换 Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换。 2) 基于血统的高效容错机制 在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。 3) Task如果失败会自动进行特定次数的重试 RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。 4) Stage如果失败会自动进行特定次数的重试 如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。 5) Checkpoint和Persist可主动或被动触发 RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RDD依赖都会被移除。 6) 数据调度弹性 Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。 7) 数据分片的高度弹性 可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。 小结 RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系统，简单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。 RDD的特点RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。 1) 分区 RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。 2) 只读 如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 3) 依赖 RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 4) 缓存 如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。 5) checkpoint 虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。给定一个RDD我们至少可以知道如下几点信息： 1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：2、Spark 集群部署]]></title>
    <url>%2F2018%2F02%2F06%2Fspark-2-spark-cluster.html</url>
    <content type="text"><![CDATA[集群角色 从物理部署层面上来看，Spark主要分为两种类型的节点，Master节点和Worker节点，Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。 从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。 Spark的部署模式有Local、Local-Cluster、Standalone、Yarn、Mesos。 安装部署Standalone模式1) 解压spark $ tar -zxvf /opt/software/installations/spark-2.2.0-bin-hadoop2.6.0.tgz -C /opt/software/ 2) 进入spark安装目录下的conf目录中，重命名“.template”结尾的文件 $ mv docker.properties.template docker.properties $ mv log4j.properties.template log4j.properties $ mv metrics.properties.template metrics.properties $ mv slaves.template slaves $ mv spark-defaults.conf.template spark-defaults.conf $ mv spark-env.sh.template spark-env.sh 3) 修改slaves hadoop001 hadoop002 hadoop003 4) 修改spark-default.conf，用于配置Job History Server spark.eventLog.enabled true spark.eventLog.dir hdfs://hadoop001:9000/directory spark.eventLog.compress true 5) 修改spark-env.sh SPARK_MASTER_HOST= hadoop001 SPARK_MASTER_PORT=7077 export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://hadoop001:9000/directory&quot; 6) 分发配置好的spark安装包 $ scp -r spark-2.2.0-bin-hadoop2.6.0/ hadoop002:/opt/software/ $ scp -r spark-2.2.0-bin-hadoop2.6.0/ hadoop003:/opt/software/ $ sbin/start-all.sh 7) 启动spark（启动spark之前，确保HDFS已经启动） $ bin/spark-shell \ --master spark://hadoop001:7077 \ --executor-memory 2g \ --total-executor-cores 2 Spark的高可用Spark-Master的高可用，需要借助于Zookeeper，所以先确保Zookeeper启动完成。 高可用架构图： 1) 修改spark-env.sh文件(注释掉之前的：SPARK_MASTER_HOST=hadoop001) # SPARK_MASTER_HOST=hadoop001 SPARK_MASTER_PORT=7077 export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://hadoop001:9000/directory&quot; export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop001:2181,hadoop002:2181,hadoop003:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 2) 分发配置 $ scp -r spark-2.2.0-bin-hadoop2.6.0/conf hadoop002:/opt/software/spark-2.2.0-bin-hadoop2.6.0/ $ scp -r spark-2.2.0-bin-hadoop2.6.0/conf hadoop003:/opt/software/spark-2.2.0-bin-hadoop2.6.0 3) 按照如下规则执行启动脚本 第一台master机器 $ sbin/start-all.sh 第二台master机器 $ sbin/start-master.sh 4) client连接高可用的spark集群 $ bin/spark-shell –master spark://hadoop001:7077,hadoop002:7077 安装部署Yarn模式1) 修改yarn-site.xml &lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 2) 修改spark-evn.sh HADOOP_CONF_DIR=/opt/software/hadoop-2.6.0/etc/hadoop YARN_CONF_DIR=/opt/software/hadoop-2.6.0/etc/hadoop 运行Spark任务Standalone上运行bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://hadoop001:7077 \ --executor-memory 1G \ --total-executor-cores 2 \ /opt/software/spark-2.2.0-bin-hadoop2.6.0/examples/jars/spark-examples_2.11-2.2.0.jar \ 100 参数说明 --master spark://hadoop001:7077 指定Master的地址 --executor-memory 1G 指定每个executor可用内存为1G --total-executor-cores 2 指定每个executor使用的cup核数为2个 --jars 添加任务所需的其他依赖 Yarn上运行/opt/software/spark-2.2.0-bin-hadoop2.6.0/bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode client \ /opt/software/spark-2.2.0-bin-hadoop2.6.0/examples/jars/spark-examples_2.11-2.2.0.jar \ 100 应用提交的方式1) 打包完成后，可以使用bin/spark-submit脚本来提交应用，格式如下 bin/spark-submit \ --class &lt;main-class&gt; --master &lt;master-url&gt; \ --deploy-mode &lt;deploy-mode&gt; \ --conf &lt;key&gt;=&lt;value&gt; \ ... # other options &lt;application-jar&gt; \ [application-arguments] 2) 解释 其中–master可以有如下URL形式 Spark-Shell交互式编程启动Spark-Shellbin/spark-shell \ --master spark://hadoop001:7077 \ --executor-memory 2g \ --total-executor-cores 2 如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可。 运行WordCount程序随意创建一个单词本，例如words.txt，然后上传至HDFS中，使用spark进行操作。 举例 sc.textFile(&quot;hdfs://hadoop001:9000/words.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://hadoop001:9000/output&quot;) Spark概念总结每个Spark应用都由一个驱动器程序(driver program)来发起集群上的各种 并行操作。驱动器程序包含应用的 main 函数，并且定义了集群上的分布式数据集，还对这 些分布式数据集应用了相关操作。 驱动器程序通过一个 SparkContext 对象来访问 Spark。这个对象代表对计算集群的一个连 接。shell 启动时已经自动创建了一个 SparkContext 对象，是一个叫作 sc 的变量。 驱动器程序一般要管理多个执行器(executor)节点。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark：1、Spark 概述]]></title>
    <url>%2F2018%2F02%2F05%2Fspark-1-spark-summary.html</url>
    <content type="text"><![CDATA[什么是spark官网地址 Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。项目是用Scala进行编写。 目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是基于内存计算的大数据并行计算框架。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark 使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分 别管理的负担。 Spark的内置项目如下 Spark CoreSpark Core实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义。 Spark SQLSpark SQL是Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。 Spark StreamingSpark Streaming是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。 Spark MLlib提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 集群管理器Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(cluster manager)上运行，包括 Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度 器，叫作独立调度器。 Spark得到了众多大数据公司的支持，这些公司包括Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、阿里、腾讯、京东、携程、优酷土豆。当前百度的Spark已应用于凤巢、大搜索、直达号、百度大数据等业务；阿里利用GraphX构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯Spark集群达到8000台的规模，是当前已知的世界上最大的Spark集群。 Spark特点快与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。 易用Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。 通用Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。 兼容性Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。 Spark的用户和用途大致把Spark的用例分为两类：数据科学应用和数据处理应用。也就对应的有两种人群：数据科学家和工程师。 数据科学任务主要是数据分析领域，数据科学家要负责分析数据并建模，具备 SQL、统计、预测建模(机器学习)等方面的经验，以及一定的使用 Python、 Matlab 或 R 语言进行编程的能力。 数据处理应用工程师定义为使用 Spark 开发 生产环境中的数据处理应用的软件开发者，通过对接Spark的API实现对处理的处理和转换等任务。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase：6、HBase 优化]]></title>
    <url>%2F2018%2F02%2F03%2Fhbase-6-hbase-optimization.html</url>
    <content type="text"><![CDATA[高可用在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。 1) 关闭HBase集群（如果没有开启则跳过此步） $ bin/stop-hbase.sh 2) 在conf目录下创建backup-masters文件 $ touch conf/backup-masters 3) 在backup-masters文件中配置高可用HMaster节点 $ echo hadoop002 &gt; conf/backup-masters 4) 将整个conf目录scp到其他节点 $ scp -r conf/ hadoop002:/opt/software/hbase-1.2.0 $ scp -r conf/ hadoop003:/opt/software/hbase-1.2.0 5) 重新启动HBase后打开页面测试查看 0.98版本之前：http://hadoop001:60010 0.98版本之后：http://hadoop001:16010 Hadoop的通用性优化1) NameNode元数据备份使用SSD 2) 定时备份NameNode上的元数据 每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。 3) 为NameNode指定多个元数据目录 使用dfs.name.dir或者dfs.namenode.name.dir指定。这样可以提供元数据的冗余和健壮性，以免发生故障。 4) NameNode的dir自恢复 设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。 5) HDFS保证RPC调用会有较多的线程数 hdfs-site.xml 属性：dfs.namenode.handler.count 解释：该属性是NameNode服务默认线程数，默认值是10，根据机器的可用内存可以调整为50~100。 属性：dfs.datanode.handler.count 解释：该属性默认值为10，是DataNode的处理线程数，如果HDFS客户端程序读写请求比较多，可以调高到15~20，设置的值越大，内存消耗越多，不要调整的过高，一般业务中，5~10即可。 6) HDFS副本数的调整 hdfs-site.xml 属性：dfs.replication 解释：如果数据量巨大，且不是非常之重要，可以调整为2~3，如果数据非常之重要，可以调整为3~5。 7) HDFS文件块大小的调整 hdfs-site.xml 属性：dfs.blocksize 解释：块大小定义，该属性应该根据存储的大量的单个文件大小来设置，如果大量的单个文件都小于100M，建议设置成64M块大小，对于大于100M或者达到GB的这种情况，建议设置成256M，一般设置范围波动在64M~256M之间。 8) MapReduce Job任务服务线程数调整 mapred-site.xml 属性：mapreduce.jobtracker.handler.count 解释：该属性是Job任务线程数，默认值是10，根据机器的可用内存可以调整为50~100 9) Http服务器工作线程数 mapred-site.xml 属性：mapreduce.tasktracker.http.threads 解释：定义HTTP服务器工作线程数，默认值为40，对于大集群可以调整到80~100 10) 文件排序合并优化 mapred-site.xml 属性：mapreduce.task.io.sort.factor 解释：文件排序时同时合并的数据流的数量，这也定义了同时打开文件的个数，默认值为10，如果调高该参数，可以明显减少磁盘IO，即减少文件读取的次数。 11) 设置任务并发 mapred-site.xml 属性：mapreduce.map.speculative 解释：该属性可以设置任务是否可以并发执行，如果任务多而小，该属性设置为true可以明显加快任务执行效率，但是对于延迟非常高的任务，建议改为false，这就类似于迅雷下载。 12) MR输出数据的压缩 mapred-site.xml 属性：mapreduce.map.output.compress、mapreduce.output.fileoutputformat.compress 解释：对于大集群而言，建议设置Map-Reduce的输出为压缩的数据，而对于小集群，则不需要。 13) 优化Mapper和Reducer的个数 mapred-site.xml 属性： mapreduce.tasktracker.map.tasks.maximum mapreduce.tasktracker.reduce.tasks.maximum 解释：以上两个属性分别为一个单独的Job任务可以同时运行的Map和Reduce的数量。 设置上面两个参数时，需要考虑CPU核数、磁盘和内存容量。假设一个8核的CPU，业务内容非常消耗CPU，那么可以设置map数量为4，如果该业务不是特别消耗CPU类型的，那么可以设置map数量为40，reduce数量为20。这些参数的值修改完成之后，一定要观察是否有较长等待的任务，如果有的话，可以减少数量以加快任务执行，如果设置一个很大的值，会引起大量的上下文切换，以及内存与磁盘之间的数据交换，这里没有标准的配置数值，需要根据业务和硬件配置以及经验来做出选择。 在同一时刻，不要同时运行太多的MapReduce，这样会消耗过多的内存，任务会执行的非常缓慢，我们需要根据CPU核数，内存容量设置一个MR任务并发的最大值，使固定数据量的任务完全加载到内存中，避免频繁的内存和磁盘数据交换，从而降低磁盘IO，提高性能。 大概估算公式 map = 2 + ⅔cpu_core reduce = 2 + ⅓cpu_core Linux优化1) 开启文件系统的预读缓存可以提高读取速度 $ sudo blockdev --setra 32768 /dev/sda ra是readahead的缩写 2) 关闭进程睡眠池 即不允许后台进程进入睡眠状态，如果进程空闲，则直接kill掉释放资源 $ sudo sysctl -w vm.swappiness=0 3) 调整ulimit上限，默认值为比较小的数字 $ ulimit -n 查看允许最大进程数 $ ulimit -u 查看允许打开最大文件数 优化修改 $ sudo vi /etc/security/limits.conf 修改打开文件数限制 末尾添加： * soft nofile 1024000 * hard nofile 1024000 Hive - nofile 1024000 hive - nproc 1024000 $ sudo vi /etc/security/limits.d/90-nproc.conf 修改用户打开进程数限制修改为 #* soft nproc 4096 #root soft nproc unlimited * soft nproc 40960 root soft nproc unlimited 4) 开启集群的时间同步NTP 集群中某台机器同步网络时间服务器的时间，集群中其他机器则同步这台机器的时间。 5) 更新系统补丁 更新补丁前，请先测试新版本补丁对集群节点的兼容性。 Zookeeper优化1) 优化Zookeeper会话超时时间 hbase-site.xml 参数：zookeeper.session.timeout 解释：In hbase-site.xml, set zookeeper.session.timeout to 30 seconds or less to bound failure detection (20-30 seconds is a good start).该值会直接关系到master发现服务器宕机的最大周期，默认值为30秒（不同的HBase版本，该默认值不一样），如果该值过小，会在HBase在写入大量数据发生而GC时，导致RegionServer短暂的不可用，从而没有向ZK发送心跳包，最终导致认为从节点shutdown。一般20台左右的集群需要配置5台zookeeper。 HBase优化预分区每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据索要投放的分区提前大致的规划好，以提高HBase性能。 1) 手动设定预分区 hbase&gt; create &#39;staff&#39;,&#39;info&#39;,&#39;partition1&#39;,SPLITS =&gt; [&#39;1000&#39;,&#39;2000&#39;,&#39;3000&#39;,&#39;4000&#39;] 2) 生成16进制序列预分区 create &#39;staff2&#39;,&#39;info&#39;,&#39;partition2&#39;,{NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#39;HexStringSplit&#39;} 3) 按照文件中设置的规则预分区 创建splits.txt文件内容如下 bbbb aaaa cccc dddd 然后执行 create &#39;staff3&#39;,&#39;partition3&#39;,SPLITS_FILE =&gt; &#39;splits.txt&#39; 4) 使用JavaAPI创建预分区 //自定义算法，产生一系列Hash散列值存储在二维数组中 byte[][] splitKeys = 某个散列值函数 //创建HBaseAdmin实例 HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create()); //创建HTableDescriptor实例 HTableDescriptor tableDesc = new HTableDescriptor(tableName); //通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表 hAdmin.createTable(tableDesc, splitKeys); 3.5.2、RowKey设计 一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。 1) 生成随机数、hash、散列值 比如 原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7 原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd 原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913 在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。 2) 字符串反转 20170524000001转成10000042507102 20170524000002转成20000042507102 这样也可以在一定程度上散列逐步put进来的数据。 3) 字符串拼接 20170524000001_a12e 20170524000001_93i7 内存优化HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。 基础优化1) 允许在HDFS的文件中追加内容 不是不允许追加内容么？没错，请看背景故事 http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/ hdfs-site.xml、hbase-site.xml 属性：dfs.support.append 解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。 2) 优化DataNode允许的最大文件打开数 hdfs-site.xml 属性：dfs.datanode.max.transfer.threads 解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096 3) 优化延迟高的数据操作的等待时间 hdfs-site.xml 属性：dfs.image.transfer.timeout 解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。 4) 优化数据的写入效率 mapred-site.xml 属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec 解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。 5) 优化DataNode存储 hdfs-site.xml 属性：dfs.datanode.failed.volumes.tolerated 解释： 默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。 6) 设置RPC监听数量 hbase-site.xml 属性：hbase.regionserver.handler.count 解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 7) 优化HStore文件大小 hbase-site.xml 属性：hbase.hregion.max.filesize 解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。 8) 优化hbase客户端缓存 hbase-site.xml 属性：hbase.client.write.buffer 解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。 9) 指定scan.next扫描HBase所获取的行数 hbase-site.xml 属性：hbase.client.scanner.caching 解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。 10) flush、compact、split机制 当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。 涉及属性： 即：128M就是Memstore的默认阈值 hbase.hregion.memstore.flush.size：134217728 即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。 hbase.regionserver.global.memstore.upperLimit：0.4 hbase.regionserver.global.memstore.lowerLimit：0.38新版本为none 即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase：5、HBase与Hive集成]]></title>
    <url>%2F2018%2F02%2F02%2Fhbase-5-hbase-hive.html</url>
    <content type="text"><![CDATA[HBase与Hive的对比Hive(1) 数据仓库 Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。 (2) 用于数据分析、清洗 Hive适用于离线的数据分析和清洗，延迟较高。 (3) 基于HDFS、MapReduce Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。 HBase(1) 数据库 是一种面向列存储的非关系型数据库。 (2) 用于存储结构化和非结构话的数据 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。 (3) 基于HDFS 数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。 (4) 延迟较低，接入在线业务使用 面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。 HBase与Hive集成使用HBase与Hive的集成在最新的两个版本中无法兼容。所以，我们只能重新编译：hive-hbase-handler-1.2.0.jar 环境准备 因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。记得还有把zookeeper的jar包考入到hive的lib目录下。 $ export HBASE_HOME=/opt/software/hbase-1.2.0 $ export HIVE_HOME=/opt/software/apache-hive-1.1.0-bin $ ln -s $HBASE_HOME/lib/hbase-common-1.2.0.jar $HIVE_HOME/lib/hbase-common-1.2.0.jar $ ln -s $HBASE_HOME/lib/hbase-server-1.2.0.jar $HIVE_HOME/lib/hbase-server-1.2.0.jar $ ln -s $HBASE_HOME/lib/hbase-client-1.2.0.jar $HIVE_HOME/lib/hbase-client-1.2.0.jar $ ln -s $HBASE_HOME/lib/hbase-protocol-1.2.0.jar $HIVE_HOME/lib/hbase-protocol-1.2.0.jar $ ln -s $HBASE_HOME/lib/hbase-it-1.2.0.jar $HIVE_HOME/lib/hbase-it-1.2.0.jar $ ln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar $ ln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.2.0.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.2.0.jar $ ln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.2.0.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.2.0.jar 同时在hive-site.xml中修改zookeeper的属性 &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001, hadoop002, hadoop003 &lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt; &lt;/property&gt; 1) 案例一 目标：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。 (1) 在Hive中创建表同时关联HBase CREATE TABLE hive_hbase_emp_table( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;); 完成之后，可以分别进入Hive和HBase查看，都生成了对应的表 (2) 在Hive中创建临时中间表，用于load文件中的数据 不能将数据直接load进Hive所关联HBase的那张表中 CREATE TABLE emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) row format delimited fields terminated by &#39;\t&#39;; (3) 向Hive中间表中load数据 hive&gt; load data local inpath &#39;/opt/softwares/data/emp.txt&#39; into table emp; (4) 通过insert命令将中间表中的数据导入到Hive关联HBase的那张表中 hive&gt; insert into table hive_hbase_emp_table select * from emp; (5) 查看Hive以及关联的HBase表中是否已经成功的同步插入了数据 Hive： hive&gt; select * from hive_hbase_emp_table; HBase： hbase&gt; scan ‘hbase_emp_table’ 2) 案例二 目标：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。 注：该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。 (1) 在Hive中创建外部表 CREATE EXTERNAL TABLE relevance_hbase_emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;); (2) 关联后就可以使用Hive函数进行一些分析操作了 hive (default)&gt; select * from relevance_hbase_emp;]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase：4、HBase 使用（二）]]></title>
    <url>%2F2018%2F02%2F01%2Fhbase-4-hbase-use.html</url>
    <content type="text"><![CDATA[HBase读写流程HBase读数据流程1) HRegionServer保存着.META.的这样一张表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取-ROOT-表所在位置（新版本没有，暂不讨论新版本），进而找到.META.表所在的位置信息，即找到这个.META.表在哪个HRegionServer上保存着。 2) 接着Client通过刚才获取到的HRegionServer的IP来访问.META.表所在的HRegionServer，从而读取到.META.，进而获取到.META.表中存放的元数据。 3) Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。 4) 最后HRegionServer把查询到的数据响应给Client。 HBase写数据流程1) Client也是先访问zookeeper，找到-ROOT-表（新版本没有该表，暂不讨论），进而找到.META.表，并获取.META.表信息。 2) 确定当前将要写入的数据所对应的RegionServer服务器和Region。 3) Client向该RegionServer服务器发起写入数据请求，然后RegionServer收到请求并响应。 4) Client先把数据写入到HLog，以防止数据丢失。 5) 然后将数据写入到Memstore。 6) 如果Hlog和Memstore均写入成功，则这条数据写入成功。在此过程中，如果Memstore达到阈值，会把Memstore中的数据flush到StoreFile中。 7) 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile。当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。 提示：因为内存空间是有限的，所以说溢写过程必定伴随着大量的小文件产生。 JavaAPI新建Maven Project新建项目后在pom.xml中添加依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 下面这个依赖不是必须添加 --&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; 编写HBaseAPI这部分的内容，我们先学习使用老版本的API，接着再写出新版本的API调用方式。因为有些时候我们需要一些过时的API来提供更好的兼容性。 首先需要获取Configuration对象 public static Configuration conf; static{ //使用HBaseConfiguration的单例方法实例化 conf = HBaseConfiguration.create(); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.216.20&quot;); conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); } 判断表是否存在 public static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ //在HBase中管理、访问表需要先创建HBaseAdmin对象 //Connection connection = ConnectionFactory.createConnection(conf); //HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); HBaseAdmin admin = new HBaseAdmin(conf); return admin.tableExists(tableName); } 创建表 public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ HBaseAdmin admin = new HBaseAdmin(conf); //判断表是否存在 if(isTableExist(tableName)){ System.out.println(&quot;表&quot; + tableName + &quot;已存在&quot;); //System.exit(0); }else{ //创建表属性对象,表名需要转字节 HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); //创建多个列族 for(String cf : columnFamily){ descriptor.addFamily(new HColumnDescriptor(cf)); } //根据对表的配置，创建表 admin.createTable(descriptor); System.out.println(&quot;表&quot; + tableName + &quot;创建成功！&quot;); } } 删除表 public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ HBaseAdmin admin = new HBaseAdmin(conf); if(isTableExist(tableName)){ admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println(&quot;表&quot; + tableName + &quot;删除成功！&quot;); }else{ System.out.println(&quot;表&quot; + tableName + &quot;不存在！&quot;); } } 向表中插入数据 public static void addRowData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException{ //创建HTable对象 HTable hTable = new HTable(conf, tableName); //向表中插入数据 Put put = new Put(Bytes.toBytes(rowKey)); //向Put对象中组装数据 put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); hTable.put(put); hTable.close(); System.out.println(&quot;插入数据成功&quot;); } 删除多行数据 public static void deleteMultiRow(String tableName, String... rows) throws IOException{ HTable hTable = new HTable(conf, tableName); List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); for(String row : rows){ Delete delete = new Delete(Bytes.toBytes(row)); deleteList.add(delete); } hTable.delete(deleteList); hTable.close(); } 得到所有数据 public static void getAllRows(String tableName) throws IOException{ HTable hTable = new HTable(conf, tableName); //得到用于扫描region的对象 Scan scan = new Scan(); //使用HTable得到resultcanner实现类的对象 ResultScanner resultScanner = hTable.getScanner(scan); for(Result result : resultScanner){ Cell[] cells = result.rawCells(); for(Cell cell : cells){ //得到rowkey System.out.println(&quot;行键:&quot; + Bytes.toString(CellUtil.cloneRow(cell))); //得到列族 System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); } } } 得到某一行所有数据 public static void getRow(String tableName, String rowKey) throws IOException{ HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); //get.setMaxVersions();显示所有版本 //get.setTimeStamp();显示指定时间戳的版本 Result result = table.get(get); for(Cell cell : result.rawCells()){ System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); System.out.println(&quot;时间戳:&quot; + cell.getTimestamp()); } } 获取某一行指定“列族:列”的数据 public static void getRowQualifier(String tableName, String rowKey, String family, String qualifier) throws IOException{ HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier)); Result result = table.get(get); for(Cell cell : result.rawCells()){ System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); } } MapReduce通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。 官方HBase-MapReduce查看HBase的MapReduce任务的所需的依赖 $ bin/hbase mapredcp 执行环境变量的导入 $ export HBASE_HOME=/opt/software/hbase-1.2.0 $ export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp` 运行官方的MapReduce任务 案例一：统计Student表中有多少行数据 $ /opt/software/hadoop-2.6.0//bin/yarn jar /opt/software/hbase-1.2.0/lib/hbase-server-1.2.0.jar rowcounter student 案例二：使用MapReduce将本地数据导入到HBase (1) 在本地创建一个tsv格式的文件：fruit.tsv 1001 Apple Red 1002 Pear Yellow 1003 Pineapple Yellow (2) 创建HBase表 hbase(main):001:0&gt; create &#39;fruit&#39;,&#39;info&#39; (3) 在HDFS中创建input_fruit文件夹并上传fruit.tsv文件 $ /opt/software/hadoop-2.6.0/bin/hdfs dfs -mkdir /input_fruit/ $ /opt/software/hadoop-2.6.0/bin/hdfs dfs -put fruit.tsv /input_fruit/ (4) 执行MapReduce到HBase的fruit表中 $ /opt/software/hadoop-2.6.0/bin/yarn jar lib/hbase-server-1.2.0.jar importtsv \ -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \ hdfs://hadoop001:9000/input_fruit (5) 使用scan命令查看导入后的结果 hbase(main):001:0&gt; scan ‘fruit’ 自定义HBase-MapReduce1目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中。 1) 构建ReadFruitMapper类，用于读取fruit表中的数据 package com.z.hbase_mr; import java.io.IOException; import org.apache.hadoop.hbase.Cell; import org.apache.hadoop.hbase.CellUtil; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.client.Result; import org.apache.hadoop.hbase.io.ImmutableBytesWritable; import org.apache.hadoop.hbase.mapreduce.TableMapper; import org.apache.hadoop.hbase.util.Bytes; public class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; { @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException { //将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。 Put put = new Put(key.get()); //遍历添加column行 for(Cell cell: value.rawCells()){ //添加/克隆列族:info if(&quot;info&quot;.equals(Bytes.toString(CellUtil.cloneFamily(cell)))){ //添加/克隆列：name if(&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){ //将该列cell加入到put对象中 put.add(cell); //添加/克隆列:color }else if(&quot;color&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){ //向该列cell加入到put对象中 put.add(cell); } } } //将从fruit读取到的每行数据写入到context中作为map的输出 context.write(key, put); } } 2) 构建WriteFruitMRReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中 package com.z.hbase_mr; import java.io.IOException; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.io.ImmutableBytesWritable; import org.apache.hadoop.hbase.mapreduce.TableReducer; import org.apache.hadoop.io.NullWritable; public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; { @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException { //读出来的每一行数据写入到fruit_mr表中 for(Put put: values){ context.write(NullWritable.get(), put); } } } 3) 构建Fruit2FruitMRRunner extends Configured implements Tool用于组装运行Job任务 //组装Job public int run(String[] args) throws Exception { //得到Configuration Configuration conf = this.getConf(); //创建Job任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Fruit2FruitMRRunner.class); //配置Job Scan scan = new Scan(); scan.setCacheBlocks(false); scan.setCaching(500); //设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本 TableMapReduceUtil.initTableMapperJob( &quot;fruit&quot;, //数据源的表名 scan, //scan扫描控制器 ReadFruitMapper.class,//设置Mapper类 ImmutableBytesWritable.class,//设置Mapper输出key类型 Put.class,//设置Mapper输出value值类型 job//设置给哪个JOB ); //设置Reducer TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRReducer.class, job); //设置Reduce数量，最少1个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess){ throw new IOException(&quot;Job running with error&quot;); } return isSuccess ? 0 : 1; } 4) 主函数中调用运行该Job任务 public static void main( String[] args ) throws Exception{ Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args); System.exit(status); } 5) 打包运行任务 $ /opt/software/hadoop-2.6.0/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr1.Fruit2FruitMRRunner 运行任务前，如果待数据导入的表不存在，则需要提前创建之。maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin） 自定义HBase-MapReduce2目标：实现将HDFS中的数据写入到HBase表中。 1) 构建ReadFruitFromHDFSMapper于读取HDFS中的文件数据 package com.z.hbase.mr2; import java.io.IOException; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.io.ImmutableBytesWritable; import org.apache.hadoop.hbase.util.Bytes; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //从HDFS中读取的数据 String lineValue = value.toString(); //读取出来的每行数据使用\t进行分割，存于String数组 String[] values = lineValue.split(&quot;\t&quot;); //根据数据中值的含义取值 String rowKey = values[0]; String name = values[1]; String color = values[2]; //初始化rowKey ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey)); //初始化put对象 Put put = new Put(Bytes.toBytes(rowKey)); //参数分别:列族、列、值 put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(name)); put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;color&quot;), Bytes.toBytes(color)); context.write(rowKeyWritable, put); } } 2) 构建WriteFruitMRFromTxtReducer类 package com.z.hbase.mr2; import java.io.IOException; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.io.ImmutableBytesWritable; import org.apache.hadoop.hbase.mapreduce.TableReducer; import org.apache.hadoop.io.NullWritable; public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; { @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException { //读出来的每一行数据写入到fruit_hdfs表中 for(Put put: values){ context.write(NullWritable.get(), put); } } } 3) 创建Txt2FruitRunner组装Job public int run(String[] args) throws Exception { //得到Configuration Configuration conf = this.getConf(); //创建Job任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Txt2FruitRunner.class); Path inPath = new Path(&quot;hdfs://linux01:8020/input_fruit/fruit.tsv&quot;); FileInputFormat.addInputPath(job, inPath); //设置Mapper job.setMapperClass(ReadFruitFromHDFSMapper.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setMapOutputValueClass(Put.class); //设置Reducer TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRFromTxtReducer.class, job); //设置Reduce数量，最少1个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess){ throw new IOException(&quot;Job running with error&quot;); } return isSuccess ? 0 : 1; } 4) 调用执行Job public static void main(String[] args) throws Exception { Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Txt2FruitRunner(), args); System.exit(status); } 5) 打包运行 $ /opt/software/hadoop-2.6.0/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr2.Txt2FruitRunner 运行任务前，如果待数据导入的表不存在，则需要提前创建之。maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase：3、HBase 使用（一）]]></title>
    <url>%2F2018%2F01%2F29%2Fhbase-3-hbase-use.html</url>
    <content type="text"><![CDATA[简单使用基本操作进入HBase客户端命令行 $ bin/hbase shell 查看帮助命令 hbase(main)&gt; help 查看当前数据库中有哪些表 hbase(main)&gt; list 表的操作创建表 hbase(main)&gt; create &#39;student&#39;,&#39;info&#39; 插入数据到表 hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Thomas&#39; hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:sex&#39;,&#39;male&#39; hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;18&#39; hbase(main) &gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:name&#39;,&#39;Janna&#39; hbase(main) &gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:sex&#39;,&#39;female&#39; hbase(main) &gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:age&#39;,&#39;20&#39; 扫描查看表数据 hbase(main) &gt; scan &#39;student&#39; hbase(main) &gt; scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;, STOPROW =&gt; &#39;1001&#39;} hbase(main) &gt; scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;} 查看表结构 hbase(main):012:0&gt; describe ‘student’ 更新指定字段的数据 hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Nick&#39; hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;100&#39; 查看“指定行”或“指定列族:列”的数据 hbase(main) &gt; get &#39;student&#39;,&#39;1001&#39; hbase(main) &gt; get &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39; 删除某rowkey的全部数据 hbase(main) &gt; deleteall &#39;student&#39;,&#39;1001&#39; 删除某rowkey的某一列数据 hbase(main) &gt; delete &#39;student&#39;,&#39;1002&#39;,&#39;info:sex&#39; 清空表数据 hbase(main) &gt; truncate &#39;student&#39; 提示：清空表的操作顺序为先disable，然后再truncating。 删除表 首先需要先让该表为disable状态 hbase(main) &gt; disable &#39;student&#39; 然后才能drop这个表 hbase(main) &gt; drop &#39;student&#39; 提示：如果直接drop表，会报错：Drop the named table. Table must first be disabledERROR: Table student is enabled. Disable it first. 统计表数据行数 hbase(main) &gt; count &#39;student&#39; 变更表信息 将info列族中的数据存放3个版本 hbase(main) &gt; alter &#39;student&#39;,{NAME=&gt;&#39;info&#39;,VERSIONS=&gt;3} 常用Shell操作satus显示服务器状态 hbase&gt; status ‘linux01’ whoami显示HBase当前用户 hbase&gt; whoami list显示当前所有的表 hbase&gt; list count统计指定表的记录数，例如： hbase&gt; count &#39;hbase_book&#39; describe展示表结构信息 hbase&gt; describe &#39;hbase_book&#39; exist检查表是否存在，适用于表量特别多的情况 hbase&gt; exist &#39;hbase_book&#39; is_enabled/is_disabled检查表是否启用或禁用 hbase&gt; is_enabled &#39;hbase_book&#39; hbase&gt; is_disabled &#39;hbase_book&#39; alter该命令可以改变表和列族的模式 为当前表增加列族 hbase&gt; alter &#39;hbase_book&#39;, NAME =&gt; &#39;CF2&#39;, VERSIONS =&gt; 2 为当前表删除列族 hbase&gt; alter &#39;hbase_book&#39;, &#39;delete&#39; =&gt; &#39;CF2&#39; disable禁用一张表 hbase&gt; disable &#39;hbase_book&#39; drop删除一张表，记得在删除表之前必须先禁用 hbase&gt; drop &#39;hbase_book&#39; delete删除一行中一个单元格的值 hbase&gt; delete ‘hbase_book’, ‘rowKey’, ‘CF:C’ truncate清空表数据，即禁用表-删除表-创建表 hbase&gt; truncate &#39;hbase_book&#39; create创建表 hbase&gt; create ‘table’, ‘cf’ 创建多个列族 hbase&gt; create &#39;t1&#39;, {NAME =&gt; &#39;f1&#39;}, {NAME =&gt; &#39;f2&#39;}, {NAME =&gt; &#39;f3&#39;}]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase：2、HBase 部署]]></title>
    <url>%2F2018%2F01%2F28%2Fhbase-2-hbase-cluster.html</url>
    <content type="text"><![CDATA[Zookeeper正常部署首先保证Zookeeper集群的正常部署，并启动 $ bin/zkServer.sh start Hadoop正常部署Hadoop集群的正常部署并启动 $ sbin/start-dfs.sh $ sbin/start-yarn.sh HBase解压解压HBase到指定目录 $ tar -zxf ~/softwares/installations/hbase-1.2.0-bin.tar.gz -C /opt/software/ HBase配置文件hbase-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_121 export HBASE_MANAGES_ZK=false hbase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt; hadoop001:2181, hadoop002:2181, hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/software/zookeeper-3.4.5/zkData&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; regionservers hadoop001 hadoop002 hadoop003 HBase需要依赖的Jar包由于HBase需要依赖Hadoop，所以替换HBase的lib目录下的jar包，以解决兼容问题 1) 删除原有的jar $ rm -rf /opt/software/hbase-1.2.0/lib/hadoop-* $ rm -rf /opt/software/hbase-1.2.0/lib/zookeeper-3.4.6.jar 2) 拷贝新jar，涉及的jar hadoop-annotations-2.6.0.jar hadoop-auth-2.6.0.jar hadoop-client-2.6.0.jar hadoop-common-2.6.0.jar hadoop-hdfs-2.6.0.jar hadoop-mapreduce-client-app-2.6.0.jar hadoop-mapreduce-client-common-2.6.0.jar hadoop-mapreduce-client-core-2.6.0.jar hadoop-mapreduce-client-hs-2.6.0.jar hadoop-mapreduce-client-hs-plugins-2.6.0.jar hadoop-mapreduce-client-jobclient-2.6.0.jar hadoop-mapreduce-client-jobclient-2.6.0-tests.jar hadoop-mapreduce-client-shuffle-2.6.0.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-applications-distributedshell-2.6.0.jar hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar hadoop-yarn-client-2.6.0.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-applicationhistoryservice-2.6.0.jar hadoop-yarn-server-common-2.6.0.jar hadoop-yarn-server-nodemanager-2.6.0.jar hadoop-yarn-server-resourcemanager-2.6.0.jar hadoop-yarn-server-tests-2.6.0.jar hadoop-yarn-server-web-proxy-2.6.0.jar zookeeper-3.4.5.jar 提示：这些jar包的对应版本应替换成你目前使用的hadoop版本，具体情况具体分析。 查找jar包举例 $ find /opt/software/hadoop-2.6.0/ -name hadoop-annotations*然后将找到的jar包复制到HBase的lib目录下即可。 HBase软连接Hadoop配置$ ln -s /opt/software/hadoop-2.6.0/etc/hadoop/core-site.xml /opt/software/hbase-1.2.0/conf/core-site.xml $ ln -s /opt/software/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /opt/software/hbase-1.2.0/conf/hdfs-site.xml HBase远程scp到其他机器$ scp -r /opt/software/hbase-1.2.0/ hadoop002:/opt/software/ $ scp -r /opt/software/hbase-1.2.0/ hadoop003:/opt/software/ HBase服务的启动启动方式1 $ bin/hbase-daemon.sh start master $ bin/hbase-daemon.sh start regionserver 提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。 修复提示 a、同步时间服务 b、属性：hbase.master.maxclockskew设置更大的值 &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt; 启动方式2 $ bin/start-hbase.sh 对应的停止服务 $ bin/stop-hbase.sh 提示：如果使用的是JDK8以上版本，则应在hbase-evn.sh中移除“HBASE_MASTER_OPTS”和“HBASE_REGIONSERVER_OPTS”配置。 查看HBase页面启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如： http://hadoop001:16010]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase：1、HBase 概述]]></title>
    <url>%2F2018%2F01%2F27%2Fhbase-1-hbase-summary.html</url>
    <content type="text"><![CDATA[HBaes介绍HBase的起源HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。 官方网站 2006年Google发表BigTable白皮书。 2006年开始开发HBase。 2008年北京成功开奥运会，程序员默默地将HBase弄成了Hadoop的子项目。 2010年HBase成为Apache顶级项目。 现在很多公司二次开发出了很多发行版本，也开始使用了。 HBase的角色HMaster功能： 1) 监控RegionServer 2) 处理RegionServer故障转移 3) 处理元数据的变更 4) 处理region的分配或移除 5) 在空闲时间进行数据的负载均衡 6) 通过Zookeeper发布自己的位置给客户端 RegionServer功能： 1) 负责存储HBase的实际数据 2) 处理分配给它的Region 3) 刷新缓存到HDFS 4) 维护HLog 5) 执行压缩 6) 负责处理Region分片 组件： 1) Write-Ahead logs HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 2) HFile 这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。 3) Store HFile存储在Store中，一个Store对应HBase表中的一个列族。 4) MemStore 顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。 5) Region Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。 HBase的架构HBase一种是作为存储的分布式文件系统，另一种是作为数据处理模型的MR框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在HDFS直接存储的文件往往不具有结构化，所以催生出了HBase在HDFS上的操作。如果需要查询数据，只需要通过键值便可以成功访问。 架构图如下图所示： HBase内置有Zookeeper，但一般我们会有其他的Zookeeper集群来监管master和regionserver，Zookeeper通过选举，保证任何时候，集群中只有一个活跃的HMaster，HMaster与HRegionServer 启动时会向ZooKeeper注册，存储所有HRegion的寻址入口，实时监控HRegionserver的上线和下线信息。并实时通知给HMaster，存储HBase的schema和table元数据，默认情况下，HBase 管理ZooKeeper 实例，Zookeeper的引入使得HMaster不再是单点故障。一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。 一个RegionServer可以包含多个HRegion，每个RegionServer维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致，请参考如下架构图：]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka：6、Kafka Streams]]></title>
    <url>%2F2018%2F01%2F26%2Fkafka-6-kafka-streams.html</url>
    <content type="text"><![CDATA[概述Kafka StreamsKafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。 Kafka Streams特点1）功能强大 高扩展性，弹性，容错 2）轻量级 无需专门的集群 一个库，而不是框架 3）完全集成 100%的Kafka 0.10.0版本兼容 易于集成到现有的应用程序 4）实时性 毫秒级延迟 并非微批处理 窗口允许乱序数据 允许迟到数据 为什么要有Kafka Stream当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。 既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因。 第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。 第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。 第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。 第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。但是Kafka作为类库不占用系统资源。 第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。 第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。 Kafka Stream数据清洗案例0）需求 实时处理单词带有”&gt;&gt;&gt;”前缀的内容。例如输入”coderf&gt;&gt;&gt;xiaoming”，最终处理成“xiaoming” 1）需求分析 2）案例实操 （1）创建一个工程，并添加jar包 （2）创建主类 package com.coderf.kafka.stream; import java.util.Properties; import org.apache.kafka.streams.KafkaStreams; import org.apache.kafka.streams.StreamsConfig; import org.apache.kafka.streams.processor.Processor; import org.apache.kafka.streams.processor.ProcessorSupplier; import org.apache.kafka.streams.processor.TopologyBuilder; public class Application { public static void main(String[] args) { // 定义输入的topic String from = &quot;first&quot;; // 定义输出的topic String to = &quot;second&quot;; // 设置参数 Properties settings = new Properties(); settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;logFilter&quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;hadoop102:9092&quot;); StreamsConfig config = new StreamsConfig(settings); // 构建拓扑 TopologyBuilder builder = new TopologyBuilder(); builder.addSource(&quot;SOURCE&quot;, from) .addProcessor(&quot;PROCESS&quot;, new ProcessorSupplier&lt;byte[], byte[]&gt;() { @Override public Processor&lt;byte[], byte[]&gt; get() { // 具体分析处理 return new LogProcessor(); } }, &quot;SOURCE&quot;) .addSink(&quot;SINK&quot;, to, &quot;PROCESS&quot;); // 创建kafka stream KafkaStreams streams = new KafkaStreams(builder, config); streams.start(); } } （3）具体业务处理 package com.coderf.kafka.stream; import org.apache.kafka.streams.processor.Processor; import org.apache.kafka.streams.processor.ProcessorContext; public class LogProcessor implements Processor&lt;byte[], byte[]&gt; { private ProcessorContext context; @Override public void init(ProcessorContext context) { this.context = context; } @Override public void process(byte[] key, byte[] value) { String input = new String(value); // 如果包含“&gt;&gt;&gt;”则只保留该标记后面的内容 if (input.contains(&quot;&gt;&gt;&gt;&quot;)) { input = input.split(&quot;&gt;&gt;&gt;&quot;)[1].trim(); // 输出到下一个topic context.forward(&quot;logProcessor&quot;.getBytes(), input.getBytes()); }else{ context.forward(&quot;logProcessor&quot;.getBytes(), input.getBytes()); } } @Override public void punctuate(long timestamp) { } @Override public void close() { } } （4）运行程序 （5）在hadoop003上启动生产者 [root@hadoop003 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic first &gt;hello&gt;&gt;&gt;world &gt;h&gt;&gt;&gt;coderf &gt;hahaha （6）在hadoop002上启动消费者 [root@hadoop002 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop001:2181 --from-beginning --topic second world coderf hahaha]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka：5、Kafka Producer Interceptor]]></title>
    <url>%2F2018%2F01%2F25%2Fkafka-5-kafka-producer-interceptor.html</url>
    <content type="text"><![CDATA[拦截器原理Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。 对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。 Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor， 其定义的方法包括： （1）configure(configs) 获取配置信息和初始化数据时调用。 （2）onSend(ProducerRecord)： 该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算 （3）onAcknowledgement(RecordMetadata, Exception)： 该方法会在消息被应答或消息发送失败时调用，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率 （4）close： 关闭interceptor，主要用于执行一些资源清理工作 如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 拦截器案例1）需求： 实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。 2）案例实操 （1）增加时间戳拦截器 package com.coderf.kafka.interceptor; import java.util.Map; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; { @Override public void configure(Map&lt;String, ?&gt; configs) { } @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { // 创建一个新的record，把时间戳写入消息体的最前部 return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + &quot;,&quot; + record.value().toString()); } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { } @Override public void close() { } } （2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器 package com.coderf.kafka.interceptor; import java.util.Map; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;{ private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map&lt;String, ?&gt; configs) { } @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { return record; } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { // 统计成功和失败的次数 if (exception == null) { successCounter++; } else { errorCounter++; } } @Override public void close() { // 保存结果 System.out.println(&quot;Successful sent: &quot; + successCounter); System.out.println(&quot;Failed sent: &quot; + errorCounter); } } （3）producer主程序 package com.coderf.kafka.interceptor; import java.util.ArrayList; import java.util.List; import java.util.Properties; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; public class InterceptorProducer { public static void main(String[] args) throws Exception { // 1 设置配置信息 Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); props.put(&quot;acks&quot;, &quot;all&quot;); props.put(&quot;retries&quot;, 0); props.put(&quot;batch.size&quot;, 16384); props.put(&quot;linger.ms&quot;, 1); props.put(&quot;buffer.memory&quot;, 33554432); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 2 构建拦截链 List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(&quot;com.coderf.kafka.interceptor.TimeInterceptor&quot;); interceptors.add(&quot;com.coderf.kafka.interceptor.CounterInterceptor&quot;); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = &quot;first&quot;; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 3 发送消息 for (int i = 0; i &lt; 10; i++) { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i); producer.send(record); } // 4 一定要关闭producer，这样才会调用interceptor的close方法 producer.close(); } } 3）测试 （1）在kafka上启动消费者，然后运行客户端java程序。 [root@hadoop001 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop001:2181 --from-beginning --topic first 1501904047034,message0 1501904047225,message1 1501904047230,message2 1501904047234,message3 1501904047236,message4 1501904047240,message5 1501904047243,message6 1501904047246,message7 1501904047249,message8 1501904047252,message9 （2）观察java平台控制台输出数据如下 Successful sent: 10 Failed sent: 0]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka：4、Kafka API]]></title>
    <url>%2F2018%2F01%2F23%2Fkafka-4-kafka-api.html</url>
    <content type="text"><![CDATA[Kafka生产过程分析写入方式producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。 分区（Partition）Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。 Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。 消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示： 下图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。 我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。 发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。 1） 分区的原因 （1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了； （2）可以提高并发，因为可以以Partition为单位读写了。 传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。 Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。 2）分区的原则 （1）指定了patition，则直接使用 （2）未指定patition但指定key，通过对key的value进行hash出一个patition （3）patition和key都未指定，使用轮询选出一个patition DefaultPartitioner类 public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) { int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) { int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); } else { // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; } } else { // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; } } 副本（Replication）同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。 写入流程producer写入消息流程如下 1）producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader 2）producer将消息发送给该leader 3）leader将消息写入本地log 4）followers从leader pull消息，写入本地log后向leader发送ACK 5）leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK Broker保存消息存储方式物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下： [root@hadoop001 logs]$ ll drwxrwxr-x. 2 root root 4096 8月 6 14:37 first-0 drwxrwxr-x. 2 root root 4096 8月 6 14:35 first-1 drwxrwxr-x. 2 root root 4096 8月 6 14:37 first-2 [root@hadoop001 logs]$ cd first-0 [root@hadoop001 first-0]$ ll -rw-rw-r--. 1 root root 10485760 8月 6 14:33 00000000000000000000.index -rw-rw-r--. 1 root root 219 8月 6 15:07 00000000000000000000.log -rw-rw-r--. 1 root root 10485756 8月 6 14:33 00000000000000000000.timeindex -rw-rw-r--. 1 root root 8 8月 6 14:37 leader-epoch-checkpoint 存储策略无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据： 1）基于时间：log.retention.hours=168 2）基于大小：log.retention.bytes=1073741824 需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。 Zookeeper存储结构 注意：producer不在zk中注册，消费者在zk中注册 Kafka消费过程kafka提供了两套consumer API：高级Consumer API和低级API。 消费模型消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。 基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。 Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。 在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉。 高级API1）高级API优点 高级API 写起来简单 不需要自行去管理offset，系统通过zookeeper自行管理。 不需要管理分区，副本等情况，.系统自动管理。 消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset） 可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响） 2）高级API缺点 不能自行控制offset（对于某些特殊需求来说） 不能细化控制如分区、副本、zk等 低级API1）低级 API 优点 能够让开发者自己控制offset，想从哪里读取就从哪里读取。 自行控制连接分区，对分区自定义进行负载均衡 对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中） 2）低级API缺点 太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。 消费者组 消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。 在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。 消费方式consumer采用pull（拉）模式从broker中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。 消费者组案例1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。 2）案例实操 （1）在hadoop002、hadoop003上修改/opt/software/kafka/config/consumer.properties配置文件中的group.id属性为任意组名。 [root@hadoop002 config]$ vi consumer.properties group.id=coderf （2）在hadoop002、hadoop003上分别启动消费者 [root@ hadoop002 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop002:2181 --topic first --consumer.config config/consumer.properties [root@ hadoop003 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop002:2181 --topic first --consumer.config config/consumer.properties （3）在hadoop001上启动生产者 [root@hadoop001 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop002:9092 --topic first &gt;hello world （4）查看hadoop002和hadoop003的接收者。 同一时刻只有一个消费者接收到消息。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka：3、Kafka 工作流程]]></title>
    <url>%2F2018%2F01%2F21%2Fkafka-3-kafka-workflow.html</url>
    <content type="text"><![CDATA[Kafka生产过程分析写入方式producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。 分区（Partition）Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。 Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。 消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示： 下图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。 我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。 发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。 1） 分区的原因 （1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了； （2）可以提高并发，因为可以以Partition为单位读写了。 传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。 Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。 2）分区的原则 （1）指定了patition，则直接使用 （2）未指定patition但指定key，通过对key的value进行hash出一个patition （3）patition和key都未指定，使用轮询选出一个patition DefaultPartitioner类 public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) { int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) { int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); } else { // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; } } else { // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; } } 副本（Replication）同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。 写入流程producer写入消息流程如下 1）producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader 2）producer将消息发送给该leader 3）leader将消息写入本地log 4）followers从leader pull消息，写入本地log后向leader发送ACK 5）leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK Broker保存消息存储方式物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下： [root@hadoop001 logs]$ ll drwxrwxr-x. 2 root root 4096 8月 6 14:37 first-0 drwxrwxr-x. 2 root root 4096 8月 6 14:35 first-1 drwxrwxr-x. 2 root root 4096 8月 6 14:37 first-2 [root@hadoop001 logs]$ cd first-0 [root@hadoop001 first-0]$ ll -rw-rw-r--. 1 root root 10485760 8月 6 14:33 00000000000000000000.index -rw-rw-r--. 1 root root 219 8月 6 15:07 00000000000000000000.log -rw-rw-r--. 1 root root 10485756 8月 6 14:33 00000000000000000000.timeindex -rw-rw-r--. 1 root root 8 8月 6 14:37 leader-epoch-checkpoint 存储策略无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据： 1）基于时间：log.retention.hours=168 2）基于大小：log.retention.bytes=1073741824 需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。 Zookeeper存储结构 注意：producer不在zk中注册，消费者在zk中注册 Kafka消费过程kafka提供了两套consumer API：高级Consumer API和低级API。 消费模型消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。 基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。 Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。 在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉。 高级API1）高级API优点 高级API 写起来简单 不需要自行去管理offset，系统通过zookeeper自行管理。 不需要管理分区，副本等情况，.系统自动管理。 消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset） 可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响） 2）高级API缺点 不能自行控制offset（对于某些特殊需求来说） 不能细化控制如分区、副本、zk等 低级API1）低级 API 优点 能够让开发者自己控制offset，想从哪里读取就从哪里读取。 自行控制连接分区，对分区自定义进行负载均衡 对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中） 2）低级API缺点 太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。 消费者组 消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。 在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。 消费方式consumer采用pull（拉）模式从broker中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。 消费者组案例1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。 2）案例实操 （1）在hadoop002、hadoop003上修改/opt/software/kafka/config/consumer.properties配置文件中的group.id属性为任意组名。 [root@hadoop002 config]$ vi consumer.properties group.id=coderf （2）在hadoop002、hadoop003上分别启动消费者 [root@ hadoop002 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop002:2181 --topic first --consumer.config config/consumer.properties [root@ hadoop003 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop002:2181 --topic first --consumer.config config/consumer.properties （3）在hadoop001上启动生产者 [root@hadoop001 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop002:9092 --topic first &gt;hello world （4）查看hadoop002和hadoop003的接收者。 同一时刻只有一个消费者接收到消息。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka：2、Kafka 集群部署]]></title>
    <url>%2F2018%2F01%2F20%2Fkafka-2-kafka-cluster.html</url>
    <content type="text"><![CDATA[环境准备集群规划 hadoop001 hadoop002 hadoop003 zk zk zk kafka kafka kafka 安装包下载Kafka安装包 安装JDK（略）安装Zookeeper解压安装解压zookeeper安装包到/opt/software/目录下 [root@hadoop001 apps]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/software/ 在/opt/software/zookeeper-3.4.10/这个目录下创建zkData mkdir -p zkData 重命名/opt/software/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg mv zoo_sample.cfg zoo.cfg 配置zoo.cfg文件具体配置 dataDir=/opt/software/zookeeper-3.4.10/zkData 增加如下配置 #######################cluster########################## server.1=hadoop001:2888:3888 server.2=hadoop002:2888:3888 server.3=hadoop003:2888:3888 配置参数解读 Server.A=B:C:D A是一个数字，表示这个是第几号服务器； B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 集群操作在/opt/software/zookeeper-3.4.10/zkData目录下创建一个myid的文件 touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 编辑myid文件 vi myid 在文件中添加与server对应的编号：如1 拷贝配置好的zookeeper到其他机器上 scp -r zookeeper-3.4.10/ root@hadoop002:/opt/software/ scp -r zookeeper-3.4.10/ root@hadoop003:/opt/software/ 并分别修改myid文件中内容为2、3 分别启动zookeeper [root@hadoop001 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop002 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop003 zookeeper-3.4.10]# bin/zkServer.sh start 查看状态 [root@hadoop001 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/software/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop002 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/software/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop003 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/software/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower Kafka集群部署解压安装包 [root@hadoop001 apps]$ tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/software/ 修改解压后的文件名称 [root@hadoop001 software]$ mv kafka_2.11-0.11.0.0/ kafka 在/opt/software/kafka目录下创建logs文件夹 [root@hadoop001 kafka]$ mkdir logs 修改配置文件 [root@hadoop001 kafka]$ cd config/ [root@hadoop001 config]$ vi server.properties 输入以下内容 #broker的全局唯一编号，不能重复 broker.id=0 #是否允许删除topic delete.topic.enable=true #处理网络请求的线程数量 num.network.threads=3 #用来处理磁盘IO的线程数量 num.io.threads=8 #发送套接字的缓冲区大小 socket.send.buffer.bytes=102400 #接收套接字的缓冲区大小 socket.receive.buffer.bytes=102400 #请求套接字的最大缓冲区大小 socket.request.max.bytes=104857600 #kafka运行日志存放的路径 log.dirs=/opt/software/kafka/logs #topic在当前broker上的分区个数 num.partitions=1 #用来恢复和清理data下数据的线程数量 num.recovery.threads.per.data.dir=1 #segment文件保留的最长时间，超时将被删除 log.retention.hours=168 #配置连接Zookeeper集群地址 zookeeper.connect=hadoop001:2181,hadoop002:2181,hadoop003:2181 配置环境变量 [root@hadoop001 software]# vi /etc/profile #KAFKA_HOME export KAFKA_HOME=/opt/software/kafka export PATH=$PATH:$KAFKA_HOME/bin [root@hadoop001 module]# source /etc/profile 分发安装包 scp -r /opt/software/kafka root@hadoop002:/opt/software/ scp -r /opt/software/kafka root@hadoop003:/opt/software/ 分别在hadoop002和hadoop003上修改配置文件/opt/software/kafka/config/server.properties中的broker.id=1、broker.id=2 注：broker.id不得重复 启动集群 依次在hadoop001、hadoop002、hadoop003节点上启动kafka [root@hadoop001 kafka]$ bin/kafka-server-start.sh config/server.properties &amp; [root@hadoop002 kafka]$ bin/kafka-server-start.sh config/server.properties &amp; [root@hadoop003 kafka]$ bin/kafka-server-start.sh config/server.properties &amp; 关闭集群 [root@hadoop001 kafka]$ bin/kafka-server-stop.sh stop [root@hadoop002 kafka]$ bin/kafka-server-stop.sh stop [root@hadoop003 kafka]$ bin/kafka-server-stop.sh stop Kafka命令行操作查看当前服务器中的所有topic [root@hadoop001 kafka]$ bin/kafka-topics.sh --zookeeper hadoop001:2181 --list 创建topic [root@hadoop001 kafka]$ bin/kafka-topics.sh --zookeeper hadoop001:2181 --create --replication-factor 3 --partitions 1 --topic first 选项说明： –topic 定义topic名 –replication-factor 定义副本数 –partitions 定义分区数 删除topic [root@hadoop001 kafka]$ bin/kafka-topics.sh --zookeeper hadoop001:2181 --delete --topic first 需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。 发送消息 [root@hadoop001 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic first &gt;hello world &gt;atguigu atguigu 消费消息 [root@hadoop002 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop001:2181 --from-beginning --topic first –from-beginning：会把first主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。 查看某个Topic的详情 [root@hadoop001 kafka]$ bin/kafka-topics.sh --zookeeper hadoop001:2181 --describe --topic first Kafka配置信息Broker配置信息 Producer配置信息 Consumer配置信息]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka：1、Kafka 概述]]></title>
    <url>%2F2018%2F01%2F18%2Fkafka-1-kafka-summary.html</url>
    <content type="text"><![CDATA[Kafka是什么在流式计算中，Kafka一般用来缓存数据，Storm、Spark Streaming通过消费Kafka的数据进行计算。 1）Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。 2）Kafka最初是由LinkedIn公司开发，并于 2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。 3）Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。 4）无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。 消息队列内部实现原理 1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除） 点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。 2）发布/订阅模式（一对多，数据生产后，推送给所有订阅者） 发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。 为什么需要消息队列1）解耦： 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 2）冗余： 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 3）扩展性： 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。 4）灵活性 &amp; 峰值处理能力： 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 5）可恢复性： 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 6）顺序保证： 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性） 7）缓冲： 有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 8）异步通信： 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 Kafka架构 1）Producer ： 消息生产者，就是向kafka broker发消息的客户端。 2）Consumer ： 消息消费者，向kafka broker取消息的客户端 3）Topic ： 可以理解为一个队列。 4） Consumer Group （CG）： 这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制-给consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。 5）Broker ： 一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。 6）Partition： 为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。 7）Offset： kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka 分布式模型Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本（Leader），其他节点作为备份副本（Follower，也叫作从副本）。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本出现故障时，备份副本中的一个副本会被选择为新的主副本。因为每个分区的副本中只有主副本接受读写，所以每个服务器端都会作为某些分区的主副本，以及另外一些分区的备份副本，这样Kafka集群的所有服务端整体上对客户端是负载均衡的。 Kafka的生产者和消费者相对于服务器端而言都是客户端。 Kafka生产者客户端发布消息到服务端的指定主题，会指定消息所属的分区。生产者发布消息时根据消息是否有键，采用不同的分区策略。消息没有键时，通过轮询方式进行客户端负载均衡；消息有键时，根据分区语义（例如hash）确保相同键的消息总是发送到同一分区。 Kafka的消费者通过订阅主题来消费消息，并且每个消费者都会设置一个消费组名称。因为生产者发布到主题的每一条消息都只会发送给消费者组的一个消费者。所以，如果要实现传统消息系统的“队列”模型，可以让每个消费者都拥有相同的消费组名称，这样消息就会负责均衡到所有的消费者；如果要实现“发布-订阅”模型，则每个消费者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。 分区是消费者现场模型的最小并行单位。如下图（图1）所示，生产者发布消息到一台服务器的3个分区时，只有一个消费者消费所有的3个分区。在下图（图2）中，3个分区分布在3台服务器上，同时有3个消费者分别消费不同的分区。假设每个服务器的吞吐量时300MB，在下图（图1）中分摊到每个分区只有100MB，而在下图（图2）中，集群整体的吞吐量有900MB。可以看到，增加服务器节点会提升集群的性能，增加消费者数量会提升处理性能。 同一个消费组下多个消费者互相协调消费工作，Kafka会将所有的分区平均地分配给所有的消费者实例，这样每个消费者都可以分配到数量均等的分区。Kafka的消费组管理协议会动态地维护消费组的成员列表，当一个新消费者加入消费者组，或者有消费者离开消费组，都会触发再平衡操作。 Kafka的消费者消费消息时，只保证在一个分区内的消息的完全有序性，并不保证同一个主题汇中多个分区的消息顺序。而且，消费者读取一个分区消息的顺序和生产者写入到这个分区的顺序是一致的。比如，生产者写入“hello”和“Kafka”两条消息到分区P1，则消费者读取到的顺序也一定是“hello”和“Kafka”。如果业务上需要保证所有消息完全一致，只能通过设置一个分区完成，但这种做法的缺点是最多只能有一个消费者进行消费。一般来说，只需要保证每个分区的有序性，再对消息假设键来保证相同键的所有消息落入同一分区，就可以满足绝大多数的应用。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：10、Hive 元数据]]></title>
    <url>%2F2018%2F01%2F16%2Fhive-10-hive-metadata.html</url>
    <content type="text"><![CDATA[Hive元数据Hive的原数据表，默认是存储在derby中的，但是我们一般会修改为mysql。 hive-site.xml&lt;?xml version=&quot;1.0&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 查看hive元数据涉及的表mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | coderf | | hive | | mysql | | performance_schema | | test | +--------------------+ 6 rows in set (0.00 sec) mysql&gt; use hive; Database changed mysql&gt; show tables; +---------------------------+ | Tables_in_hive | +---------------------------+ | bucketing_cols | | cds | | columns_v2 | | database_params | | dbs | | func_ru | | funcs | | global_privs | | part_col_stats | | partition_key_vals | | partition_keys | | partition_params | | partitions | | roles | | sd_params | | sds | | sequence_table | | serde_params | | serdes | | skewed_col_names | | skewed_col_value_loc_map | | skewed_string_list | | skewed_string_list_values | | skewed_values | | sort_cols | | tab_col_stats | | table_params | | tbls | | version | +---------------------------+ 29 rows in set (0.00 sec) Hive版本表（version）存储Hive版本的元数据表，如果该表出现问题，根本进入不了Hive-Cli。比如该表不存在，当启动Hive-Cli时候，就会报错”Table ‘hive.version’ doesn’t exist”。 mysql&gt; select * from version; +--------+----------------+-----------------------------------------+ | VER_ID | SCHEMA_VERSION | VERSION_COMMENT | +--------+----------------+-----------------------------------------+ | 1 | 1.1.0 | Set by MetaStore mysqladmin@192.168.0.2 | +--------+----------------+-----------------------------------------+ 1 row in set (0.00 sec) Hive数据库相关的元数据表（DBS、DATABASE_PARAMS）DBSmysql&gt; select * from dbs; +-------+-----------------------+--------------------------------------+---------+------------+------------+ | DB_ID | DESC | DB_LOCATION_URI | NAME | OWNER_NAME | OWNER_TYPE | +-------+-----------------------+--------------------------------------+---------+------------+------------+ | 1 | Default Hive database | hdfs://mycluster/user/hive/warehouse | default | public | ROLE | +-------+-----------------------+--------------------------------------+---------+------------+------------+ 1 row in set (0.00 sec) DB_ID：数据库ID DESC：数据库描述信息 DB_LOCATION_URI：数据库HDFS路径 NAME：数据库名称 OWNER_NAME：数据库所有者用户名 OWNER_TYPE：数据库所有者角色 DATABASE_PARAMS该表存储数据库的相关参数，在CREATE DATABASE时候用WITH DBPROPERTIES (property_name=property_value, …)指定的参数。 元数据表字段 说明 示例数据 DB_ID 数据库ID 1 PARAM_KEY 参数名 createdby PARAM_VALUE 参数值 coderf Hive表和视图相关的元数据表TBLSmysql&gt; select * from tbls; +--------+-------------+-------+------------------+-------+-----------+-------+-------------------+---------------+--------------------+--------------------+ | TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER | RETENTION | SD_ID | TBL_NAME | TBL_TYPE | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT | +--------+-------------+-------+------------------+-------+-----------+-------+-------------------+---------------+--------------------+--------------------+ | 1 | 1518415174 | 1 | 0 | root | 0 | 1 | helloworld | MANAGED_TABLE | NULL | NULL | | 6 | 1520056550 | 1 | 0 | root | 0 | 6 | page_views | MANAGED_TABLE | NULL | NULL | | 11 | 1520064829 | 1 | 0 | root | 0 | 11 | page_views_snappy | MANAGED_TABLE | NULL | NULL | | 16 | 1520444005 | 1 | 0 | root | 0 | 16 | lzo | MANAGED_TABLE | NULL | NULL | +--------+-------------+-------+------------------+-------+-----------+-------+-------------------+---------------+--------------------+--------------------+ 4 rows in set (0.00 sec) TBL_ID：表ID CREATE_TIME：创建时间 DB_ID：数据库ID LAST_ACCESS_TIME：上次访问时间 OWNER：所有者 RETENTION：保留字段 SD_ID：序列号配置信息（对应SDS表中的SD_ID） TBL_NAME：表名 TBL_TYPE：表类型 VIEW_EXPANDED_TEXT：视图的详细HQL语句 VIEW_ORIGINAL_TEXT：视图的原始HQL语句 TABLE_PARAMSmysql&gt; select * from table_params; +--------+-----------------------+-------------+ | TBL_ID | PARAM_KEY | PARAM_VALUE | +--------+-----------------------+-------------+ | 1 | COLUMN_STATS_ACCURATE | true | | 1 | numFiles | 1 | | 1 | numRows | 0 | | 1 | rawDataSize | 0 | | 1 | totalSize | 27 | | 1 | transient_lastDdlTime | 1518459827 | | 6 | COLUMN_STATS_ACCURATE | true | | 6 | numFiles | 1 | | 6 | numRows | 0 | | 6 | rawDataSize | 0 | | 6 | totalSize | 19014993 | | 6 | transient_lastDdlTime | 1520056601 | | 11 | COLUMN_STATS_ACCURATE | true | | 11 | numFiles | 1 | | 11 | numRows | 100000 | | 11 | rawDataSize | 18914993 | | 11 | totalSize | 8813722 | | 11 | transient_lastDdlTime | 1520064829 | | 16 | transient_lastDdlTime | 1520444005 | +--------+-----------------------+-------------+ 19 rows in set (0.00 sec) TBL_ID：表ID PARAM_KEY：属性名 PARAM_VALUE：属性值 TBL_PRIVS该表存储表、视图的授权信息 Hive文件存储信息相关的元数据表由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。 SDS文件存储的基本信息 mysql&gt; select * from sds; +-------+-------+------------------------------------------------+---------------+---------------------------+--------------------------------------------------------+-------------+------------------------------------------------------------+----------+ | SD_ID | CD_ID | INPUT_FORMAT | IS_COMPRESSED | IS_STOREDASSUBDIRECTORIES | LOCATION | NUM_BUCKETS | OUTPUT_FORMAT | SERDE_ID | +-------+-------+------------------------------------------------+---------------+---------------------------+--------------------------------------------------------+-------------+------------------------------------------------------------+----------+ | 1 | 1 | org.apache.hadoop.mapred.TextInputFormat | | | hdfs://mycluster/user/hive/warehouse/helloworld | -1 | org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat | 1 | | 6 | 6 | org.apache.hadoop.mapred.TextInputFormat | | | hdfs://mycluster/user/hive/warehouse/page_views | -1 | org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat | 6 | | 11 | 11 | org.apache.hadoop.mapred.TextInputFormat | | | hdfs://mycluster/user/hive/warehouse/page_views_snappy | -1 | org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat | 11 | | 16 | 16 | com.hadoop.mapred.DeprecatedLzoTextInputFormat | | | hdfs://mycluster/user/hive/warehouse/lzo | -1 | org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat | 16 | +-------+-------+------------------------------------------------+---------------+---------------------------+--------------------------------------------------------+-------------+------------------------------------------------------------+----------+ 4 rows in set (0.00 sec) SD_ID：存储信息ID（对应tbl_id） CD_ID：字段信息ID（对应column_v2 的CD_ID） INPUT_FORMAT：文件输入格式 IS_COMPRESSED：是否压缩 IS_STOREDASSUBDIRECTORIES：是否以子目录存储 LOCATION：HDFS路径 NUM_BUCKETS：分桶数量 OUTPUT_FORMAT：文件输出格式 SERDE_ID：序列化类ID SD_PARAMS该表存储Hive存储的属性信息，在创建表时候使用STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)指定。 SERDES存储序列化使用的类信息 mysql&gt; select * from serdes; +----------+------+----------------------------------------------------+ | SERDE_ID | NAME | SLIB | +----------+------+----------------------------------------------------+ | 1 | NULL | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | | 6 | NULL | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | | 11 | NULL | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | | 16 | NULL | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | +----------+------+----------------------------------------------------+ 4 rows in set (0.00 sec) SERDE_ID：序列化类配置ID（对应SDS的 SERDE_ID） NAME：序列化类别名 SLIB：序列化类 SERDE_PARAMS存储序列化的一些属性、格式信息,比如：行、列分隔符 mysql&gt; select * from serde_params; +----------+----------------------+-------------+ | SERDE_ID | PARAM_KEY | PARAM_VALUE | +----------+----------------------+-------------+ | 1 | field.delim | | | 1 | serialization.format | | | 6 | field.delim | | | 6 | serialization.format | | | 11 | field.delim | | | 11 | serialization.format | | | 16 | serialization.format | 1 | +----------+----------------------+-------------+ 7 rows in set (0.01 sec) SERDE_ID：序列化类配置ID（对应SDS的 SERDE_ID） PARAM_KEY：属性名 PARAM_VALUE：属性值 Hive表字段相关的元数据表columns_v2表的字段信息 mysql&gt; select * from columns_v2; +-------+---------+-------------+-----------+-------------+ | CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME | INTEGER_IDX | +-------+---------+-------------+-----------+-------------+ | 1 | NULL | id | int | 0 | | 1 | NULL | name | string | 1 | | 6 | NULL | city_id | string | 6 | | 6 | NULL | end_user_id | string | 5 | | 6 | NULL | ip | string | 4 | | 6 | NULL | referer | string | 3 | | 6 | NULL | session_id | string | 2 | | 6 | NULL | track_time | string | 0 | | 6 | NULL | url | string | 1 | | 11 | NULL | city_id | string | 6 | | 11 | NULL | end_user_id | string | 5 | | 11 | NULL | ip | string | 4 | | 11 | NULL | referer | string | 3 | | 11 | NULL | session_id | string | 2 | | 11 | NULL | track_time | string | 0 | | 11 | NULL | url | string | 1 | | 16 | NULL | id | int | 0 | | 16 | NULL | name | string | 1 | +-------+---------+-------------+-----------+-------------+ 18 rows in set (0.00 sec) CD_ID：字段信息ID（对应SDS 表CD_ID） COMMENT：字段注释 COLUMN_NAME：字段名 TYPE_NAME：字段类型 INTEGER_IDX：字段顺序 Hive表分区相关的元数据表主要涉及PARTITIONS、PARTITION_KEYS、PARTITION_KEY_VALS、 PARTITION_PARAMS PARTITIONS分区的基本信息 mysql&gt; select * from partitions ; +---------+-------------+------------------+--------------+-------+--------+ | PART_ID | CREATE_TIME | LAST_ACCESS_TIME | PART_NAME | SD_ID | TBL_ID | +---------+-------------+------------------+--------------+-------+--------+ | 1 | 1529474368 | 0 | month=201709 | 23 | 22 | | 2 | 1529474481 | 0 | month=201708 | 24 | 22 | +---------+-------------+------------------+--------------+-------+--------+ 2 rows in set (0.00 sec) PART_ID：分区ID CREATE_TIME：分区创建时间 LAST_ACCESS_TIME：最后一次访问时间 PART_NAME：分区名 SD_ID：分区存储ID TBL_ID：表ID PARTITION_KEYS分区的字段信息 mysql&gt; select * from partition_keys; +--------+--------------+-------------+-----------+-------------+ | TBL_ID | PKEY_COMMENT | PKEY_NAME | PKEY_TYPE | INTEGER_IDX | +--------+--------------+-------------+-----------+-------------+ | 21 | NULL | event_month | string | 0 | | 22 | NULL | month | string | 0 | +--------+--------------+-------------+-----------+-------------+ 2 rows in set (0.00 sec) TBL_ID：表ID PKEY_COMMENT：分区字段说明 PKEY_NAME：分区字段名 PKEY_TYPE：分区字段类型 INTEGER_IDX：分区字段顺序 PARTITION _KEY _VALS分区字段值 mysql&gt; select * from partition_key_vals; +---------+--------------+-------------+ | PART_ID | PART_KEY_VAL | INTEGER_IDX | +---------+--------------+-------------+ | 1 | 201709 | 0 | | 2 | 201708 | 0 | +---------+--------------+-------------+ 2 rows in set (0.00 sec) PART_ID：分区ID PART_KEY_VAL：分区字段值 INTEGER_IDX：分区字段值顺序 PARTITION_PARAMS分区的属性信息 mysql&gt; select * from partition_params; +---------+-----------------------+-------------+ | PART_ID | PARAM_KEY | PARAM_VALUE | +---------+-----------------------+-------------+ | 1 | COLUMN_STATS_ACCURATE | true | | 1 | numFiles | 1 | | 1 | numRows | 0 | | 1 | rawDataSize | 0 | | 1 | totalSize | 258 | | 1 | transient_lastDdlTime | 1529474368 | | 2 | COLUMN_STATS_ACCURATE | true | | 2 | numFiles | 1 | | 2 | numRows | 0 | | 2 | rawDataSize | 0 | | 2 | totalSize | 258 | | 2 | transient_lastDdlTime | 1529474481 | +---------+-----------------------+-------------+ 12 rows in set (0.00 sec) PART_ID：分区ID PARAM_KEY：分区属性名 PARAM_VALUE：分区属性值 其他元数据表DB_PRIVS：数据库权限信息表。通过GRANT语句对数据库授权后，将会在这里存储 IDXS：索引表，存储Hive索引相关的元数据 INDEX_PARAMS ：索引相关的属性信息 TAB_COL_STATS：表字段的统计信息，使用ANALYZE语句对表字段分析后记录在这里 TBL_COL_PRIVS：表字段的授权信息 PART_PRIVS：分区的授权信息 PART_COL_STATS：分区字段的统计信息 PART_COL_PRIVS：分区字段的权限信息 FUNCS：用户注册的函数信息 FUNC_RU：用户注册函数的资源信息]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：9、Hive 文件存储格式]]></title>
    <url>%2F2018%2F01%2F15%2Fhive-9-hive-storage-format.html</url>
    <content type="text"><![CDATA[Hadoop压缩详解 PREDICATE PUSH DOWN（PPD谓词下压） 文件存储格式Hive支持的存储格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。 列式存储和行式存储 上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。 行存储的特点查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。 列存储的特点因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；ORC和PARQUET是基于列式存储的； TEXTFILE格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。 ORC格式Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。 可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer： 1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引只是记录某行的各字段在Row Data中的offset。 2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。 3）Stripe Footer：存的是各个Stream的类型，长度等信息。 每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。 PARQUET格式Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。 Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。 通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。 上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。 主流文件存储格式对比实验从存储文件的压缩比和查询速度两个角度对比。 存储文件的压缩比测试1）TextFile （1）创建表，存储数据格式为TEXTFILE hive&gt; create table log_text ( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS TEXTFILE; （2）向表中加载数据 hive&gt; load data local inpath &#39;log.data&#39; into table log_text ; （3）查看表中数据大小 hive&gt; dfs -du -h /user/hive/warehouse/log_text; 18.1 M /user/hive/warehouse/log_text/log.data 2）ORC （1）创建表，存储数据格式为ORC hive&gt; create table log_orc( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS orc; （2）向表中加载数据 hive&gt; insert into table log_orc select * from log_text; （3）查看表中数据大小 hive&gt; dfs -du -h /user/hive/warehouse/log_orc/; 2.8 M /user/hive/warehouse/log_orc/123456_0 3）Parquet （1）创建表，存储数据格式为parquet hive&gt; create table log_parquet( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS PARQUET; （2）向表中加载数据 hive&gt; insert into table log_parquet select * from log_text; （3）查看表中数据大小 hive&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ; 13.1 M /user/hive/warehouse/log_parquet/123456_0 存储文件的压缩比总结ORC &gt; Parquet &gt; textFile 存储文件的查询速度测试1）TextFile hive&gt; select count(*) from log_text; _c0 100000 Time taken: 21.54 seconds, Fetched: 1 row(s) 2）ORC hive&gt; select count(*) from log_orc; _c0 100000 Time taken: 20.867 seconds, Fetched: 1 row(s) 3）Parquet hive&gt; select count(*) from log_parquet; _c0 100000 Time taken: 22.922 seconds, Fetched: 1 row(s) 存储文件的查询速度总结ORC &gt; TextFile &gt; Parquet 存储和压缩结合官网文档 ORC存储方式的压缩 Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0 and &lt;1.0) 总结： ORC能很大程序的节省存储和计算资源，但它在读写时候需要消耗额外的CPU资源来压缩和解压缩，当然这部分的CPU消耗是非常少的。对性能提升的另一个方面是通过在ORC文件中为每一个字段建立一个轻量级的索引，来判定一个文件中是否满足WHERE子句中的过滤条件。比如：当执行HQL语句”SELECT COUNT(1) FROM lxw1234_orc WHERE id = 0”时候，先从ORC文件的metadata中读取索引信息，快速定位到id=0所在的offsets，如果从索引信息中没有发现id=0的信息，则直接跳过该文件。 上图中原始的TEXT文本文件为585GB，使用Hive早期的RCFILE压缩后为505GB，使用Impala中的PARQUET压缩后为221GB，而Hive中的ORC压缩后仅为131GB，压缩比最高。 1）创建一个非压缩的的ORC存储方式 （1）建表语句 hive&gt; create table log_orc_none( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;); （2）插入数据 hive&gt; insert into table log_orc_none select * from log_text ; （3）查看插入后数据 hive&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ; 7.7 M /user/hive/warehouse/log_orc_none/123456_0 2）创建一个SNAPPY压缩的ORC存储方式 （1）建表语句 hive&gt; create table log_orc_snappy( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS orc tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;); （2）插入数据 hive&gt; insert into table log_orc_snappy select * from log_text; （3）查看插入后数据 hive&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ; 3.8 M /user/hive/warehouse/log_orc_snappy/123456_0 3）上一节中默认创建的ORC存储方式，导入数据后的大小为 2.8 M /user/hive/warehouse/log_orc/123456_0 比Snappy压缩的还小。原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。 4）存储方式和压缩总结： 在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：8、Hive 函数]]></title>
    <url>%2F2018%2F01%2F14%2Fhive-8-hive-func.html</url>
    <content type="text"><![CDATA[Hive内置函数在Hive中给我们内置了很多函数官方文档地址 也可以在启动hive后输入命令查看函数 hive&gt; SHOW FUNCTIONS; hive&gt; DESCRIBE FUNCTION &lt;function_name&gt;; hive&gt; DESCRIBE FUNCTION EXTENDED &lt;function_name&gt;; 查看所有的内置函数hive&gt; show functions; 查看函数的具体语法hive&gt; DESCRIBE FUNCTION case; OK CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END - When a = b, returns c; when a = d, return e; else return f hive&gt; DESCRIBE FUNCTION EXTENDED case; hive&gt; DESCRIBE FUNCTION EXTENDED case; OK CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END - When a = b, returns c; when a = d, return e; else return f Example: SELECT CASE deptno WHEN 1 THEN Engineering WHEN 2 THEN Finance ELSE admin END, CASE zone WHEN 7 THEN Americas ELSE Asia-Pac END FROM emp_de tails Hive自定义函数Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。 当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 根据用户自定义函数类别分为以下三种： （1）UDF（User-Defined-Function） 一进一出 （2）UDAF（User-Defined Aggregation Function） 聚集函数，多进一出 类似于：count/max/min （3）UDTF（User-Defined Table-Generating Functions） 一进多出 如lateral view explore() 官方文档地址 编程步骤1）继承org.apache.hadoop.hive.ql.UDF2）需要实现evaluate函数；evaluate函数支持重载； 注意事项1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；2）UDF中常用Text/LongWritable等类型，不推荐使用java类型； 自定义函数开发案例1）创建一个java工程，并创建一个lib文件夹 2）将hive的jar包解压后，将apache-hive-1.2.2-bin\lib文件下的jar包都拷贝到java工程中 3）创建一个类 package com.example.hive.udf; import org.apache.hadoop.hive.ql.exec.UDF; public class Lower extends UDF { public String evaluate(final String s) { if (s == null) { return null; } return s.toString().toLowerCase(); } } 4）打成jar包上传到服务器/opt/module/jars/udf.jar 5）将jar包添加到hive的classpath hive&gt; add jar /opt/module/jars/udf.jar; 6）创建自定义函数有两种格式:临时函数，永久函数（0.13版本后)： hive&gt; create temporary function my_lower as &#39;com.example.hive.udf.Lower&#39;; hive&gt; create function my_db.my_lower as &#39;com.example.hive.udf.Lower&#39;; 7）即可在hql中使用自定义的函数strip hive&gt; select ename, my_lower(ename) lowername from emp; 永久函数把jar包上传到hdfs上 [root@hadoop001 lib]$ hdfs dfs -put /home/hadoop/lib/hive-1.0.jar /lib [root@hadoop001 lib]$ hdfs dfs -ls /lib -rw-r--r-- 1 hadoop supergroup 4232 2018-01-14 08:03 /lib/hive-1.0.jar 创建永久函数 hive&gt; CREATE FUNCTION say_hello1 &gt; AS &#39;cn.example.HelloUdf&#39; &gt; USING JAR &#39;hdfs:///lib/hive-1.0.jar&#39;; converting to local hdfs:///lib/hive-1.0.jar Added [/tmp/7f999adc-e8a4-4642-af56-581f078cdc32_resources/hive-1.0.jar] to class path Added resources: [hdfs:///lib/hive-1.0.jar] OK 测试 hive&gt; select ename,say_hello1(ename)from emp; OK ENAME Hello:ENAME SMITH Hello:SMITH ALLEN Hello:ALLEN WARD Hello:WARD JONES Hello:JONES MARTIN Hello:MARTIN Time taken: 0.084 seconds, Fetched: 6 row(s) MySQL中查询创建的自定义函数mysql&gt; select * from funcs; +---------+---------------------+-------------+-------+------------+-----------+------------+------------+ | FUNC_ID | CLASS_NAME | CREATE_TIME | DB_ID | FUNC_NAME | FUNC_TYPE | OWNER_NAME | OWNER_TYPE | +---------+---------------------+-------------+-------+------------+-----------+------------+------------+ | 1 | cn.zhangyu.HelloUdf | 1515888313 | 1 | say_hello1 | 1 | NULL | USER | +---------+---------------------+-------------+-------+------------+-----------+------------+------------+ 1 row in set (0.00 sec)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：7、Hive 分区、分桶]]></title>
    <url>%2F2018%2F01%2F13%2Fhive-7-hive-buck.html</url>
    <content type="text"><![CDATA[分区表分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 创建分区表语法hive (default)&gt; create table dept_partition( deptno int, dname string, loc string ) partitioned by (month string) row format delimited fields terminated by &#39;\t&#39;; 加载数据到分区表中hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition partition(month=&#39;201709&#39;); hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition partition(month=&#39;201708&#39;); hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition partition(month=&#39;201707&#39;); 查询分区表中数据单分区查询 hive (default)&gt; select * from dept_partition where month=&#39;201709&#39;; OK 10 ACCOUNTING NEWYORK 201709 10 ACCOUNTING NEWYORK 201709 10 ACCOUNTING NEWYORK 201709 20 RESEARCH DALLAS 201709 20 RESEARCH DALLAS 201709 20 RESEARCH DALLAS 201709 30 OPERATIONS BOSTON 201709 30 OPERATIONS BOSTON 201709 30 OPERATIONS BOSTON 201709 Time taken: 0.362 seconds, Fetched: 9 row(s) 多分区联合查询 hive (default)&gt; select * from dept_partition where month=&#39;201709&#39; union all select * from dept_partition where month=&#39;201708&#39;; OK 10 ACCOUNTING NEWYORK 201709 10 ACCOUNTING NEWYORK 201709 10 ACCOUNTING NEWYORK 201709 20 RESEARCH DALLAS 201709 20 RESEARCH DALLAS 201709 20 RESEARCH DALLAS 201709 30 OPERATIONS BOSTON 201709 30 OPERATIONS BOSTON 201709 30 OPERATIONS BOSTON 201709 10 ACCOUNTING NEWYORK 201708 10 ACCOUNTING NEWYORK 201708 10 ACCOUNTING NEWYORK 201708 20 RESEARCH DALLAS 201708 20 RESEARCH DALLAS 201708 20 RESEARCH DALLAS 201708 30 OPERATIONS BOSTON 201708 30 OPERATIONS BOSTON 201708 30 OPERATIONS BOSTON 201708 Time taken: 30.91 seconds, Fetched: 18 row(s) 增加分区增加单个分区 hive (default)&gt; alter table dept_partition add partition(month=&#39;201706&#39;); 同时创建多个分区 hive (default)&gt; alter table dept_partition add partition(month=&#39;201705&#39;) partition(month=&#39;201704&#39;); 删除分区删除单个分区 hive (default)&gt; alter table dept_partition drop partition (month=&#39;201704&#39;); 同时删除多个分区 hive (default)&gt; alter table dept_partition drop partition (month=&#39;201705&#39;), partition (month=&#39;201706&#39;); 查看分区表有多少分区hive&gt;show partitions dept_partition; 查看分区表结构hive (default)&gt; desc formatted dept_partition; OK col_name data_type comment # col_name data_type comment deptno int dname string loc string # Partition Information # col_name data_type comment month string # Detailed Table Information Database: default Owner: root CreateTime: Wed Jun 20 13:53:42 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://mycluster/user/hive/warehouse/dept_partition Table Type: MANAGED_TABLE Table Parameters: transient_lastDdlTime 1529474022 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.09 seconds, Fetched: 34 row(s) 分区表注意事项创建二级分区表hive (default)&gt; create table dept_partition2( deptno int, dname string, loc string ) partitioned by (month string, day string) row format delimited fields terminated by &#39;\t&#39;; 正常的加载数据加载数据到二级分区表中 hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition2 partition(month=&#39;201709&#39;, day=&#39;13&#39;); 查询分区数据 hive (default)&gt; select * from dept_partition2 where month=&#39;201709&#39; and day=&#39;13&#39;; 把数据直接上传到分区目录上，让分区表和数据产生关联的两种方式方式一：上传数据后修复 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12; hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; 查询数据（查询不到刚上传的数据） hive (default)&gt; select * from dept_partition2 where month=&#39;201709&#39; and day=&#39;12&#39;; 执行修复命令 hive&gt;msck repair table dept_partition2; 再次查询数据 hive (default)&gt; select * from dept_partition2 where month=&#39;201709&#39; and day=&#39;12&#39;; 方式二：上传数据后添加分区 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11; hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11; 执行添加分区 hive (default)&gt; alter table dept_partition2 add partition(month=&#39;201709&#39;, day=&#39;11&#39;); 查询数据 hive (default)&gt; select * from dept_partition2 where month=&#39;201709&#39; and day=&#39;11&#39;; 分桶表分区针对的是数据的存储路径；分桶针对的是数据文件。 分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。 分桶是将数据集分解成更容易管理的若干部分的另一个技术。 创建分桶表，通过直接导入数据文件的方式创建分桶表 hive&gt; create table stu_buck(id int, name string) clustered by(id) into 4 buckets row format delimited fields terminated by &#39;\t&#39;; 查看表结构 hive&gt; desc formatted stu_buck; Num Buckets: 4 导入数据到分桶表中 hive&gt; load data local inpath &#39;/opt/module/datas/student.txt&#39; into table stu_buck; 查看创建的分桶表中是否分成4个桶 发现并没有分成4个桶。是什么原因呢？ 创建分桶表，数据通过子查询的方式导入先建一个普通的stu表 hive&gt; create table stu(id int, name string) row format delimited fields terminated by &#39;\t&#39;; 向普通的stu表中导入数据 hive&gt; load data local inpath &#39;/opt/module/datas/student.txt&#39; into table stu; 清空stu_buck表中数据 hive&gt; truncate table stu_buck; hive&gt; select * from stu_buck; 导入数据到分桶表，通过子查询的方式 hive&gt; insert into table stu_buck select id, name from stu cluster by(id); 发现还是只有一个分桶 需要设置一个属性 hive&gt; set hive.enforce.bucketing=true; hive&gt; set mapreduce.job.reduces=-1; hive&gt; insert into table stu_buck select id, name from stu cluster by(id); 查看创建的分桶表中是否分成4个桶 分桶抽样查询对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。 查询表stu_buck中的数据 hive&gt; select * from stu_buck TABLESAMPLE(bucket 1 out of 4 on id); 注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) y必须是table总bucket数的倍数或者因子。 hive根据y的大小，决定抽样的比例。 例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。 x表示从哪个bucket开始抽取。 例如，table总bucket数为4，tablesample(bucket 4 out of 4)，表示总共抽取（4/4=）1个bucket的数据，抽取第4个bucket的数据。 数据块抽样Hive提供了另外一种按照百分比进行抽样的方式，这种事基于行数的，按照输入路径下的数据块百分比进行的抽样。 hive&gt; select * from stu tablesample(0.1 percent); 注：这种抽样方式不一定适用于所有的文件格式。另外，这种抽样的最小抽样单元是一个HDFS数据块。因此，如果表的数据大小小于普通的块大小128M的话，那么将会返回所有行。 基于百分比的抽样方式提供了一个变量，用于控制基于数据块的调优的种子信息。 &lt;property&gt; &lt;name&gt;hive.sample.seednumber&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：6、Hive ImportExport]]></title>
    <url>%2F2018%2F01%2F12%2Fhive-6-hive-import-export.html</url>
    <content type="text"><![CDATA[官网语法Hive 0.8.0 之后引入了EXPORT and IMPORT 命令 EXPORT命令将表或分区的数据连同元数据一起导出到指定的输出位置（HDFS上）。然后可以将此输出位置移至不同的Hadoop或Hive实例，并使用IMPORT命令进行导入操作。 导出分区表时，原始数据可能位于不同的HDFS位置。还支持导出/导入分区子集的功能。 导出的元数据存储在目标目录中，数据文件存储在子目录中。 EXPORT和IMPORT命令独立于所使用的源和目标Metastore DBMS工作;例如，它们可以在Derby和MySQL数据库之间使用。 EXPORTEXPORT TABLE tablename [PARTITION (part_column=&quot;value&quot;[, ...])] TO &#39;export_target_path&#39; [ FOR replication(&#39;eventid&#39;) ] 1 2 将数据导出到HDFS export table join_a to &#39;/home/hadoop/data&#39;; Copying data from file:/tmp/hadoop/b96064b0-888a-4618-a110-fc9d09d7c00b/hive_2018-01-10_08-05-04_501_1397817648848080864-1/-local-10000/_metadata Copying file: file:/tmp/hadoop/b96064b0-888a-4618-a110-fc9d09d7c00b/hive_2018-01-10_08-05-04_501_1397817648848080864-1/-local-10000/_metadata Copying data from hdfs://192.168.137.200:9000/user/hive/warehouse/join_a Copying file: hdfs://192.168.137.200:9000/user/hive/warehouse/join_a/join_a.txt OK 查看HDFS上的数据 [root@hadoop001 export]$ hdfs dfs -ls /home/hadoop/data Found 2 items -rwxr-xr-x 1 hadoop supergroup 1259 2018-01-10 08:05 /home/hadoop/data/_metadata drwxr-xr-x - hadoop supergroup 0 2018-01-10 08:05 /home/hadoop/data/data Hive Shell命令导出语法：hive -f/-e 执行语句或者脚本 &gt; file [root@hadoop001 hive]$ bin/hive -e &#39;select * from default.student;&#39; &gt; /opt/module/datas/export/student4.txt; IMPORTIMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column=&quot;value&quot;[, ...])]] FROM &#39;source_path&#39; [LOCATION &#39;import_target_path&#39;] 导入数据 hive&gt; import table join_a from &#39;/home/hadoop/data&#39;; Copying data from hdfs://192.168.137.200:9000/home/hadoop/data/data Copying file: hdfs://192.168.137.200:9000/home/hadoop/data/data/join_a.txt Loading data to table hive1.join_a OK Time taken: 7.444 seconds 查询 hive&gt; show tables; OK join_a hive&gt; select * from join_a; OK 1 zhangsan 2 lisi 3 wangwu1 其他例子Rename table on import:（重命名表名）export table department to &#39;hdfs_exports_location/department&#39;; import table imported_dept from &#39;hdfs_exports_location/department&#39;; Export partition and import:（导出加上分区）export table employee partition (emp_country=&quot;in&quot;, emp_state=&quot;ka&quot;) to &#39;hdfs_exports_location/employee&#39;; import from &#39;hdfs_exports_location/employee&#39;; Export table and import partition:（导入加上分区）export table employee to &#39;hdfs_exports_location/employee&#39;; import table employee partition (emp_country=&quot;us&quot;, emp_state=&quot;tn&quot;) from &#39;hdfs_exports_location/employee&#39;; Specify the import location:（指定地址）export table department to &#39;hdfs_exports_location/department&#39;; import table department from &#39;hdfs_exports_location/department&#39; location &#39;import_target_location/department&#39;; Import as an external table: export table department to &#39;hdfs_exports_location/department&#39;; import external table department from &#39;hdfs_exports_location/department&#39;;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：5、Hive Cli]]></title>
    <url>%2F2018%2F01%2F11%2Fhive-5-hive-cli.html</url>
    <content type="text"><![CDATA[官网语法Hive Command Line OptionsTo get help, run &quot;hive -H&quot; or &quot;hive --help&quot;. Usage (as it is in Hive 0.9.0): usage: hive -d,--define &lt;key=value&gt; Variable substitution to apply to Hive commands. e.g. -d A=B or --define A=B -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information -h &lt;hostname&gt; Connecting to Hive Server on remote host --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable substitution to apply to hive commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -p &lt;port&gt; Connecting to Hive Server on port number -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) hive -e[root@hadoop001 data]$ hive -e &#39;select * from default.student&#39;; OK 1 zhangsan 18310982765 20 2 lisi 18282823434 30 3 wangwu 1575757668 40 Time taken: 3.373 seconds, Fetched: 3 row(s) [root@hadoop0011 data]$ hive -e &#39;show tables&#39;; OK dept dept1 emmp2 emp emp1 emp3 emp_dynamic_partition emp_partition helloword order_multi_partition order_partition stu stu_age_partition student Time taken: 1.352 seconds, Fetched: 14 row(s) hive -s(数据存储到指定的目录)[root@hadoop001 data]$ hive -S -e &#39;show tables&#39; &gt; a.txt [hadoop@zydatahadoop001 data]$ ll total 12 -rw-r--r--. 1 hadoop hadoop 18 Jan 9 10:35 000000_0 -rw-rw-r--. 1 hadoop hadoop 145 Jan 10 01:33 a.txt -rw-rw-r--. 1 hadoop hadoop 113 Jan 10 00:48 student.txt [hadoop@zydatahadoop001 data]$ cat a.txt dept dept1 emmp2 emp emp1 emp3 emp_dynamic_partition emp_partition helloword order_multi_partition order_partition stu stu_age_partition student hive -f使用非交互的方式执行本地脚本 [root@hadoop001 data]$ vi hfile.sql select * from default.student ; [hadoop@zydatahadoop001 data]$ hive -f hfile.sql OK 1 zhangsan 18310982765 20 2 lisi 18282823434 30 3 wangwu 1575757668 40 Time taken: 2.14 seconds, Fetched: 3 row(s) hive -i在初始化脚本之前进入到交互模式 [root@hadoop001 data]$ hive -i hfile.sql 1 zhangsan 18310982765 20 2 lisi 18282823434 30 3 wangwu 1575757668 40]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：4、Hive 聚合、分组]]></title>
    <url>%2F2018%2F01%2F10%2Fhive-4-hive-sql-func.html</url>
    <content type="text"><![CDATA[聚合函数常用聚合函数：sum，count，max，min，avg。hive&gt; select * from emp; OK 7369 SMITH CLERK 7902 1980/12/17 800.0 NULL 20 7499 ALLEN SALESMAN 7698 1981/2/20 1600.0 300.0 30 7521 WARD SALESMAN 7698 1981/2/22 1250.0 500.0 30 7566 JONES MANAGER 7839 1981/4/2 2975.0 NULL 20 7654 MARTIN SALESMAN 7698 1981/9/28 1250.0 1400.0 30 7698 BLAKE MANAGER 7839 1981/5/1 2850.0 NULL 30 7782 CLARK MANAGER 7839 1981/6/9 2450.0 NULL 10 7788 SCOTT ANALYST 7566 1987/4/19 3000.0 NULL 20 7839 KING PRESIDENT NULL 1981/11/17 5000.0 NULL 10 7844 TURNER SALESMAN 7698 1981/9/8 1500.0 0.0 30 7876 ADAMS CLERK 7788 1987/5/23 1100.0 NULL 20 7900 JAMES CLERK 7698 1981/12/3 950.0 NULL 30 7902 FORD ANALYST 7566 1981/12/3 3000.0 NULL 20 7934 MILLER CLERK 7782 1982/1/23 1300.0 NULL 10 Time taken: 5.998 seconds, Fetched: 14 row(s) 查询员工的最大、最小、平均工资及所有工资的和hive&gt; select max(salary),min(salary),avg(salary),sum(salary) from emp; OK 5000.0 800.0 2073.214285714286 29025.0 查询记录数hive&gt; select count(*) from emp; OK 14 分组函数使用Group by时，在Group by后面出现的字段也要出现在select后面。 会执行mapreduce 按照部门进行分组hive&gt; select deptno from emp group by deptno; OK 10 20 30 查询每个部门的平均工资hive&gt; select deptno,avg(salary) avg_sal from emp group by deptno; OK 10 2916.6666666666665 20 2175.0 30 1566.6666666666667 查询平均工资大于2000的部门（使用having子句限定分组查询）hive&gt; select deptno,avg(salary) from emp group by deptno having avg(salary) &gt; 2000; OK 10 2916.6666666666665 20 2175.0 按照部门和入职时间进行分组（先按照部门进行分组，然后针对每组按照入职时间进行分组）hive&gt; select deptno,hiredate from emp group by deptno,hiredate; OK 10 1981/11/17 10 1981/6/9 10 1982/1/23 20 1980/12/17 20 1981/12/3 20 1981/4/2 20 1987/4/19 20 1987/5/23 30 1981/12/3 30 1981/2/20 30 1981/2/22 30 1981/5/1 30 1981/9/28 30 1981/9/8 按照部门和入职时间进行分组并计算出每组的人数hive&gt; select deptno,hiredate,count(ename) from emp group by deptno,hiredate; OK 10 1981/11/17 1 10 1981/6/9 1 10 1982/1/23 1 20 1980/12/17 1 20 1981/12/3 1 20 1981/4/2 1 20 1987/4/19 1 20 1987/5/23 1 30 1981/12/3 1 30 1981/2/20 1 30 1981/2/22 1 30 1981/5/1 1 30 1981/9/28 1 30 1981/9/8 1 case when then end（不会执行mr）查询员工的姓名和工资等级，按如下规则显示salary小于等于1000，显示LOWERsalaray大于1000且小于等于2000，显示MIDDLEsalaray大于2000小于等于4000，显示MIDDLEsal大于4000，显示highest hive&gt; select ename, salary, case when salary &gt; 1 and salary &lt;= 1000 then &#39;LOWER&#39; when salary &gt; 1000 and salary &lt;= 2000 then &#39;MIDDLE&#39; when salary &gt; 2000 and salary &lt;= 4000 then &#39;HIGH&#39; ELSE &#39;HIGHEST&#39; end from emp; OK SMITH 800.0 LOWER ALLEN 1600.0 MIDDLE WARD 1250.0 MIDDLE JONES 2975.0 HIGH MARTIN 1250.0 MIDDLE BLAKE 2850.0 HIGH CLARK 2450.0 HIGH SCOTT 3000.0 HIGH KING 5000.0 HIGHEST TURNER 1500.0 MIDDLE ADAMS 1100.0 MIDDLE JAMES 950.0 LOWER FORD 3000.0 HIGH MILLER 1300.0 MIDDLE 多表查询Hive中Join的关联键必须在ON中指定，不能在Where中指定，否则就会先做笛卡尔积，再过滤。 创建表hive&gt; create table join_a( &gt; id int, &gt; name string &gt; ) &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;; hive&gt; load data local inpath &#39;/home/hadoop/data/join_a.txt&#39; OVERWRITE INTO TABLE join_a; hive&gt; select * from join_a; OK 1 zhangsan 2 lisi 3 wangwu hive&gt; create table join_b( &gt; id int, &gt; age int &gt; ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;; hive&gt; load data local inpath &#39;/home/hadoop/data/join_b.txt&#39; OVERWRITE INTO TABLE join_b; hive&gt; select * from join_b; OK 1 20 2 30 4 40 内连接内连接即基于on语句，仅列出表1和表2符合连接条件的数据 hive&gt; select a.id,a.name,b.age from join_a a join join_b b on a.id=b.id; OK 1 zhangsan 20 2 lisi 30 hive&gt; select * from join_a a join join_b b on a.id=b.id; OK 1 zhangsan 1 20 2 lisi 2 30 左连接左连接是显示左边的表的所有数据，如果有右边表与之对应，则显示；否则显示null hive&gt; select a.id,a.name,b.age from join_a a left join join_b b on a.id=b.id; OK 1 zhangsan 20 2 lisi 30 3 wangwu NULL 右连接右连接是显示右边的表的所有数据，如果有左边表与之对应，则显示；否则显示null hive&gt; select a.id,a.name,b.age from join_a a right join join_b b on a.id=b.id; OK 1 zhangsan 20 2 lisi 30 NULL NULL 40 全连接相当于表1和表2的数据都显示，如果没有对应的数据，则显示Null hive &gt; select a.id,a.name,b.age from join_a a full join join_b b on a.id=b.id; OK 1 zhangsan 20 2 lisi 30 3 wangwu1 NULL NULL NULL 40 笛卡尔积笛卡尔积(没有连接条件)会针对表1和表2的每条数据做连接 join(cross join) hive &gt; select a.id,a.name,b.age from join_a a cross join join_b b; OK 1 zhangsan 20 1 zhangsan 30 1 zhangsan 40 2 lisi 20 2 lisi 30 2 lisi 40 3 wangwu1 20 3 wangwu1 30 3 wangwu1 40]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：3、Hive DML]]></title>
    <url>%2F2018%2F01%2F09%2Fhive-3-hive-dml.html</url>
    <content type="text"><![CDATA[DML（Data Manipulation Language）Loading files into tablesLOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] LOCAL：表示输入文件在本地文件系统（Linux），如果没有加LOCAL，hive则会去HDFS上查找该文件。 OVERWRITE：表示如果表中有数据，则先删除数据，再插入新数据，如果没有OVERWRITE，则直接附加数据到表中。 PARTITION：如果表中存在分区，可以按照分区进行导入。 Inserting data into Hive Tables from queriesStandard syntax: INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; Hive extension (multiple inserts): FROM from_statement INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 [INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] [INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...; FROM from_statement INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] [INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] ...; Hive extension (dynamic partition inserts): INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; INSERT OVERWRITE：将覆盖在表或分区的任何现 INSERT INTO：将追加到表或分区，保留原有数据不变（注：INSERT INTO语法只能在开始0.8版本） multiple inserts：多表插入 dynamic partition inserts：动态分区插入(0.6版本开始)动态分区列必须在最后的SELECT语句中的列中指定，按PARTITION（）子句中的顺序出现 FROM page_view_stg pvs INSERT OVERWRITE TABLE page_view PARTITION(dt=&#39;2008-06-08&#39;, country) SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.cnt Writing data into the filesystem from queriesStandard syntax: INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0) SELECT ... FROM ... Hive extension (multiple inserts): FROM from_statement INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1 [INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] (Note: Only available starting with Hive 0.13) LOCAL：加上LOCAL关键字代表导入本地系统，不加默认导入HDFS STORED AS:可以指定存储格式 Examples hive&gt; insert overwrite local directory &#39;/home/hadoop/data&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; select * from stu; HDFS上查看结果： [hadoop@zydatahadoop001 data]$ pwd /home/hadoop/data [hadoop@zydatahadoop001 data]$ cat 000000_0 1 zhangsan 2 lisi Inserting values into tables from SQLStandard Syntax: INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...] Where values_row is: ( value [, value ...] ) where a value is either null or any valid SQL literal Examples CREATE TABLE students (name VARCHAR(64), age INT, gpa DECIMAL(3, 2)) CLUSTERED BY (age) INTO 2 BUCKETS STORED AS ORC; INSERT INTO TABLE students VALUES (&#39;fred flintstone&#39;, 35, 1.28), (&#39;barney rubble&#39;, 32, 2.32); CREATE TABLE pageviews (userid VARCHAR(64), link STRING, came_from STRING) PARTITIONED BY (datestamp STRING) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC; INSERT INTO TABLE pageviews PARTITION (datestamp = &#39;2014-09-23&#39;) VALUES (&#39;jsmith&#39;, &#39;mail.com&#39;, &#39;sports.com&#39;), (&#39;jdoe&#39;, &#39;mail.com&#39;, null); INSERT INTO TABLE pageviews PARTITION (datestamp) VALUES (&#39;tjohnson&#39;, &#39;sports.com&#39;, &#39;finance.com&#39;, &#39;2014-09-23&#39;), (&#39;tlee&#39;, &#39;finance.com&#39;, null, &#39;2014-09-21&#39;); Selecthive&gt; select * from emp where deptno=10; OK 7782 CLARK MANAGER 7839 1981/6/9 2450.0 NULL 10 7839 KING PRESIDENT NULL 1981/11/17 5000.0 NULL 10 7934 MILLER CLERK 7782 1982/1/23 1300.0 NULL 10 Time taken: 1.144 seconds, Fetched: 3 row(s) hive&gt; select * from emp where empno &lt;= 7800; OK 7369 SMITH CLERK 7902 1980/12/17 800.0 NULL 20 7499 ALLEN SALESMAN 7698 1981/2/20 1600.0 300.0 30 7521 WARD SALESMAN 7698 1981/2/22 1250.0 500.0 30 7566 JONES MANAGER 7839 1981/4/2 2975.0 NULL 20 7654 MARTIN SALESMAN 7698 1981/9/28 1250.0 1400.0 30 7698 BLAKE MANAGER 7839 1981/5/1 2850.0 NULL 30 7782 CLARK MANAGER 7839 1981/6/9 2450.0 NULL 10 7788 SCOTT ANALYST 7566 1987/4/19 3000.0 NULL 20 Time taken: 0.449 seconds, Fetched: 8 row(s) hive&gt; select * from emp where salary between 1000 and 1500; OK 7521 WARD SALESMAN 7698 1981/2/22 1250.0 500.0 30 7654 MARTIN SALESMAN 7698 1981/9/28 1250.0 1400.0 30 7844 TURNER SALESMAN 7698 1981/9/8 1500.0 0.0 30 7876 ADAMS CLERK 7788 1987/5/23 1100.0 NULL 20 7934 MILLER CLERK 7782 1982/1/23 1300.0 NULL 10 Time taken: 0.178 seconds, Fetched: 5 row(s) hive&gt; select * from emp limit 5; OK 7369 SMITH CLERK 7902 1980/12/17 800.0 NULL 20 7499 ALLEN SALESMAN 7698 1981/2/20 1600.0 300.0 30 7521 WARD SALESMAN 7698 1981/2/22 1250.0 500.0 30 7566 JONES MANAGER 7839 1981/4/2 2975.0 NULL 20 7654 MARTIN SALESMAN 7698 1981/9/28 1250.0 1400.0 30 Time taken: 0.47 seconds, Fetched: 5 row(s) hive&gt; select * from emp where empno in(7566,7499); OK 7499 ALLEN SALESMAN 7698 1981/2/20 1600.0 300.0 30 7566 JONES MANAGER 7839 1981/4/2 2975.0 NULL 20 Time taken: 0.4 seconds, Fetched: 2 row(s) hive&gt; select * from emp where comm is not null; OK 7499 ALLEN SALESMAN 7698 1981/2/20 1600.0 300.0 30 7521 WARD SALESMAN 7698 1981/2/22 1250.0 500.0 30 7654 MARTIN SALESMAN 7698 1981/9/28 1250.0 1400.0 30 7844 TURNER SALESMAN 7698 1981/9/8 1500.0 0.0 30 Time taken: 0.262 seconds, Fetched: 4 row(s) UpdateStandard Syntax: UPDATE tablename SET column = value [, column = value ...] [WHERE expression] DeleteStandard Syntax: DELETE FROM tablename [WHERE expression] MergeStandard Syntax: MERGE INTO &lt;target table&gt; AS T USING &lt;source expression/table&gt; AS S ON &lt;boolean expression1&gt; WHEN MATCHED [AND &lt;boolean expression2&gt;] THEN UPDATE SET &lt;set clause list&gt; WHEN MATCHED [AND &lt;boolean expression3&gt;] THEN DELETE WHEN NOT MATCHED [AND &lt;boolean expression4&gt;] THEN INSERT VALUES&lt;value list&gt;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：2、Hive DDL]]></title>
    <url>%2F2018%2F01%2F08%2Fhive-2-hive-ddl.html</url>
    <content type="text"><![CDATA[DatabaseHive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置） TableHive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/table Partition分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：/user/hadoop/hive/warehouse/[databasename.db]/table DDL(Data Definition Language)Create DatabaseCREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。COMMENT：数据库的描述LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下WITH DBPROPERTIES：数据库的属性 Drop DatabaseDROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; RESTRICT：默认是restrict，如果该数据库还有表存在则报错；CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。 Alter DatabaseALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0) ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0) ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later) Use DatabaseUSE database_name; USE DEFAULT; Show DatabasesSHOW (DATABASES|SCHEMAS) [LIKE &#39;identifier_with_wildcards&#39; “ | ”：可以选择其中一种 “[ ]”：可选项 LIKE ‘identifier_with_wildcards’：模糊查询数据库 Describe DatabaseDESCRIBE DATABASE [EXTENDED] db_name; DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息； EXTENDED：加上数据库键值对的属性信息。 hive&gt; describe database default; OK default Default Hive database hdfs://hadoop1:9000/user/hive/warehouse public ROLE Time taken: 0.065 seconds, Fetched: 1 row(s) hive&gt; hive&gt; describe database extended hive2; OK hive2 it is my database hdfs://hadoop1:9000/user/hive/warehouse/hive2.db hadoop USER {date=2018-08-08, creator=zhangsan} Time taken: 0.135 seconds, Fetched: 1 row(s) Create TableCREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; data_type : primitive_type | array_type | map_type | struct_type | union_type -- (Note: Available in Hive 0.7.0 and later) primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later) array_type : ARRAY &lt; data_type &gt; map_type : MAP &lt; primitive_type, data_type &gt; struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt; union_type : UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later) row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] file_format: : SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname constraint_specification: : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE TEMPORARY（临时表）Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。语法：CREATE TEMPORARY TABLE … 注意：1. 如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表2. 临时表限制：不支持分区字段和创建索引 EXTERNAL（外部表）Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。 hive&gt; create external table external_table( &gt; id int, &gt; name string &gt; ); PARTITIONED BY（分区表）产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。 可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下； 分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。 分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。 单分区： hive&gt; CREATE TABLE order_partition ( &gt; order_number string, &gt; event_time string &gt; ) &gt; PARTITIONED BY (event_month string); OK 多分区： hive&gt; CREATE TABLE order_partition2 ( &gt; order_number string, &gt; event_time string &gt; ) &gt; PARTITIONED BY (event_month string,every_day string); OK [hadoop@hadoop1 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db 18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Found 2 items drwxr-xr-x - hadoop supergroup 0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partition drwxr-xr-x - hadoop supergroup 0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2 [hadoop@hadoop1 ~]$ ROW FORMAT官网解释： : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] DELIMITED：分隔符（可以自定义分隔符）； FIELDS TERMINATED BY char:每个字段之间使用的分割； 例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n; COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）； MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符； LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]） 一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。 创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n： hive&gt; create table demo1( &gt; id int, &gt; name string &gt; ) &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;; OK 创建demo2表，并指定其他字段： hive&gt; create table demo2 ( &gt; id int, &gt; name string, &gt; hobbies ARRAY &lt;string&gt;, &gt; address MAP &lt;string, string&gt; &gt; ) &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; &gt; COLLECTION ITEMS TERMINATED BY &#39;-&#39; &gt; MAP KEYS TERMINATED BY &#39;:&#39;; OK STORED AS（存储格式）Create Table As Select创建表（拷贝表结构及数据，并且会运行MapReduce作业） CREATE TABLE emp ( empno int, ename string, job string, mgr int, hiredate string, salary double, comm double, deptno int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;; #加载数据 LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp; #复制整张表 hive&gt; create table emp2 as select * from emp; Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350 Total jobs = 3 Launching Job 1 out of 3 Number of reduce tasks is set to 0 since there&#39;s no reduce operator Starting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/ Kill Command = /opt/software/hadoop/bin/hadoop job -kill job_1514116522188_0003 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0 2018-01-08 05:21:07,707 Stage-1 map = 0%, reduce = 0% 2018-01-08 05:21:19,605 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.81 sec MapReduce Total cumulative CPU time: 1 seconds 810 msec Ended Job = job_1514116522188_0003 Stage-4 is selected by condition resolver. Stage-3 is filtered out by condition resolver. Stage-5 is filtered out by condition resolver. Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001 Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2 Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650] MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 1.81 sec HDFS Read: 3927 HDFS Write: 730 SUCCESS Total MapReduce CPU Time Spent: 1 seconds 810 msec OK Time taken: 33.322 seconds hive&gt; show tables; OK emp emp2 order_partition order_partition2 Time taken: 0.071 seconds, Fetched: 4 row(s) hive&gt; #复制表中的一些字段 create table emp3 as select empno,ename from emp; LIKE使用like创建表时，只会复制表的结构，不会复制表的数据 hive&gt; create table emp4 like emp; OK Time taken: 0.149 seconds hive&gt; select * from emp4; OK Time taken: 0.151 seconds hive&gt; 并没有查询到数据 desc formatted table_name查询表的详细信息 hive&gt; desc formatted emp; OK # col_name data_type comment empno int ename string job string mgr int hiredate string salary double comm double deptno int # Detailed Table Information Database: hive Owner: hadoop CreateTime: Mon Jan 08 05:17:54 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 0 rawDataSize 0 totalSize 668 transient_lastDdlTime 1515359982 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.228 seconds, Fetched: 39 row(s) hive&gt; 通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params; 查询数据库下的所有表hive&gt; show tables; OK emp emp1 emp2 emp3 emp4 order_partition order_partition2 Time taken: 0.047 seconds, Fetched: 7 row(s) hive&gt; 查询创建表的语法hive&gt; show create table emp; OK CREATE TABLE `emp`( `empno` int, `ename` string, `job` string, `mgr` int, `hiredate` string, `salary` double, `comm` double, `deptno` int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39; OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39; LOCATION &#39;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&#39; TBLPROPERTIES ( &#39;COLUMN_STATS_ACCURATE&#39;=&#39;true&#39;, &#39;numFiles&#39;=&#39;1&#39;, &#39;numRows&#39;=&#39;0&#39;, &#39;rawDataSize&#39;=&#39;0&#39;, &#39;totalSize&#39;=&#39;668&#39;, &#39;transient_lastDdlTime&#39;=&#39;1515359982&#39;) Time taken: 0.192 seconds, Fetched: 24 row(s) hive&gt; Drop TableDROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later) 指定PURGE后，数据不会放到回收箱，会直接删除 DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失 删除EXTERNAL表时，表中的数据不会从文件系统中删除 Alter Table#重命名 hive&gt; alter table demo2 rename to new_demo2; OK Add PartitionsALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &#39;location&#39;][, PARTITION partition_spec [LOCATION &#39;location&#39;], ...]; partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...) 用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。 注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。 hive&gt; create table dept( &gt; deptno int, &gt; dname string, &gt; loc string &gt; ) &gt; PARTITIONED BY (dt string) &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;; OK Time taken: 0.953 seconds hive&gt; load data local inpath &#39;/home/hadoop/dept.txt&#39;into table dept partition (dt=&#39;2018-08-08&#39;); Loading data to table default.dept partition (dt=2018-08-08) Partition default.dept{dt=2018-08-08} stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0] OK Time taken: 5.147 seconds #查询结果 hive&gt; select * from dept; OK 10 ACCOUNTING NEW YORK 2018-08-08 20 RESEARCH DALLAS 2018-08-08 30 SALES CHICAGO 2018-08-08 40 OPERATIONS BOSTON 2018-08-08 Time taken: 0.481 seconds, Fetched: 4 row(s) hive&gt; ALTER TABLE dept ADD PARTITION (dt=&#39;2018-09-09&#39;); OK Drop PartitionsALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...] hive&gt; ALTER TABLE dept DROP PARTITION (dt=&#39;2018-09-09&#39;); 查看分区语句hive&gt; show partitions dept; OK dt=2018-08-08 dt=2018-09-09 Time taken: 0.385 seconds, Fetched: 2 row(s) 按分区查询hive&gt; select * from dept where dt=&#39;2018-08-08&#39;; OK 10 ACCOUNTING NEW YORK 2018-08-08 20 RESEARCH DALLAS 2018-08-08 30 SALES CHICAGO 2018-08-08 40 OPERATIONS BOSTON 2018-08-08 Time taken: 2.323 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive：1、Hive的安装部署]]></title>
    <url>%2F2018%2F01%2F03%2Fhive-1-hive-install.html</url>
    <content type="text"><![CDATA[Hive的产生背景MR编程不便性 传统RDBMS人员的需要 Hive是构建在Hadoop之上的数据仓库数据存储在HDFS之上 计算是使用MR 弹性：线性扩展 Hive底层的执行引擎MapReduce、Tez、Spark Hive常用于离线批处理 为什么要使用Hive简单易用 弹性 统一的元数据管理 安装步骤下载hive-1.1.0-cdh5.7.0.tar.gz，并解压[hadoop@hadoop-01 software]$ wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz [hadoop@hadoop-01 software]$ tar -xzvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app/ 配置环境变量并生效[hadoop@hadoop-01 software]$ vi ~/.bash_profile export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0 export PATH=$HIVE_HOME/bin:$PATH [hadoop@hadoop-01 software]$ source ~/.bash_profile Hive部署[hadoop@hadoop-01 software]$ cd ../app/hive-1.1.0-cdh5.7.0/conf/ [hadoop@hadoop-01 conf]$ cp hive-env.sh.template hive-env.sh [hadoop@hadoop-01 conf]$ vi hive-env.sh # 修改HADOOP_HOME HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 默认情况下，Hive元数据保存在内嵌的Derby数据库中，只能允许一个会话连接，只适合简单的测试为了支持多用户多会话，则需要一个独立的元数据库，我们使用MySQL作为元数据库 # 新建hive-site.xml [hadoop@hadoop-01 conf]$ touch hive-site.xml [hadoop@hadoop-01 conf]$ vi hive-site.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/ruoze_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; // 这里的用户和密码要与MySQL的一直 &lt;/property&gt; &lt;/configuration&gt; 拷贝MySQL驱动包(`mysql-connector-java.jar`)到`$HIVE_HOME/lib/`]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：9、Hadoop 压缩详解]]></title>
    <url>%2F2017%2F12%2F28%2Fhadoop-9-hadoop-compress.html</url>
    <content type="text"><![CDATA[Hadoop中的压缩格式 压缩格式 工具 算法 文件扩展名 是否可切分 DEFLATE 无 DEFLATE .deflate No gzip gzip DEFLATE .gz No LZ4 无 LZ4 .LZ4 NO bzip bzip bzip .bz2 YES LZO lzop LZO .lzo YES if indexed Snappy 无 Snappy .snappy NO 压缩比与压缩速度测试环境: 8 core i7 cpu 8GB memory 64 bit CentOS 1.4GB Wikipedia Corpus 2-gram text input 压缩比 压缩时间 可以看出压缩比越高，压缩时间越长，压缩比：Snappy &lt; LZ4 &lt; LZO &lt; GZIP &lt; BZIP2 gzip:优点：压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 lzo压缩优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便。 缺点：压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）。 snappy压缩优点：压缩速度快；支持hadoop native库。 缺点：不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令。 bzip2压缩优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 压缩优缺点对于压缩的好处可以从两方面考虑：Storage + Compute； 1）Storage ：基于HDFS考虑，减少了存储文件所占空间，提升了数据传输速率； 2）Compute：基于YARN上的计算(MapReduce/Hive/Spark/….)速度的提升。 在hadoop大数据的背景下，这两点尤为重要，怎样达到一个高效的处理，选择什么样的压缩方式和存储格式（下篇博客介绍）是很关键的。 从这幅图带领大家进一步认识压缩于解压的优缺点，看到这幅图后你们有一定自己的认识吗？优点：减少存储空间（HDFS）,降低网络带宽，减少磁盘IO缺点：既然存在优点，那必然存在缺点，那就是CPU啦，压缩和解压肯定要消耗CPU的，如果CPU过高那肯定会导致集群负载过高，从而导致你的计算缓慢，job阻塞，文件读取变慢一系列原因。 总结： 1）不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。 2）分片的理解：举个例子，一个未压缩的文件有1GB大小，hdfs默认的block大小是64MB,那么这个文件就会被分为16个block作为mapreduce的输入，每一个单独使用一个map任务。如果这个文件是已经使用gzip压缩的呢，如果分成16个块，每个块做成一个输入，显然是不合适的，因为gzip压缩流的随即读是不可能的。实际上，当mapreduce处理压缩格式的文件的时候它会认识到这是一个gzip的压缩文件，而gzip又不支持随即读，它就会把16个块分给一个map去处理，这里就会有很多非本地处理的map任务，整个过程耗费的时间就会相当长。lzo压缩格式也会是同样的问题，但是通过使用hadoop lzo库的索引工具以后，lzo就可以支持splittable。bzip2也是支持splittable的。 压缩在MapReduce中的应用场景 在hadoop中的应用场景总结在三方面：输入，中间，输出。 整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce 1）Use Compressd Map Input:从HDFS中读取文件进行Mapreuce作业，如果数据很大，可以使用压缩并且选择支持分片的压缩方式（Bzip2,LZO），可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式例如Sequence Files，RC,ORC等； 2）Compress Intermediate Data:Map输出作为Reducer的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少了存储文件所占空间，提升了数据传输速率，建议使用压缩速度快的压缩方式，例如Snappy和LZO； 3）Compress Reducer Output:进行归档处理或者链接Mapreduce的工作（该作业的输出作为下个作业的输入），压缩可以减少了存储文件所占空间，提升了数据传输速率，如果作为归档处理，可以采用高的压缩比（Gzip,Bzip2），如果作为下个作业的输入，考虑是否要分片进行选择； 压缩配置core-site.xml &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, #zlib-&gt;Default org.apache.hadoop.io.compress.BZip2Codec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec, org.apache.hadoop.io.compress.Lz4Codec, org.apache.hadoop.io.compress.SnappyCodec, &lt;/value&gt; &lt;/property&gt; mapred-site.xml #mapper输出 &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt; #reducer输 &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt; &lt;/property&gt; Hive开启Map输出阶段压缩开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下： 1）开启hive中间传输数据压缩功能 hive&gt;set hive.exec.compress.intermediate=true; 2）开启mapreduce中map输出压缩功能 hive&gt;set mapreduce.map.output.compress=true; 3）设置mapreduce中map输出数据的压缩方式 hive&gt;set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec; 4）执行查询语句 hive&gt; select count(ename) name from emp; Hive开启Reduce输出阶段压缩当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。 1）开启hive最终输出数据压缩功能 hive&gt;set hive.exec.compress.output=true; 2）开启mapreduce最终输出数据压缩 hive&gt;set mapreduce.output.fileoutputformat.compress=true; 3）设置mapreduce最终数据输出压缩方式 hive&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; 4）设置mapreduce最终数据输出压缩为块压缩 hive&gt;set mapreduce.output.fileoutputformat.compress.type=BLOCK; 5）测试一下输出结果是否是压缩文件 insert overwrite local directory &#39;/opt/module/datas/distribute-result&#39; select * from emp distribute by deptno sort by empno desc;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：8、Hadoop HA]]></title>
    <url>%2F2017%2F12%2F27%2Fhadoop-8-hadoop-ha.html</url>
    <content type="text"><![CDATA[准备1、安装jdk/配置ssh/部署好zookeeper集群 2、注意同步所有节点的时间 3、配置主机名与ip映射关系 vi /etc/hosts 192.168.149.141 hadoop001 192.168.149.142 hadoop002 192.168.149.143 hadoop003 hadoop-env.sh配置export JAVA_HOME=/usr/java/jdk core-site.xml配置&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;!--==============================Trash机制======================================= --&gt; &lt;property&gt; &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 --&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/software/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec &lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml配置&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--HDFS超级用户 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;!--开启web hdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/software/hadoop/data/dfs/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;${dfs.namenode.name.dir}&lt;/value&gt; &lt;description&gt;namenode粗放 transaction file(edits)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/opt/software/hadoop/data/dfs/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小256M （默认128M） --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;!--======================================================================= --&gt; &lt;!--HDFS高可用配置 --&gt; &lt;!--指定hdfs的nameservice为mycluster,需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode editlog同步 ============================================ --&gt; &lt;!--保证数据恢复 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt; &lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--JournalNode存放数据地址 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/software/hadoop/data/dfs/jn&lt;/value&gt; &lt;/property&gt; &lt;!--==================DataNode editlog同步 ============================================ --&gt; &lt;property&gt; &lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode fencing：=============================================== --&gt; &lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少milliseconds 认为fencing失败 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--==================NameNode auto failover base ZKFC and Zookeeper====================== --&gt; &lt;!--开启基于Zookeeper --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--动态许可datanode连接namenode列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/software/hadoop/etc/hadoop/slaves&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml配置&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- nodemanager 配置 ================================================= --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23344&lt;/value&gt; &lt;description&gt;Address where the localizer IPC is.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23999&lt;/value&gt; &lt;description&gt;NM Webapp address.&lt;/description&gt; &lt;/property&gt; &lt;!-- HA 配置 =============================================================== --&gt; &lt;!-- Resource Manager Configs --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群名称，确保HA选举时对应的集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--这里RM主备结点需要单独指定,（可选） &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;/property&gt; &lt;!-- ZKRMStateStore 配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23140&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23140&lt;/value&gt; &lt;/property&gt; &lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23130&lt;/value&gt; &lt;/property&gt; &lt;!-- RM admin interface --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23141&lt;/value&gt; &lt;/property&gt; &lt;!--NM访问RM的RPC端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23125&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23125&lt;/value&gt; &lt;/property&gt; &lt;!-- RM web application 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop001:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml配置&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- 配置 MapReduce Applications --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- JobHistory Server ============================================================== --&gt; &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop001:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop001:19888&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 Map段输出的压缩,snappy--&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; slaves配置hadoop001 hadoop002 hadoop003 分发到其他机器scp -r /home/hadoop/apps/hadoop root@hadoop002:/home/hadoop/apps scp -r /home/hadoop/apps/hadoop root@hadoop003:/home/hadoop/apps 启动zookeeper（hadoop001、hadoop002、hadoop003）zkServer.sh start 启动JournalNode(部署了JN的节点)hadoop-daemon.sh start journalnode 格式化HDFScd /home/hadoop/apps/hadoop mkdir data/ cd data/ hdfs namenode -format(格式化命令) cd ../ scp -r data/ root@hadoop002:/home/hadoop/apps/hadoop scp -r data/ root@hadoop003:/home/hadoop/apps/hadoop 在zookeeper的主节点leader上格式化zookeeper集群hdfs zkfc -formatZK 启动hadoop集群start-all.sh 热备的RM要单独启动，故这一步在hadoop002上执行yarn-daemon.sh start resourcemanager]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：7、YARN的内存和CPU配置]]></title>
    <url>%2F2017%2F12%2F26%2Fhadoop-7-hadoop-yarn-detail.html</url>
    <content type="text"><![CDATA[内存配置关于内存相关的配置可以参考hortonwork公司的文档Determine HDP Memory Configuration Settings来配置你的集群。 YARN以及MAPREDUCE所有可用的内存资源应该要除去系统运行需要的以及其他的hadoop的一些程序，总共保留的内存=系统内存+HBASE内存。 可以参考下面的表格确定应该保留的内存: 每台机子内存 系统需要的内存 HBase需要的内存 4GB 1GB 1GB 8GB 2GB 1GB 16GB 2GB 2GB 24GB 4GB 4GB 48GB 6GB 8GB 64GB 8GB 8GB 72GB 8GB 8GB 96GB 12GB 16GB 128GB 24GB 24GB 255GB 32GB 32GB 512GB 64GB 64GB 计算每台机子最多可以拥有多少个container，可以使用下面的公式: containers = min (2*CORES, 1.8*DISKS, (Total available RAM) / MIN_CONTAINER_SIZE) 说明： CORES为机器CPU核数 DISKS为机器上挂载的磁盘个数 Total available RAM为机器总内存 MIN_CONTAINER_SIZE是指container最小的容量大小，这需要根据具体情况去设置，可以参考下面的表格： 每台机子可用的RAM container最小值 小于4GB 256MB 4GB到8GB之间 512MB 8GB到24GB之间 1024MB 大于24GB 2048MB 每个container的平均使用内存大小计算方式为： RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers)) 通过上面的计算，YARN以及MAPREDUCE可以这样配置： 配置文件 配置设置 默认值 计算值 yarn-site.xml yarn.nodemanager.resource.memory-mb 8192 MB = containers * RAM-per-container yarn-site.xml yarn.scheduler.minimum-allocation-mb 1024MB = RAM-per-container yarn-site.xml yarn.scheduler.maximum-allocation-mb 8192 MB = containers * RAM-per-container yarn-site.xml (check) yarn.app.mapreduce.am.resource.mb 1536 MB = 2 * RAM-per-container yarn-site.xml (check) yarn.app.mapreduce.am.command-opts -Xmx1024m = 0.8 * 2 * RAM-per-container mapred-site.xml mapreduce.map.memory.mb 1024 MB = RAM-per-container mapred-site.xml mapreduce.reduce.memory.mb 1024 MB = 2 * RAM-per-container mapred-site.xml mapreduce.map.java.opts = 0.8 * RAM-per-container mapred-site.xml mapreduce.reduce.java.opts = 0.8 * 2 * RAM-per-container table th:nth-of-type(1) { width: 20%; } table th:nth-of-type(2) { width: 45%; } table th:nth-of-type(3) { width: 15%; } table th:nth-of-type(4) { width: 20%; } 举个例子：对于128G内存、32核CPU的机器，挂载了7个磁盘，根据上面的说明，系统保留内存为24G，不适应HBase情况下，系统剩余可用内存为104G，计算containers值如下： containers = min (232, 1.8 7 , (128-24)/2) = min (64, 12.6 , 51) = 13 计算RAM-per-container值如下： RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8 你也可以使用脚本yarn-utils.py来计算上面的值： #!/usr/bin/env python import optparse from pprint import pprint import logging import sys import math import ast &#39;&#39;&#39; Reserved for OS + DN + NM, Map: Memory =&gt; Reservation &#39;&#39;&#39; reservedStack = { 4:1, 8:2, 16:2, 24:4, 48:6, 64:8, 72:8, 96:12, 128:24, 256:32, 512:64} &#39;&#39;&#39; Reserved for HBase. Map: Memory =&gt; Reservation &#39;&#39;&#39; reservedHBase = {4:1, 8:1, 16:2, 24:4, 48:8, 64:8, 72:8, 96:16, 128:24, 256:32, 512:64} GB = 1024 def getMinContainerSize(memory): if (memory &lt;= 4): return 256 elif (memory &lt;= 8): return 512 elif (memory &lt;= 24): return 1024 else: return 2048 pass def getReservedStackMemory(memory): if (reservedStack.has_key(memory)): return reservedStack[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 1 return ret def getReservedHBaseMem(memory): if (reservedHBase.has_key(memory)): return reservedHBase[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 2 return ret def main(): log = logging.getLogger(__name__) out_hdlr = logging.StreamHandler(sys.stdout) out_hdlr.setFormatter(logging.Formatter(&#39; %(message)s&#39;)) out_hdlr.setLevel(logging.INFO) log.addHandler(out_hdlr) log.setLevel(logging.INFO) parser = optparse.OptionParser() memory = 0 cores = 0 disks = 0 hbaseEnabled = True parser.add_option(&#39;-c&#39;, &#39;--cores&#39;, default = 16, help = &#39;Number of cores on each host&#39;) parser.add_option(&#39;-m&#39;, &#39;--memory&#39;, default = 64, help = &#39;Amount of Memory on each host in GB&#39;) parser.add_option(&#39;-d&#39;, &#39;--disks&#39;, default = 4, help = &#39;Number of disks on each host&#39;) parser.add_option(&#39;-k&#39;, &#39;--hbase&#39;, default = &quot;True&quot;, help = &#39;True if HBase is installed, False is not&#39;) (options, args) = parser.parse_args() cores = int (options.cores) memory = int (options.memory) disks = int (options.disks) hbaseEnabled = ast.literal_eval(options.hbase) log.info(&quot;Using cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;GB&quot; + &quot; disks=&quot; + str(disks) + &quot; hbase=&quot; + str(hbaseEnabled)) minContainerSize = getMinContainerSize(memory) reservedStackMemory = getReservedStackMemory(memory) reservedHBaseMemory = 0 if (hbaseEnabled): reservedHBaseMemory = getReservedHBaseMem(memory) reservedMem = reservedStackMemory + reservedHBaseMemory usableMem = memory - reservedMem memory -= (reservedMem) if (memory &lt; 2): memory = 2 reservedMem = max(0, memory - reservedMem) memory *= GB containers = int (min(2 * cores, min(math.ceil(1.8 * float(disks)), memory/minContainerSize))) if (containers &lt;= 2): containers = 3 log.info(&quot;Profile: cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;MB&quot; + &quot; reserved=&quot; + str(reservedMem) + &quot;GB&quot; + &quot; usableMem=&quot; + str(usableMem) + &quot;GB&quot; + &quot; disks=&quot; + str(disks)) container_ram = abs(memory/containers) if (container_ram &gt; GB): container_ram = int(math.floor(container_ram / 512)) * 512 log.info(&quot;Num Container=&quot; + str(containers)) log.info(&quot;Container Ram=&quot; + str(container_ram) + &quot;MB&quot;) log.info(&quot;Used Ram=&quot; + str(int (containers*container_ram/float(GB))) + &quot;GB&quot;) log.info(&quot;Unused Ram=&quot; + str(reservedMem) + &quot;GB&quot;) log.info(&quot;yarn.scheduler.minimum-allocation-mb=&quot; + str(container_ram)) log.info(&quot;yarn.scheduler.maximum-allocation-mb=&quot; + str(containers*container_ram)) log.info(&quot;yarn.nodemanager.resource.memory-mb=&quot; + str(containers*container_ram)) map_memory = container_ram reduce_memory = 2*container_ram if (container_ram &lt;= 2048) else container_ram am_memory = max(map_memory, reduce_memory) log.info(&quot;mapreduce.map.memory.mb=&quot; + str(map_memory)) log.info(&quot;mapreduce.map.java.opts=-Xmx&quot; + str(int(0.8 * map_memory)) +&quot;m&quot;) log.info(&quot;mapreduce.reduce.memory.mb=&quot; + str(reduce_memory)) log.info(&quot;mapreduce.reduce.java.opts=-Xmx&quot; + str(int(0.8 * reduce_memory)) + &quot;m&quot;) log.info(&quot;yarn.app.mapreduce.am.resource.mb=&quot; + str(am_memory)) log.info(&quot;yarn.app.mapreduce.am.command-opts=-Xmx&quot; + str(int(0.8*am_memory)) + &quot;m&quot;) log.info(&quot;mapreduce.task.io.sort.mb=&quot; + str(int(0.4 * map_memory))) pass if __name__ == &#39;__main__&#39;: try: main() except(KeyboardInterrupt, EOFError): print(&quot;\nAborting ... Keyboard Interrupt.&quot;) sys.exit(1) 执行下面命令： python yarn-utils.py -c 32 -m 128 -d 7 -k False 返回结果如下： Using cores=32 memory=128GB disks=7 hbase=False Profile: cores=32 memory=106496MB reserved=24GB usableMem=104GB disks=7 Num Container=13 Container Ram=8192MB Used Ram=104GB Unused Ram=24GB yarn.scheduler.minimum-allocation-mb=8192 yarn.scheduler.maximum-allocation-mb=106496 yarn.nodemanager.resource.memory-mb=106496 mapreduce.map.memory.mb=8192 mapreduce.map.java.opts=-Xmx6553m mapreduce.reduce.memory.mb=8192 mapreduce.reduce.java.opts=-Xmx6553m yarn.app.mapreduce.am.resource.mb=8192 yarn.app.mapreduce.am.command-opts=-Xmx6553m mapreduce.task.io.sort.mb=3276 这样的话，每个container内存为8G，似乎有点多，我更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下： 配置文件 配置设置 计算值 yarn-site.xml yarn.nodemanager.resource.memory-mb = 52 * 2 =104 G yarn-site.xml yarn.scheduler.minimum-allocation-mb = 2G yarn-site.xml yarn.scheduler.maximum-allocation-mb = 52 * 2 = 104G yarn-site.xml (check) yarn.app.mapreduce.am.resource.mb = 2 * 2=4G yarn-site.xml (check) yarn.app.mapreduce.am.command-opts = 0.8 * 2 * 2=3.2G mapred-site.xml mapreduce.map.memory.mb = 2G mapred-site.xml mapreduce.reduce.memory.mb = 2 * 2=4G mapred-site.xml mapreduce.map.java.opts = 0.8 * 2=1.6G mapred-site.xml mapreduce.reduce.java.opts = 0.8 * 2 * 2=3.2G 对应的xml配置为： &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt; &lt;/property&gt; 另外，还有一下几个参数： yarn.nodemanager.vmem-pmem-ratio：任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1。 yarn.nodemanager.pmem-check-enabled：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 yarn.nodemanager.vmem-pmem-ratio：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 第一个参数的意思是当一个map任务总共分配的物理内存为2G的时候，该任务的container最多内分配的堆内存为1.6G，可以分配的虚拟内存上限为2*2.1=4.2G。另外，照这样算下去，每个节点上YARN可以启动的Map数为104/2=52个。 CPU配置YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。 在YARN中，CPU相关配置参数如下： yarn.nodemanager.resource.cpu-vcores：表示该节点上YARN可使用的虚拟CPU个数，默认是8，注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。 yarn.scheduler.minimum-allocation-vcores：单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。 yarn.scheduler.maximum-allocation-vcores：单个任务可申请的最多虚拟CPU个数，默认是32。 对于一个CPU核数较多的集群来说，上面的默认配置显然是不合适的，在我的测试集群中，4个节点每个机器CPU核数为31，留一个给操作系统，可以配置为： &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;31&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;124&lt;/value&gt; &lt;/property&gt; 参考文章Determine HDP Memory Configuration Settings Hadoop YARN如何调度内存和CPU 转自 http://blog.javachen.com/2015/06/05/yarn-memory-and-cpu-configuration.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：6、Hadoop常用命令]]></title>
    <url>%2F2017%2F12%2F21%2Fhadoop-6-hadoop-common-command.html</url>
    <content type="text"><![CDATA[hadoop fs 等同 hdfs dfsdfsadmin//查看文件系统的基本信息和统计信息 hdfs dfsadmin -report //进入安全模式 hdfs dfsadmin -safemode enter //安全模式不能上传文件 //安全模式是NameNode的一种状态，在这种状态下，NameNode是只读状态，不复制或删除块。 hdfs dfs -put jepson.log / put: Cannot create file/jepson.log._COPYING_. Name node is in safe mode. //退出安全模式 hdfs dfsadmin -safemode leave 多台机器的磁盘存储分布不均匀解决方案不加新机器，原机器的磁盘分布不均匀 [hadoop@hadoop-01 ~]$ hdfs dfsadmin -setBalancerBandwidth 52428800 Balancer bandwidth is set to 52428800 [hadoop@hadoop-01 sbin]$ ./start-balancer.sh 等价 [hadoop@hadoop-01 sbin]$ hdfs balancer Apache Hadoop集群环境: shell脚本每晚业务低谷时调度 CDH集群环境: 忽略 网络和io最高的，也是最有风险性的: 加新机器，原机器的磁盘比如450G(500G),现在的新机器磁盘规格是5T 在业务低谷时，先将多台新机器加入到HDFS，做DN； 然后选一台的DN下架掉，等待hdfs自我修复块，恢复3份 一台机器的多个磁盘分布不均匀解决方案无论加不加磁盘，且多块磁盘的分布不均匀 hdfs diskbalancer -plan node1.mycluster.com hdfs diskbalancer -execute /system/diskbalancer/nodename.plan.json 将文件上传至hadoop的根目录/下载至本地hadoop fs -put filename / hadoop fs -get /filename # &#39;/&#39;不是Linux的根目录，表示hadoop的根目录 例： [hadoop@hadoop1 ~]$ hdfs dfs -put test.log / [hadoop@hadoop1 data]$ hdfs dfs -get /test.log [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] 查看hadoop里的文件hadoop fs -ls / 例： [hadoop@hadoop1 ~]$ hadoop fs -ls / Found 3 items -rw-r--r-- 1 hadoop supergroup 6 2017-12-19 11:18 /test.log drwx------ - hadoop supergroup 0 2017-12-14 15:17 /tmp drwxr-xr-x - hadoop supergroup 0 2017-12-14 15:17 /user 查看hadoop里的文件的内容hadoop fs -cat filename 例： [hadoop@hadoop1 ~]$ hadoop fs -cat /test.log hello 创建文件夹hadoop fs -mkdir -p /filename/filename 例： [hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /dwz01/dwz001 删除[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...] // 配置回收站 vi core-site.xml &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10080&lt;/value&gt; # 回收站保留时间（分钟） &lt;/property&gt; // 测试 hadoop fs -rm -r -f /xxxx ---》进入回收站，是可以恢复的 hadoop fs -rm -r -f -skipTrash /xxxx ---》不进入回收站，是不可以恢复的 例： A.进入回收站 [hadoop@hadoop1 hadoop]$ hadoop fs -rm -r -f /test.log 17/12/19 14:06:06 INFO fs.TrashPolicyDefault: Moved: &#39;hdfs://192.168.137.130:9000/test.log&#39; to trash at: hdfs://192.168.137.130:9000/user/hadoop/.Trash/Current/test.log [hadoop@hadoop1 hadoop]$ hadoop fs -ls Found 1 items drwx------ - hadoop supergroup 0 2017-12-19 14:06 .Trash [hadoop@hadoop1 data]$ hadoop fs -ls .Trash/Current Found 1 items -rw-r--r-- 1 hadoop supergroup 6 2017-12-19 14:12 .Trash/Current/test.log 恢复刚刚删除的目录 [hadoop@hadoop1 hadoop]$ hadoop fs -mv /user/hadoop/.Trash/Current/test.log / [hadoop@hadoop1 hadoop]$ hadoop fs -ls / Found 4 items drwxr-xr-x - hadoop supergroup 0 2017-12-19 11:44 /dwz01 -rw-r--r-- 1 hadoop supergroup 6 2017-12-19 11:18 /test.log drwx------ - hadoop supergroup 0 2017-12-14 15:17 /tmp drwxr-xr-x - hadoop supergroup 0 2017-12-14 15:17 /user B.不进入回收站 [hadoop@hadoop1 hadoop]$ hadoop fs -rm -r -f -skipTrash /test.log Deleted /test.log [hadoop@hadoop1 hadoop]$ hadoop fs -ls Found 1 items drwx------ - hadoop supergroup 0 2017-12-19 14:06 .Trash [hadoop@hadoop1 hadoop]$ hadoop fs -ls .Trash Found 1 items drwx------ - hadoop supergroup 0 2017-12-19 14:09 .Trash/Current [hadoop@hadoop1 hadoop]$ hadoop fs -ls .Trash/Current [hadoop@hadoop1 hadoop]$]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：5、MapReduce详解]]></title>
    <url>%2F2017%2F12%2F20%2Fhadoop-5-hadoop-mapreduce-detail.html</url>
    <content type="text"><![CDATA[MapReduce架构设计MapReduce1架构设计 主要有四个组成部分 Client: 客户端 JobTracker: 负责资源监控和作业调度 JobTracker监控所有TaskTracker与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点 JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源 在Hadoop中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器 TaskTracker TaskTracker会周期性的通过Heartbeat将本节点上资源的使用情况和任务的运行进度汇报给JobTracker 接收JobTracker发送过来的命令并执行相应的操作 TaskTracker使用”slot”等量划分本节点上的资源量 Task: 分为Map Task和Reduce Task,均由TaskTracker启动 Map Task: 映射任务 Reduce Task: 归约任务 MapReduce2架构设计 相同叫法 MapReduce提交到Yarn的工作流程 Yarn的工作流程 Yarn的架构设计 Client向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等 ResourceManger为该应用程序分配第一个Container，并与对应的Node-Manger通信，要求它在这个Container中启动应用程序的ApplicationMaster ApplicationMaster首先向ResourceManger注册，这样用户可以直接通过ResourceManger查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7 ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务 NodeManager为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务 各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度,以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己 当用户向YARN中提交一个应用程序后,YARN将分两个阶段运行该应用程序: a. 第一个阶段是启动ApplicationMasterb. 第二个阶段是由ApplicationMaster创建应用程序,为它申请资源,并监控它的整个运行过程,直到运行完成 shuffle 洗牌 调优点 hive+sparkshuffle 词频统计：wordcount[hadoop@hadoop1 hadoop]$ vi 1.log bbb 123 1 2 2 aaa bbb ccc bbb 123 1 2 2 aaa 123 bbb ccc xxxxx [hadoop@hadoop1 hadoop]$ hdfs dfs -mkdir -p /wordcount/input [hadoop@hadoop1 hadoop]$ hdfs dfs -put 1.log /wordcount/input [hadoop@hadoop1 hadoop]$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar \ &gt; wordcount /wordcount/input /wordcount/output1 17/12/19 22:44:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 17/12/19 22:44:34 INFO input.FileInputFormat: Total input files to process : 1 17/12/19 22:44:34 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1245) at java.lang.Thread.join(Thread.java:1319) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/12/19 22:44:34 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1245) at java.lang.Thread.join(Thread.java:1319) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/12/19 22:44:34 INFO mapreduce.JobSubmitter: number of splits:1 17/12/19 22:44:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1513689033755_0001 17/12/19 22:44:35 INFO impl.YarnClientImpl: Submitted application application_1513689033755_0001 17/12/19 22:44:35 INFO mapreduce.Job: The url to track the job: http://hadoop1:8088/proxy/application_1513689033755_0001/ 17/12/19 22:44:35 INFO mapreduce.Job: Running job: job_1513689033755_0001 17/12/19 22:44:54 INFO mapreduce.Job: Job job_1513689033755_0001 running in uber mode : false 17/12/19 22:44:54 INFO mapreduce.Job: map 0% reduce 0% 17/12/19 22:45:03 INFO mapreduce.Job: map 100% reduce 0% 17/12/19 22:45:12 INFO mapreduce.Job: map 100% reduce 100% 17/12/19 22:45:13 INFO mapreduce.Job: Job job_1513689033755_0001 completed successfully 17/12/19 22:45:13 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=98 FILE: Number of bytes written=272817 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=181 HDFS: Number of bytes written=60 HDFS: Number of read operations=6 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Launched reduce tasks=1 Data-local map tasks=1 Total time spent by all maps in occupied slots (ms)=6958 Total time spent by all reduces in occupied slots (ms)=6334 Total time spent by all map tasks (ms)=6958 Total time spent by all reduce tasks (ms)=6334 Total vcore-milliseconds taken by all map tasks=6958 Total vcore-milliseconds taken by all reduce tasks=6334 Total megabyte-milliseconds taken by all map tasks=7124992 Total megabyte-milliseconds taken by all reduce tasks=6486016 Map-Reduce Framework Map input records=8 Map output records=14 Map output bytes=122 Map output materialized bytes=98 Input split bytes=114 Combine input records=14 Combine output records=8 Reduce input groups=8 Reduce shuffle bytes=98 Reduce input records=8 Reduce output records=8 Spilled Records=16 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=187 CPU time spent (ms)=1740 Physical memory (bytes) snapshot=329904128 Virtual memory (bytes) snapshot=4130594816 Total committed heap usage (bytes)=202379264 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=67 File Output Format Counters Bytes Written=60 [hadoop@hadoop1 hadoop]$ hdfs dfs -ls /wordcount/output1 Found 2 items -rw-r--r-- 1 hadoop supergroup 0 2017-12-19 22:45 /wordcount/output1/_SUCCESS -rw-r--r-- 1 hadoop supergroup 60 2017-12-19 22:45 /wordcount/output1/part-r-00000 -rw-r--r-- 1 hadoop supergroup 60 2017-12-19 22:45 /wordcount/output1/part-r-00001 [hadoop@hadoop1 hadoop]$ hdfs dfs -cat /wordcount/output1/part-r-00000 1 1 123 2 2 2 aaa 2 bbb 3 ccc 2 xxxxx 1 [hadoop@hadoop1 hadoop]$]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：4、HDFS详解]]></title>
    <url>%2F2017%2F12%2F17%2Fhadoop-4-hadoop-hdfs-detail.html</url>
    <content type="text"><![CDATA[架构设计NameNode 存储:文件系统的命名空间 文件名称 文件目录结构 文件的属性（权限 创建时间 副本数） 文件对应哪些数据块 –&gt; 这些数据块对应哪些DataNode节点上 不会持久化存储这个映射关系，是通过集群的启动和运行时，DataNode定期发送blockReport给NameNode，以此NameNode在【内存】中动态维护这种映射关系。 作用：管理文件系统的命名空间。它维护着文件系统树及整棵树内所有的文件和目录。 这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件fsimage和编辑日志文件editlog。 DataNode 存储:数据块和数据块校验和 与Namenode通信: 每隔3秒发送一个心跳包 每十次心跳发送一次blockReport 作用(主要):读写文件的数据块 Scondary NameNode 存储:命名空间镜像文件fsimage 和 编辑日志editlog 作用:定期合并fsimage+editlog文件为新的fsimage推送给namenode.俗称检查点动作,checkpoint. 参数:hdfs-default.xml文件 dfs.namenode.checkpoint.period: 3600 秒 副本放置策略 第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上 第二副本：放置在与第一个副本不同的机架的节点上 第三副本：与第二个副本相同机架的不同节点上 如果还有更多的副本：随机放在节点中 读写流程HDFS读流程 Client通过FileSystem.open()方法，去与NameNode进行rpc通信，返回该文件的部分或全部的block列表(也包含该列表各block的分布在Datanode地址的列表)，也就是返回FSDataInputStream对象 Client调用FSDataInputStream.read()方法 与第一个块的最近的Datanode进行read，读取完毕后会check，若sucessful，会关闭与当前Datanode通信；若check file，会记录失败的块和Datanode信息，下次就不会读取，那么就会去该块的第二个Datanode地址读取； 然后去第二个块的最近的Datanode上的进行读取,check后,会关闭与此datanode的通信； 假如block列表读取完了,文件还未结束,那么FileSystem会从Namenode获取下一批的block的列表； Client调用FSDataInputStream.close()方法，关闭输入流 HDFS写流程 Client调用FileSystem.create()方法，去与Namenode进行rpc通信，check该路径的文件是否存在以及有没有权限创建该文件，假如ok，就创建一个新文件，但是并不关联任何block，返回一个FSDataOutputStream对象；假如not ok，就返回错误信息,所以写代码要try-catch Client调用FSDataOutputStream.write()方法 将第一个块写入第一个Datanode，第一个Datanode写完传给第二个节点，第二个写完传给第三节点，当第三个节点写完返回一个ackpacket给第二个节点，第二个返回一个ackpacket给第一个节点，第一个节点返回ackpacket给FSDataOutputStream对象，意思标识第一个块写完，副本数为3；然后剩余的块依次这样写 当向文件写入数据完成后,Client调用FSDataOutputStream.close()方法,关闭输出流,flush缓存区的数据包; 再调用FileSystem.complete()方法,告诉Namenode节点写入成功 进程通过jps命令查看进程 NameNode SecondaryNameNode DataNode 其各自进程会生成一个对应的pid文件 常用命令jps查看进程[hadoop@hadoop1 ~]$ jps 54450 Jps 10611 NodeManager 5720 NameNode 6030 SecondaryNameNode 5823 DataNode 10511 ResourceManager [hadoop@hadoop1 ~]$ [hadoop@hadoop1 ~]$ which jps /usr/java/jdk1.8.0_45/bin/jps [hadoop@hadoop1 ~]$ [root@hadoop1 ~]# cd /tmp/hsperfdata_hadoop/ [root@hadoop1 hsperfdata_hadoop]# ll total 160 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:11 10511 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:11 10611 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:11 5720 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:11 5823 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:11 6030 正常流程[root@hadoop1 hsperfdata_hadoop]# jps 10611 -- process information unavailable 6325 jar 54487 Jps 5720 -- process information unavailable 6030 -- process information unavailable 5823 -- process information unavailable 10511 -- process information unavailable [root@hadoop1 hsperfdata_hadoop]# ps -ef |greo 10611 找到该进程的使用用户名称 [root@hadoop1 hsperfdata_hadoop]# su - hadoop [hadoop@hadoop1 ~]$ jps 10611 NodeManager 5720 NameNode 54524 Jps 6030 SecondaryNameNode 5823 DataNode 10511 ResourceManager [hadoop@hadoop1 ~]$ 异常流程[root@hadoop1 rundeck]# jps 10611 -- process information unavailable 6325 jar 5720 -- process information unavailable 6030 -- process information unavailable 54591 Jps 5823 -- process information unavailable 10511 -- process information unavailable [root@hadoop1 rundeck]# [root@hadoop1 rundeck]# kill -9 10611 [root@hadoop1 rundeck]# jps 10611 -- process information unavailable 6325 jar 5720 -- process information unavailable 54605 Jps 6030 -- process information unavailable 5823 -- process information unavailable 10511 -- process information unavailable [root@hadoop1 rundeck]# ps -ef|grep 10611 root 54618 48324 0 10:15 pts/1 00:00:00 grep 10611 [root@hadoop1 rundeck]# 10611信息残留，去/tmp/hsperfdata_hadoop文件夹删除该10611文件 [root@hadoop1 hsperfdata_hadoop]# ll total 160 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:17 10511 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:15 10611 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:17 5720 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:17 5823 -rw-------. 1 hadoop hadoop 32768 Dec 17 10:16 6030 [root@hadoop1 hsperfdata_hadoop]# rm -f 10611 [root@hadoop1 hsperfdata_hadoop]# [root@hadoop1 hsperfdata_hadoop]# jps 54626 Jps 6325 jar 5720 -- process information unavailable 6030 -- process information unavailable 5823 -- process information unavailable 10511 -- process information unavailable [root@hadoop1 hsperfdata_hadoop]# su - hadoop [hadoop@hadoop1 ~]$ jps 54661 Jps 5720 NameNode 6030 SecondaryNameNode 5823 DataNode 10511 ResourceManager [hadoop@hadoop1 ~]$ hadoop和hdfs文件系统命令hadoop fs 等价 hdfs dfs [hadoop@hadoop1 hadoop]$ bin/hdfs dfs -ls / [hadoop@hadoop1 hadoop]$ bin/hdfs dfs -mkdir -p /rzdatadir001/001 [hadoop@hadoop1 hadoop]$ bin/hdfs dfs -cat /test.log [hadoop@hadoop1 hadoop]$ bin/hdfs dfs -put rzdata.log1 /rzdatadir001/001 [hadoop@hadoop1 hadoop]$ bin/hdfs dfs -get /rzdatadir001/001/rzdata.log1 /tmp/ [hadoop@hadoop1 hadoop]$ bin/hdfs dfs -get /rzdatadir001/001/rzdata.log1 /tmp/rzdata.log123 重命名 [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] 配置回收站[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...] core-site.xml fs.trash.interval : 10080 //测试 bin/hdfs dfs -rm -r -f /xxxx ---》进入回收站，是可以恢复的 bin/hdfs dfs -rm -r -f -skipTrash /xxxx ---》不进入回收站，是不可以恢复的]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：3、Hadoop伪分布式部署(MapReduce+Yarn)]]></title>
    <url>%2F2017%2F12%2F16%2Fhadoop-3-hadoop-pseudo-distributed-mapreduce-yarn.html</url>
    <content type="text"><![CDATA[MapReduce:计算 Yarn: 资源(CPU、内存等)调度和作业(程序)平台 修改mapred-site.xml [hadoop@hadoop-01 ~]# cd /opt/software/hadoop/etc/hadoop [hadoop@hadoop-01 hadoop]# cp mapred-site.xml.template mapred-site.xml [hadoop@hadoop-01 hadoop]# vi mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改yarn-site.xml [hadoop@hadoop-01 hadoop]# vi yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动 [hadoop@hadoop-01 hadoop]# cd ../../ [hadoop@hadoop-01 hadoop]# sbin/start-yarn.sh 访问：http://192.168.137.130:8088 关闭 [hadoop@hadoop-01 hadoop]# sbin/stop-yarn.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux：5、Linux配置多台机器SSH相互通信信任]]></title>
    <url>%2F2017%2F12%2F15%2Flinux-5-linux-ssh.html</url>
    <content type="text"><![CDATA[配置多台机器SSH相互通信信任2台机器分别执行ssh-keygen生成公钥和密钥 [root@hadoop-01 ~]# ssh-keygen 生成.ssh文件夹及id_rsa和id_rsa.pub [root@hadoop-01 .ssh]# ll total 16 -rw-------. 1 root root 1675 Dec 13 21:37 id_rsa -rw-r--r--. 1 root root 396 Dec 13 21:37 id_rsa.pub 选取第一台,生成authorized_keys文件 [root@hadoop-01 ~]# cd .ssh [root@hadoop-01 .ssh]# cat ./id_rsa.pub &gt;&gt; ./authorized_keys 将另一台id_rsa.pub内容,手动copy到第一台的authorized_keys文件 [root@hadoop-02 .ssh]# more id_rsa.pub 拷贝至`authorized_keys`文件(注意copy时,最好先放到记事本中,将回车去掉,成为一行) 设置每台机器的权限 [root@hadoop-01 ~]# chmod 700 -R ~/.ssh [root@hadoop-01 ~]# chmod 600 ~/.ssh/authorized_keys 将第一台的authorized_keys scp 给hadoop-02(第一次传输,需要输入密码) [root@hadoop-01 ~]# scp authorized_keys root@hadoop-02:/root/.ssh 配置/etc/hosts(两台机器都要配置) [root@hadoop-01 ~]# vi /etc/hosts 将两台机器的IP和机器名都写入 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.137.130 hadoop-01 192.168.137.131 hadoop-02 验证(每台机器上执行下面的命令,只输入yes,不输入密码,则这两台互相通信了) [root@hadoop-01 ~]# ssh root@hadoop-02 date [root@hadoop-02 ~]# ssh root@hadoop-01 date]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume：2、Flume使用案例]]></title>
    <url>%2F2017%2F12%2F15%2Fflume-2-flume-use.html</url>
    <content type="text"><![CDATA[Flume使用案例官网地址 Source（NetCat） Sink（logger） Channel（memory）NetCat Source：监听一个指定的网络端口，即只要应用程序向这个端口里面写数据，这个source组件就可以获取到信息。 Property Name Default Description channels – type – The component type name, needs to be netcat bind – 日志需要发送到的主机名或者Ip地址，该主机运行着netcat类型的source在监听 port – 日志需要发送到的端口号，该端口号要有netcat类型的source在监听 配置 vi hello.ctest.confonf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = 0.0.0.0 a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 帮助命令 flume-ng help 启动命令 ./flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf/test.conf -Dflume.root.logger=INFO,console --name agent的名字 --conf conf目录 --conf-file 配置文件所在目录 -Dflume.root.logger=INFO,console 可以再控制台查看 测试 [root@hadoop001 ~]$ telnet localhost 44444 Trying ::1... Connected to localhost. Escape character is &#39;^]&#39;. hello OK wold OK Source（NetCat） Sink（hdfs） Channel（file）将日志写入到hdfs上 配置 vi test1.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = 0.0.0.0 a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://hadoop:9000/flume/ a1.sinks.k1.hdfs.writeFormat = Text a1.sinks.k1.hdfs.fileType = DataStream a1.sinks.k1.hdfs.rollInterval = 10 a1.sinks.k1.hdfs.rollSize = 0 a1.sinks.k1.hdfs.rollCount = 0 a1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%H-%M-%S a1.sinks.k1.hdfs.useLocalTimeStamp = true # Use a channel which buffers events in file a1.channels.c1.type = file # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动命令 ./flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf/test1.conf -Dflume.root.logger=DEBUG,console 2017-12-10 18:55:57,906 (lifecycleSupervisor-1-3) [DEBUG - org.apache.flume.source.NetcatSource.start(NetcatSource.java:190)] Source started 2017-12-10 18:55:57,907 (Thread-2) [DEBUG - org.apache.flume.source.NetcatSource$AcceptHandler.run(NetcatSource.java:270)] Starting accept handler 测试 [root@hadoop001 ~]$ telnet localhost 44444 Trying ::1... Connected to localhost. Escape character is &#39;^]&#39;. hello world OK hdfs dfs -text /flume/2017-12-10-19-05-47.1524366347156 hello world Source（Spooling Directory) Sink（hdfs) Channel（memory）Spooling Directory Source：监听一个指定的目录，即只要应用程序向这个指定的目录中添加新的文件，source组件就可以获取到该信息，并解析该文件的内容，然后写入到channle。写入完成后，标记该文件已完成或者删除该文件。官网介绍，其可靠性较强，而且即使flume重启，也不会丢失数据，为了保证可靠性，只能是不可变的，唯一命名的文件可以放在目录下，日常来说，我们可以通过log4j来定义日志名称，这样基本不会重名，而且日志文件生成之后，一般来说都不会更改，所以离线数据处理，很适合使用本Source； flume官网中Spooling Directory Source描述： Property Name Default Description channels – type – The component type name, needs to be spooldir. spoolDir – Spooling Directory Source监听的目录 fileSuffix .COMPLETED 文件内容写入到channel之后，标记该文件 deletePolicy never 文件内容写入到channel之后的删除策略: never or immediate fileHeader false Whether to add a header storing the absolute path filename. ignorePattern ^$ Regular expression specifying which files to ignore (skip) interceptors – 指定传输中event的head(头信息)，常用timestamp Spooling Directory Source的注意事项： 拷贝到spool目录下的文件不可以再打开编辑 不能将具有相同文件名字的文件拷贝到这个目录下 配置 # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = spooldir a1.sources.r1.spoolDir = /home/hadoop/inputfile a1.sources.r1.fileHeader = true a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = timestamp # Describe the sink # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://hadoop:9000/flume/ a1.sinks.k1.hdfs.writeFormat = Text a1.sinks.k1.hdfs.fileType = DataStream a1.sinks.k1.hdfs.rollInterval = 10 a1.sinks.k1.hdfs.rollSize = 0 a1.sinks.k1.hdfs.rollCount = 0 a1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%H-%M-%S a1.sinks.k1.hdfs.useLocalTimeStamp = true # Use a channel which buffers events in file a1.channels.c1.type = file # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 ./flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf/test2.conf -Dflume.root.logger=INFO,console 2017-12-10 19:52:14,083 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.channel.file.FileChannel.start(FileChannel.java:301)] Queue Size after replay: 0 [channel=c1] 2017-12-10 19:52:14,186 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean. 2017-12-10 19:52:14,188 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started 2017-12-10 19:52:14,188 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1 2017-12-10 19:52:14,190 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r1 2017-12-10 19:52:14,190 (lifecycleSupervisor-1-3) [INFO - org.apache.flume.source.SpoolDirectorySource.start(SpoolDirectorySource.java:78)] SpoolDirectorySource source starting with directory: /home/hadoop/inputfile 2017-12-10 19:52:14,200 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SINK, name: k1: Successfully registered new MBean. 2017-12-10 19:52:14,201 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SINK, name: k1 started 2017-12-10 19:52:14,240 (lifecycleSupervisor-1-3) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean. 2017-12-10 19:52:14,241 (lifecycleSupervisor-1-3) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SOURCE, name: r1 started cp input.txt ../inputfile/ 结果 [root@hadoop001 conf]$ hdfs dfs -text /flume/2017-12-10-19-53-24.1524369204246 hello java [root@hadoop001 conf]$ hdfs dfs -text /flume/2017-12-10-19-53-26.1524369206318 hello hadoop hello hive hello sqoop hello hdfs hello spark Source（Exec Source ) Sink（hdfs) Channel（memory）Exec Source：监听一个指定的命令，获取一条命令的结果作为它的数据源常用的是tail -F file指令，即只要应用程序向日志(文件)里面写数据，source组件就可以获取到日志(文件)中最新的内容 。 配置 # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/hadoop/data/data.log # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://hadoop:9000/flume a1.sinks.k1.hdfs.writeFormat = Text a1.sinks.k1.hdfs.fileType = DataStream a1.sinks.k1.hdfs.rollInterval = 10 a1.sinks.k1.hdfs.rollSize = 0 a1.sinks.k1.hdfs.rollCount = 0 a1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%H-%M-%S a1.sinks.k1.hdfs.useLocalTimeStamp = true # Use a channel which buffers events in memory a1.channels.c1.type=memory a1.channels.c1.capacity=10000 a1.channels.c1.transactionCapacity=1000 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 在hive中建立外部表hdfs://hadoop001:9000/flume的目录，方便查看日志捕获内容 create external table t1(infor string) row format delimited fields terminated by &#39;\t&#39; location &#39;/flume/&#39;; 启动 ./flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf/test3.conf -Dflume.root.logger=INFO,console echo hadoop &gt;data.log 结果 hdfs： [root@hadoop001 ~]$ hdfs dfs -text /flume/2017-12-10-20-13-38.1524370418338 hadoop hive: hive&gt; select * from t1; OK hello world hello java hello hadoop hello hive hello sqoop hello hdfs hello spark hadoop Exec source： Exec source和Spooling Directory Source是两种常用的日志采集的方式，其中Exec source可以实现对日志的实时采集，Spooling Directory Source在对日志的实时采集上稍有欠缺，尽管Exec source可以实现对日志的实时采集，但是当Flume不运行或者指令执行出错时，Exec source将无法收集到日志数据，日志会出现丢失，从而无法保证收集日志的完整性。 一台机器向另一台机器的文件传输（Avro）A机器：avro-client B机器：avro-source ==&gt; channel（memory） ==&gt;sink(logger) A机器向B机器传输日志 B机器配置文件 a1.sources=r1 a1.sinks=k1 a1.channels=c1 a1.sources.r1.type=avro a1.sources.r1.bind=0.0.0.0 a1.sources.r1.port=44444 a1.channels.c1.type=memory a1.sinks.k1.type=logger a1.sinks.k1.channel=c1 a1.sources.r1.channels=c1 执行命令 ./flume-ng agent \ --name a1 \ --conf $FLUME_HOME/conf \ --conf-file $FLUME_HOME/conf/avro.conf \ -Dflume.root.logger=INFO,console 在A机器上执行： ./flume-ng avro-client --host 0.0.0.0 --port 44444 --filename /home/hadoop/data/input.txt 注意：这种方式只能传一次，完了就会中断。这种方式在生产上肯定是不行的，那该如何是好呢，下面我们介绍另外一种方式。 A机器到B机器的文件传输（avro），不中断A机器的agent a1.sources=r1 a1.sinks=k1 a1.channels=c1 a1.sources.r1.type=exec a1.sources.r1.command=tail -F /home/hadoop/data/data.log a1.channels.c1.type=memory a1.sinks.k1.type=avro a1.sinks.k1.bind=0.0.0.0 //与B机器像对应 a1.sinks.k1.port=44444 //与B机器相对应 a1.sinks.k1.channel=c1 a1.sources.r1.channels=c1 B机器的agent b1.sources=r1 b1.sinks=k1 b1.channels=c1 b1.sources.r1.type=avro a1.sources.r1.bind = 0.0.0.0 //与A机器像对应 a1.sources.r1.port = 44444 //与A机器像对应 b1.channels.c1.type=memory b1.sinks.k1.type=logger b1.sinks.k1.channel=c1 b1.sources.r1.channels=c1 执行命令 B机器 ./flume-ng agent \ --name b1 \ --conf $FLUME_HOME/conf \ --conf-file $FLUME_HOME/conf/avro_source.conf \ -Dflume.root.logger=INFO,console A机器 ./flume-ng agent \ --name a1 \ --conf $FLUME_HOME/conf \ --conf-file $FLUME_HOME/conf/avro_sink.conf \ -Dflume.root.logger=INFO,console [root@hadoop001 data]$ echo aaa &gt; data.log [root@hadoop001 data]$ echo 112121 &gt; data.log A机器目标目录下文件内容会被打印在B机器的控制台上]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见错误：2、Hadoop多次格式化导致DataNode无法启动]]></title>
    <url>%2F2017%2F12%2F13%2Ferror-2-hadoop-multiple-formatting-datanode-cannot-startup.html</url>
    <content type="text"><![CDATA[hadoop namenode -format 多次格式化后，datanode启动不了 报错信息 2017-12-12 08:05:57,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: &lt;default&gt; at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:409) at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:388) at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:556) at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1566) at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1527) at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:327) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746) at java.lang.Thread.run(Thread.java:745) 2017-12-14 05:07:58,922 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool &lt;registering&gt; (Datanode Uuid 63404450-ed85-4636-8eac-ea75dba1d424) service to hadoop/192.168.137.5:9000. Exiting. java.io.IOException: All specified directories are failed to load. at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:557) at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1566) at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1527) at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:327) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266) at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746) at java.lang.Thread.run(Thread.java:745) 注意 Incompatible clusterIDs in /tmp/hadoop-hadoop/dfs/data: namenode clusterID = CID-c80f243c-4a07-43f3-9eb8-f40d164a4520; datanode clusterID = CID-3e6fcd99-a2fe-42f3-9ccf-bc257a065eb3 1 可以看出，namenode的clusterID和datanode的clusterID不同，导致其无法启动。原因是我们多次格式化namenode导致两者id不同，无法启动。 解决方案 0.20.2版本解决方式 1、查看名称节点上(即在配置文件中配置的hadoop.tmp.dir参数路径)/usr/hadoop0.20.2/hadoop-huanghz/dfs/name/current/文件夹中VERSION文件中的namespaceid； 2、在两个数据节点修改上dfs.data.dir配置的路径下current文件夹中的VERSION文件namespaceid与名称节点保持一致 2.x版本解决 /data/hadoop/dfs/name/current/VERSION 用name下面的clusterID，修改datanode的/data/hadoop/dfs/data/current/VERSION 里面的clusterID 每次格式化，name下面的VERSION的clusterID会产生一个新的ID，要去修改各个节点的VERSION的clusterID ……/dfs/data/current storageID=DS-1959445666-10.161.138.100-50010-1386493413986 clusterID=CID-64a3a726-29e4-4d80-86a6-035ef33a225b cTime=0 storageType=DATA_NODE layoutVersion=-47 ……/dfs/name/current/VERSION #Fri Jan 03 10:37:48 CST 2014 namespaceID=1667984727 clusterID=CID-42c6d540-c3ca-44df-95e8-01a6d87effb5 cTime=0 storageType=NAME_NODE blockpoolID=BP-220196921-10.161.138.100-1388716668863 layoutVersion=-47]]></content>
      <categories>
        <category>常见错误</category>
      </categories>
      <tags>
        <tag>常见错误</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：2、Hadoop伪分布式部署(HDFS)]]></title>
    <url>%2F2017%2F12%2F13%2Fhadoop-2-hadoop-pseudo-distributed-hdfs.html</url>
    <content type="text"><![CDATA[Hadoop部署的三种模式 单机模式（standalone） 一般不建议安装，网络上很少这方面资料 单机模式是Hadoop的默认模式。当首次解压Hadoop的源码包时，Hadoop无法了解硬件安装环境，便保守地选择了最小配置。 在这种默认模式下所有3个XML文件均为空。当配置文件为空时，Hadoop会完全运行在本地。 因为不需要与其他节点交互，单机模式就不使用HDFS，也不加载任何Hadoop的守护进程。 该模式主要用于开发调试MapReduce程序的应用逻辑。 伪分布模式（Pseudo-Distributed Mode） 伪分布模式在“单节点集群”上运行Hadoop，其中所有的守护进程都运行在同一台机器上。 该模式在单机模式之上增加了代码调试功能，允许你检查内存使用情况，HDFS输入输出，以及其他的守护进程交互。 比如namenode，datanode，secondarynamenode，jobtracker，tasktracker这5个进程，都能在集群上看到。 全分布模式（Fully Distributed Mode） Hadoop守护进程运行在一个集群上。 意思是说master上看到namenode,jobtracker，secondarynamenode可以安装在master节点，也可以单独安装。slave节点能看到datanode和tasktracker Hadoop伪分布模式部署环境要求java、ssh 添加hadoop用户 [root@hadoop-01 ~]# useradd hadoop [root@hadoop-01 ~]# vi /etc/sudoers # 找到root ALL=(ALL) ALL，添加 hadoop ALL=(ALL) NOPASSWD:ALL 上传并解压 [root@hadoop-01 software]# rz #上传hadoop-2.8.1.tar.gz [root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz 软连接 [root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop 设置环境变量 [root@hadoop-01 software]# vi /etc/profile export HADOOP_HOME=/opt/software/hadoop export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PROTOC_HOME/bin:$FINDBUGS_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH [root@hadoop-01 software]# source /etc/profile 设置用户用户组 [root@hadoop-01 software]# chown -R hadoop:hadoop hadoop [root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/* [root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# cd hadoop [root@hadoop-01 hadoop]# rm -f *.txt 切换用户hadoop [root@hadoop-01 software]# su - hadoop [root@hadoop-01 hadoop]# ll total 32 drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 bin drwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 etc drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 include drwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 lib drwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexec drwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logs drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 sbin drwxrwxr-x. 4 hadoop hadoop 4096 Jun 2 14:24 share # bin: 可执行文件 # etc: 配置文件 # sbin: shell脚本，启动关闭hdfs,yarn等 配置文件 [hadoop@hadoop-01 ~]# cd /opt/software/hadoop [hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt; # 配置自己机器的IP &lt;/property&gt; &lt;/configuration&gt; [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置hadoop用户的ssh信任关系 # 公钥/密钥 配置无密码登录 [hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa [hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys [hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys # 查看日期，看是否配置成功 [hadoop@hadoop-01 ~]# ssh hadoop-01 date The authenticity of host &#39;hadoop-01 (192.168.137.130)&#39; can&#39;t be established. RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a. Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yes Warning: Permanently added &#39;hadoop-01,192.168.137.130&#39; (RSA) to the list of known hosts. Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh hadoop-01 date #不需要回车输入yes,即OK Sun Aug 20 14:22:29 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost date The authenticity of host &#39;hadoop-01 (192.168.137.130)&#39; can&#39;t be established. RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a. Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yes Warning: Permanently added &#39;hadoop-01,192.168.137.130&#39; (RSA) to the list of known hosts. Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost date #不需要回车输入yes,即OK Sun Aug 20 14:22:29 CST 2017 格式化和启动 [hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format [hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh ERROR: hadoop-01: Error: JAVA_HOME is not set and could not be found. localhost: Error: JAVA_HOME is not set and could not be found. 解决方法:添加环境变量 [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hadoop-env.sh # 将export JAVA_HOME=${JAVA_HOME}改为 export JAVA_HOME=/usr/java/jdk1.8.0_45 [hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh ERROR: mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&#39;: Permission denied 解决方法: [hadoop@hadoop-01 hadoop]# exit [root@hadoop-01 hadoop]# cd ../ [root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# su - hadoop [root@hadoop-01 ~]# cd /opt/software/hadoop # 继续启动 [hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh 检查是否成功 [hadoop@hadoop-01 hadoop]# jps 19536 DataNode 19440 NameNode 19876 Jps 19740 SecondaryNameNode 访问： http://192.168.137.130:50070 修改dfs启动的进程，以hadoop-01启动 启动的三个进程： namenode: hadoop-01 bin/hdfs getconf -namenodes datanode: localhost datanodes (using default slaves file) etc/hadoop/slaves secondarynamenode: 0.0.0.0 [hadoop@hadoop-01 ~]# cd /opt/software/hadoop [hadoop@hadoop-01 hadoop]# echo &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves [hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves hadoop-01 [hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop-01:50091&lt;/value&gt; &lt;/property&gt; # 重启 [hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh [hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop：2、Sqoop使用]]></title>
    <url>%2F2017%2F12%2F12%2Fsqoop-2-sqoop-use.html</url>
    <content type="text"><![CDATA[sqoop介绍Apache Sqoop是专为ApacheHadoop和结构化数据存储如关系数据库之间的数据转换工具的有效工具。你可以使用Sqoop从外部结构化数据存储的数据导入到Hadoop分布式文件系统或相关系统如Hive和HBase。相反，Sqoop可以用来从Hadoop的数据提取和导出到外部结构化数据存储如关系数据库和企业数据仓库。Sqoop专为大数据批量传输设计，能够分割数据集并创建Hadoop任务来处理每个区块。sqoop的安装和下载可参考该地址 查看帮助命令sqoop help[root@hadoop lib]$ sqoop help usage: sqoop COMMAND [ARGS] Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version information See &#39;sqoop help COMMAND&#39; for information on a specific command. 这里提示我们使用sqoop help command（要查询的命令）进行该命令的详细查询 list-databases[root@hadoop001 lib]$ sqoop help list-databases] –connect jdbc:mysql://hostname:port/database指定mysql数据库主机名和端口号和数据库名(默认端口号为3306) –username : root 指定数据库用户名 –password :123456 指定数据库密码 [root@hadoop001 lib]$ sqoop list-databases \ &gt; --connect jdbc:mysql://localhost:3306 \ &gt; --username root \ &gt; --password 123456 information_schema basic01 mysql performance_schema sqoop test list-tables[root@hadoop001 lib]$ sqoop list-tables \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root \ &gt; --password 123456 stu emp 将mysql导入HDFS中(import)默认导入当前用户目录下/user/用户名/表名 说到这里扩展一个小知识点：hdfs dfs -ls和hdfs dfs -ls \的区别。 sqoop import --connect jdbc:mysql://localhost/database --username root --password 123456 --table example –m 1 –table : example mysql中即将导出的表 -m 1：指定启动一个map进程，如果表很大，可以启动多个map进程，默认是4个 这里可能会出现两个错误，如下： 第一个错误 18/01/14 16:01:19 ERROR tool.ImportTool: Error during import: No primary key could be found for table stu. Please specify one with --split-by or perform a sequential import with &#39;-m 1&#39; 提示可以看出，在我们从mysql中导出的表没有设定主键，提示我们使用把--split-by或者把参数-m设置为1，这里大家会不会问到，这是为什么呢？ 1、Sqoop通可以过–split-by指定切分的字段，–m设置mapper的数量。通过这两个参数分解生成m个where子句，进行分段查询。 2、split-by 根据不同的参数类型有不同的切分方法，如表共有100条数据其中id为int类型，并且我们指定–split-by id，我们不设置map数量使用默认的为四个，首先Sqoop会取获取切分字段的MIN()和MAX()即（–split-by），再根据map数量进行划分，这是字段值就会分为四个map：（1-25）（26-50）（51-75）（75-100）。 3、根据MIN和MAX不同的类型采用不同的切分方式支持有Date,Text,Float,Integer， Boolean,NText,BigDecimal等等。 4、所以，若导入的表中没有主键，将-m设置称1或者设置split-by，即只有一个map运行，缺点是不能并行map录入数据。（注意，当-m 设置的值大于1时，split-by必须设置字段） 。 5、split-by即便是int型，若不是连续有规律递增的话，各个map分配的数据是不均衡的，可能会有些map很忙，有些map几乎没有数据处理的情况。 第二个错误 Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/json/JSONObject at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:42) at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:742) at org.apache.sqoop.mapreduce.JobBase.putSqoopOptionsToConfiguration(JobBase.java:369) at org.apache.sqoop.mapreduce.JobBase.createJob(JobBase.java:355) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:249) at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692) at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:118) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) Caused by: java.lang.ClassNotFoundException: org.json.JSONObject at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 15 more 这里我们需要导入java-json.jar包，下载地址，把java-json.jar添加到../sqoop/lib目录 [root@hadoop001 lib]$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop --username root --password 123456 --table stu [root@hadoop001 lib]$ hdfs dfs -ls /user/hadoop/stu Found 4 items -rw-r--r-- 1 hadoop supergroup 0 2018-01-14 17:07 /user/hadoop/stu/_SUCCESS -rw-r--r-- 1 hadoop supergroup 11 2018-01-14 17:07 /user/hadoop/stu/part-m-00000 -rw-r--r-- 1 hadoop supergroup 7 2018-01-14 17:07 /user/hadoop/stu/part-m-00001 -rw-r--r-- 1 hadoop supergroup 9 2018-01-14 17:07 /user/hadoop/stu/part-m-00002 [root@hadoop001 lib]$ hdfs dfs -cat /user/hadoop/stu/&quot;part*&quot; 1,zhangsan 2,lisi 3,wangwu 加上参数m [root@hadoop001 lib]$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop --username root --password 123456 --table stu -m 1 这里大家可能也会出现一个错误，在hdfs上已经存，错误如下： 18/01/14 17:52:47 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://192.168.137.200:9000/user/hadoop/stu already exists at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146) at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:270) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325) at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:196) at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:169) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:266) at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692) at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:118) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) 删除目标目录后在导入,并且指定mapreduce的job的名字 参数：–delete-target-dir –mapreduce-job-name [root@hadoop001 lib]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --table stu \ &gt; -m 1 导入到指定目录 参数：–target-dir /directory [root@hadoop001 lib]$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop --username root -password 123456 --table stu -m 1 --target-dir /sqoop/ [root@hadoop001 lib]$ hdfs dfs -ls /sqoop Found 2 items -rw-r--r-- 1 hadoop supergroup 0 2018-01-14 18:07 /sqoop/_SUCCESS -rw-r--r-- 1 hadoop supergroup 27 2018-01-14 18:07 /sqoop/part-m-00000 [root@hadoop001 lib]$ hdfs dfs -cat /sqoop/part-m-00000 1,zhangsan 2,lisi 3,wangwu 指定字段之间的分隔符 参数：–fields-terminated-by [root@hadoop001 lib]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --table stu \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --fields-terminated-by &#39;\t&#39; \ &gt; -m 1 [root@hadoop001 lib]$ hdfs dfs -ls /user/hadoop/stu/ Found 2 items -rw-r--r-- 1 hadoop supergroup 0 2018-01-14 19:47 /user/hadoop/stu/_SUCCESS -rw-r--r-- 1 hadoop supergroup 27 2018-01-14 19:47 /user/hadoop/stu/part-m-0000 [root@hadoop001 lib]$ hdfs dfs -cat /user/hadoop/stu/part-m-00000 1 zhangsan 2 lisi 3 wangwu （字段之间变为空格） 如果表中的字段为null转化为0 参数：–null-non-string –null-string含义是 string类型的字段，当Value是NULL，替换成指定的字符 –null-non-string 含义是非string类型的字段，当Value是NULL，替换成指定字符先 导入薪资表 [root@hadoop001 lib]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --table sal \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --fields-terminated-by &#39;\t&#39; \ &gt; -m 1 [root@hadoop001 lib]$ hdfs dfs -cat /user/hadoop/sal/part-m-00000 zhangsan 1000 lisi 2000 wangwu null 加上参数`–null-string [root@hadoop001 lib]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --table sal \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --fields-terminated-by &#39;\t&#39; \ &gt; -m 1 \ &gt; --null-string 0 [root@hadoop001 lib]$ hdfs dfs -cat /user/hadoop/sal/part-m-00000 zhangsan 1000 lisi 2000 wangwu 0 导入表中的部分字段 参数：–columns [root@hadoop001 ~]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --table stu \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --fields-terminated-by &#39;\t&#39; \ &gt; -m 1 \ &gt; --null-string 0 \ &gt; --columns &quot;name&quot; [root@hadoop001 ~]$ hdfs dfs -cat /user/hadoop/stu/part-m-00000 zhangsan lisi wangwu 按条件导入数据 参数：–where [root@hadoop001 ~]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --table stu \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --fields-terminated-by &#39;\t&#39; \ &gt; -m 1 \ &gt; --null-string 0 \ &gt; --columns &quot;name&quot; \ &gt; --target-dir STU_COLUMN_WHERE \ &gt; --where &#39;id&lt;3&#39; zhangsan lisi 按照sql语句进行导入 参数：–query 使用–query关键字，就不能使用–table和–columns 自定义sql语句的where条件中必须包含字符串 $CONDITIONS，$CONDITIONS是一个变量，用于给多个map任务划分任务范 围； sqoop import \ --connect jdbc:mysql://localhost:3306/sqoop \ --username root --password 123456 \ --mapreduce-job-name FromMySQL2HDFS \ --delete-target-dir \ --fields-terminated-by &#39;\t&#39; \ -m 1 \ --null-string 0 \ --target-dir STU_COLUMN_QUERY \ --query &quot;select * from stu where id&gt;1 and \$CONDITIONS&quot; （或者quer使用这种格式：--query &#39;select * from emp where id&gt;1 and $CONDITIONS&#39;） 2 lisi 3 wangwu 在文件中执行创建文件sqoop-import-hdfs.txt [root@hadoop001 data]$ vi sqoop-import-hdfs.txt import --connect jdbc:mysql://localhost:3306/sqoop --username root --password 123456 --table stu --target-dir STU_option_file 执行 [root@hadoop001 data]$ sqoop --option-file /home/hadoop/data/sqoop-import-hdfs.txt [root@hadoop001 data]$ hdfs dfs -cat STU_option_file/&quot;part*&quot; 1,zhangsan 2,lisi 3,wangwu eval查看帮助命令对与该命令的解释为： Evaluate a SQL statement and display the results，也就是说执行一个SQL语句并查询出结果。 [root@hadoop001 data]$ sqoop eval \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --query &quot;select * from stu&quot; Warning: /opt/software/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /opt/software/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /opt/software/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /opt/software/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 18/01/14 21:35:25 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0 18/01/14 21:35:25 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/01/14 21:35:26 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. -------------------------------------- | id | name | -------------------------------------- | 1 | zhangsan | | 2 | lisi | | 3 | wangwu | -------------------------------------- HDFS数据导出到MySQL（Hive中的数据导入到MySQL）导出HDFS上的sal数据，查询数据： [root@hadoop001 data]$ hdfs dfs -cat sal/part-m-00000 zhangsan 1000 lisi 2000 wangwu 0 在执行导出语句前先创建sal_demo表（不创建表会报错） mysql&gt; create table sal_demo like sal; 导出语句 [root@hadoop001 data]$ sqoop export \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root \ &gt; --password 123456 \ &gt; --table sal_demo \ &gt; --input-fields-terminated-by &#39;\t&#39;\ &gt; --export-dir /user/hadoop/sal/ –table sal_demo ：指定导出表的名称； –input-fields-terminated-by：可以用来指定hdfs上文件的分隔符，默认是逗号（查询数据室可以看出我是用的是\t，所以这里指定为\t ，这里大家小心可能因为分隔符的原因报错） –export-dir ：导出数据的目录。 结果 mysql&gt; select * from sal_demo; +----------+--------+ | name | salary | +----------+--------+ | zhangsan | 1000 | | lisi | 2000 | | wangwu | 0 | +----------+--------+ 3 rows in set (0.00 sec) (如果再导入一次会追加在表中) 插入中文乱码问题 sqoop export --connect &quot;jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf-8&quot; --username root --password 123456 --table sal -m 1 --export-dir /user/hadoop/sal/ 指定导出的字段 --columns &lt;col,col,col...&gt; [root@hadoop001 data]$ sqoop export \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root \ &gt; --password 123456 \ &gt; --table sal_demo3 \ &gt; --input-fields-terminated-by &#39;\t&#39; \ &gt; --export-dir /user/hadoop/sal/ \ &gt; --columns name 查询结果 mysql&gt; select * from sal_demo3 -&gt; ; +----------+--------+ | name | salary | +----------+--------+ | zhangsan | NULL | | lisi | NULL | | wangwu | NULL | +----------+--------+ 3 rows in set (0.00 sec) MySQL的中的数据导入到Hive中[root@hadoop001 ~]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --table stu \ &gt; --create-hive-table \ &gt; --hive-database hive \ &gt; --hive-import \ &gt; --hive-overwrite \ &gt; --hive-table stu_import \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --fields-terminated-by &#39;\t&#39; \ &gt; -m 1 \ &gt; --null-non-string 0 –create-hive-table ：创建目标表，如果有会报错； –hive-database：指定hive数据库； –hive-import ：指定导入hive（没有这个条件导入到hdfs中）； –hive-overwrite ：覆盖； –hive-table stu_import :指定hive中表的名字，如果不指定使用导入的表的表名。 这里可能会报错，错误如下： 18/01/15 01:29:28 ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly. 18/01/15 01:29:28 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf at org.apache.sqoop.hive.HiveConfig.getHiveConf(HiveConfig.java:50) at org.apache.sqoop.hive.HiveImport.getHiveArgs(HiveImport.java:392) at org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:379) at org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:337) at org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:241) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:514) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at org.apache.sqoop.hive.HiveConfig.getHiveConf(HiveConfig.java:44) ... 12 more 网上找的资料基本都在说配置个人环境变量，并没有卵用，到hive目录的lib下拷贝几个jar包，问题就解决了！ [root@hadoop001 lib]$ cp hive-common-1.1.0-cdh5.7.0.jar /opt/software/sqoop/lib/ [root@hadoop001 lib]$ cd hive-shims* /opt/software/sqoop/lib/ 查看hive中导入的数据 hive&gt; show tables; OK stu_import Time taken: 0.067 seconds, Fetched: 1 row(s) hive&gt; select * from emp_import &gt; ; OK 1 zhangsan 2 lisiw 3 wangwu 导入Hive不建议大家使用–create-hive-table,建议事先创建好hive表 使用create创建表后，我们可以查看字段对应的类型，发现有些并不是我们想要的类型，所以我们要事先创建好表的结构再导入数据。 导入到hive指定分区 –hive-partition-key Sets the partition key to use when importing to hive–hive-partition-value Sets the partition value to use when importing to hive 示例 [root@hadoop001 lib]$ sqoop import \ &gt; --connect jdbc:mysql://localhost:3306/sqoop \ &gt; --username root --password 123456 \ &gt; --table stu \ &gt; --create-hive-table \ &gt; --hive-database hive \ &gt; --hive-import \ &gt; --hive-overwrite \ &gt; --hive-table stu_import1 \ &gt; --mapreduce-job-name FromMySQL2HDFS \ &gt; --delete-target-dir \ &gt; --fields-terminated-by &#39;\t&#39; \ &gt; -m 1 \ &gt; --null-non-string 0 \ &gt; --hive-partition-key dt \ &gt; --hive-partition-value &quot;2018-08-08&quot; hive上查询 hive&gt; select * from stu_import1; OK 1 zhangsan 2018-08-08 2 lisi 2018-08-08 3 wangwu 2018-08-08 Time taken: 0.121 seconds, Fetched: 3 row(s) sqoop job的使用就是把sqoop执行的语句变成一个job，并不是在创建语句的时候执行，你可以查看该job，可以任何时候执行该job，也可以删除job，这样就方便我们进行任务的调度 –create 创建一个新的job –delete 删除job –exec 执行job –show 显示job的参数 –list 列出所有的job 创建一个job sqoop job --create person_job1 -- import --connect jdbc:mysql://localhost:3306/sqoop \ --username root \ --password 123456 \ --table sal_demo3 \ -m 1 \ --delete-target-dir 查看可用的job [root@hadoop001 lib]$ sqoop job --list Available jobs: person_job1 执行person_job完成导入 [root@hadoop001 lib]$ sqoop job --exec person_job1 [root@hadoop001 lib]$ hdfs dfs -ls Found 6 items drwxr-xr-x - hadoop supergroup 0 2018-01-14 20:40 EMP_COLUMN_WHERE drwxr-xr-x - hadoop supergroup 0 2018-01-14 20:49 STU_COLUMN_QUERY drwxr-xr-x - hadoop supergroup 0 2018-01-14 20:45 STU_COLUMN_WHERE drwxr-xr-x - hadoop supergroup 0 2018-01-14 21:10 STU_option_file drwxr-xr-x - hadoop supergroup 0 2018-01-14 20:24 sal drwxr-xr-x - hadoop supergroup 0 2018-01-15 03:08 sal_demo3 问题：执行person_job的时候，需要输入数据库的密码，怎么样能不输入密码呢？ 配置sqoop-site.xml &lt;property&gt; &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, allow saved passwords in the metastore. &lt;/description&gt; &lt;/property&gt;]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据组件知识点第1节]]></title>
    <url>%2F2017%2F12%2F11%2Fbigdata-notes-1.html</url>
    <content type="text"><![CDATA[1、hadoop2.x版本有哪三个组件 HDFS Yarn MapReduce 2、分别是做什么的 HDFS:存储 Yarn:计算 MapReduce:资源调度和作业调度 3、jdk默认，我们部署在哪个路径下，假如那个路径不存在，我们要先创建什么 默认/usr/java，如果不存在，需要先创建 4、flume的三个组件 source:源端 channel:通道 memory|disk file sink:目标端 5、kafaka三个组件 producer :生产者 flume--&gt;kafka borker： 数据存储 comsumer：消费者 spark streaming/storm/flink 6、flume是不是起一个进程，就包含了这三个组件 是的 7、那么kafaka是不是也是一个进程包含三个进程呢？假如不是，包含哪个？ 不是，只包含borker组件 8、将文件上传至hadoop的根目录 hadoop fs -put filename / # &#39;/&#39;不是Linux的根目录，表示hadoop的根目录 9、查看hadoop里的文件 hadoop fs -ls / 10、查看hadoop里的文件的内容 hadoop fs -cat filename]]></content>
      <categories>
        <category>大数据组件知识点</category>
      </categories>
      <tags>
        <tag>大数据组件知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop：1、Sqoop安装部署]]></title>
    <url>%2F2017%2F12%2F11%2Fsqoop-1-sqoop-install.html</url>
    <content type="text"><![CDATA[Sqoop安装部署下载wget http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.7.0.tar.gz 解压重命名[root@hadoop001 software]# tar -xzvf sqoop-1.4.6-cdh5.7.0.tar.gz [root@hadoop001 software]# mv sqoop-1.4.6-cdh5.7.0/ sqoop mysqljdbc驱动包拷贝mysql的jdbc驱动包mysql-connector-java-5.1.45-bin.jar到sqoop/lib目录下 [root@hadoop001 sqoop]# cd lib/ [root@hadoop001 lib]# cp /opt/software/hive/lib/mysql-connector-java-5.1.45-bin.jar . 配置环境变量[root@ hadoop001 sqoop]$ vi /etc/profile export SQOOP_HOME=/opt/software/sqoop export PATH=$PATH:$SQOOP_HOME/bin sqoop-env.sh[root@hadoop001 conf]# mv sqoop-env-template.sh sqoop-env.sh #Set path to where bin/hadoop is available export HADOOP_COMMON_HOME=/opt/software/hadoop #Set path to where hadoop-*-core.jar is available export HADOOP_MAPRED_HOME=/opt/software/hadoop #set the path to where bin/hbase is available #export HBASE_HOME= #Set the path to where bin/hive is available export HIVE_HOME=/opt/software/hive 测试[root@hadoop001 conf]# sqoop version Warning: /opt/software/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /opt/software/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /opt/software/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. {&quot;time&quot;:&quot;2018-06-20 18:07:07,470&quot;,&quot;logtype&quot;:&quot;INFO&quot;,&quot;loginfo&quot;:&quot;org.apache.sqoop.Sqoop:Running Sqoop version: 1.4.6-cdh5.7.0&quot;} Sqoop 1.4.6-cdh5.7.0 git commit id Compiled by jenkins on Wed Mar 23 11:30:51 PDT 2016 警告是因为没有配置hbase，zookeeper，HCatalog]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume：1、Flume安装部署及简介]]></title>
    <url>%2F2017%2F12%2F11%2Fflume-1-flume-install.html</url>
    <content type="text"><![CDATA[Flume安装部署下载wget http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.7.0.tar.gz 解压重命名[root@hadoop001 software]# tar -xzvf flume-ng-1.6.0-cdh5.7.0.tar.gz [root@hadoop001 software]# mv apache-flume-1.6.0-cdh5.7.0-bin flume 配置环境变量[root@ hadoop001 sqoop]$ vi /etc/profile export FLUME_HOME=/opt/software/sqoop export PATH=$PATH:$FLUME_HOME/bin 修改配置文件cp flume-env.sh.template flume-env.sh export JAVA_HOME=/usr/java/jdk Flume 介绍Flume NG是一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。经过架构重构后，Flume NG更像是一个轻量的小工具，非常简单，容易适应各种方式日志收集，并支持failover和负载均衡。 使用场景flume-&gt;HDFS-&gt;batchflume-&gt;kafka-&gt;streaming 基本架构 Event的概念flume中event的相关概念：flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？—–event将传输的数据进行封装，是flume传输数据的基本单位，如果是文本文件，通常是一行记录，event也是事务的基本单位。event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers(头信息)信息。event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 flume三大核心组件flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent，agent本身是一个java进程，运行在日志收集节点—所谓日志收集节点就是服务器节点。 Source：负责从源端采集数据，输出到channel中，常用的Source有exec/Spooling Directory/Taildir Source/NetCat Channel：负责缓存Source端来的数据，常用的Channel有Memory/File Sink：处理Channel而来的数据写到目标端，常用的Sink有HDFS/Logger/Avro/Kafka Source Sink Channel Source+Channel+Sink=Agent,数据以event的形式从Source传送到Sink端，Flume就是写配置文件把我们的三大核心组件拼接起来，使用方便，可配置的、可插拔的、可组装的。 File Channel VS Memory ChannelFile Channel是一个持久化的隧道（channel），他持久化所有的事件，并将其存储到磁盘中。因此，即使Java 虚拟机当掉，或者操作系统崩溃或重启，再或者事件没有在管道中成功地传递到下一个代理（agent），这一切都不会造成数据丢失。Memory Channel是一个不稳定的隧道，其原因是由于它在内存中存储所有事件。如果java进程死掉，任何存储在内存的事件将会丢失。另外，内存的空间收到RAM大小的限制,而File Channel这方面是它的优势，只要磁盘空间足够，它就可以将所有事件数据存储到磁盘上。 Flume常用模式扇入 注意：多个sink节点写入一个source是为了减少同时写入hdfs上的压力；多个agent进行串联时，前一个agent的Sink和后一个agent的Source都要采用Avro的形式。 扇出]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL：5、MySQL知识点第2节]]></title>
    <url>%2F2017%2F12%2F10%2Fmysql-5-notes.html</url>
    <content type="text"><![CDATA[1、MySQL默认配置文件是在哪里？ /etc/my.cnf 2、配置文件和MySQL文件夹，我们是不是都要赋权限给用户和用户组 是的，都需要赋权限 3、解压后必然要做一件事情是什么 查看权限 4、允许所有IP可以访问，用什么表示 % 5、允许192.169.所有网段访问，那么该怎么表示 192.169.%.% 6、赋予权限的最后一个命令 flush privileges; 例如： create database 数据库名; grant all privileges on 数据库名.* to 某个用户名@&#39;%&#39; identified by &#39;密码&#39;; flush privileges; 7、插入一条语句的语法 insert into 表名 values(字段1，字段2……) 例如： 添加全部的数据： insert into user values(1,&#39;name&#39;,23); 添加指定某个字段的数据： insert into user(name,age) values(&#39;name&#39;,12); 8、更新一条数据的语法 update 表名 set 要修改的字段=修改的内容 where 字段=值 例如： 修改指定的数据： update user set name=&#39;wewe&#39; where id=1; 修改全部的数据： update user set name=&#39;wewe&#39;; 9、删除一条的语句 delete from 表名 where 字段=值 例如： 删除全部的数据： delete from user; 删除指定的某个数据： delete from user where id=1; 10、查询一张表的语句 select * from 表名 where 字段=值; 例如： 查询全部的数据： select * from user; 查询符合某个字段的信息： select * from user where id=1; 查询指定字段的数据： select name,age from user; 11、排序的语法 order by xxx desc/asc 例如： 如果不指定顺序，默认为升序： select * from user order by id; 降序： select * from user order by id desc; 升序： select * from user order by id asc; 12、降序是什么 降序是desc 例如： select * from user order by id desc; 13、聚合函数有哪些 求和： sum() 求数量：count() 求平均：avg() 例如： 求数量： select COUNT(category_id) from tb_content; 求和： select SUM(category_id) from tb_content; 求平均： select AVG(category_id) from tb_content; 14、聚合语法是什么，也就是分组语法 select 列1,列2……,sum(字段) from 表名 group by 列1,列2…… having sum(字段) &gt; 某个数量; 15、分组语法谨记一个点是什么 不是聚合函数的字段要出现在group by 后面 16、分组语法有个取多少行的语法是什么？比如工资和大于5000 limit 例如： select id from tb_content group by id having sum(money) &gt; 5000 LIMIT 2; 17、left join哪个表数据最全？谁去匹配？匹配不上，用什么表示 左边的表最全，右边的表去匹配，匹配不上用null表示]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop：1、Hadoop基础与编译]]></title>
    <url>%2F2017%2F12%2F10%2Fhadoop-1-hadoop-foundation-and-compilation.html</url>
    <content type="text"><![CDATA[Hadoop基础大数据概述可以用“5V + 1C”来概括： Variety (多样化） Volume (海量） Velocity (快速） Vitality (灵活） Value (价值性） Complexity (复杂） Hadoop与Hadoop生态圈 Hadoop 狭义: 软件(HDFS、MapReduce、Yarn) 广义: 以hadoop为主的生态圈 Hadoop1.x hdfs: 分布式文件管理系统 存储 mapreduce1: 执行引擎 计算+资源、作业调度 Hadoop2.x三大组件 hdfs: 分布式文件管理系统 存储 mapreduce2: 执行引擎 计算 yarn: 资源(memory cpu)和作业调度平台 资源 Hadoop编译[root@hadoop-01 ~]# cd /opt/ [root@hadoop-01 opt]# mkdir sourcecode software [root@hadoop-01 opt]# cd sourcecode [root@hadoop-01 sourcecode]# pwd /opt/sourcecode hadoop源代码下载 # 将hadoop-2.8.1-src.tar.gz下载（或者用rz上传）到sourcecode目录 [root@hadoop-01 sourcecode]# ll total 33756 -rw-r--r--. 1 root root 34523353 Aug 20 12:14 hadoop-2.8.1-src.tar.gz # 解压 [root@hadoop-01 sourcecode]# tar -xzvf hadoop-2.8.1-src.tar.gz [root@hadoop-01 sourcecode]# ll total 33760 drwxr-xr-x. 17 root root 4096 Jun 2 14:13 hadoop-2.8.1-src -rw-r--r--. 1 root root 34523353 Aug 20 12:14 hadoop-2.8.1-src.tar.gz [root@hadoop-01 sourcecode]# cd hadoop-2.8.1-src JAVA安装 [root@hadoop-01 ~]# mkdir -p /usr/java [root@hadoop-01 ~]# cd /usr/java [root@hadoop-01 java]# rz #上传jdk-8u45-linux-x64.gz [root@hadoop-01 java]# tar -xzvf jdk-8u45-linux-x64.gz # 修改用户和用户组 [root@hadoop-01 java]# chown -R root:root jdk1.8.0_45 # 设置环境变量 [root@hadoop-01 java]# vi /etc/profile # 在最底下加入 export JAVA_HOME=/usr/java/jdk1.8.0_45 export PATH=$JAVA_HOME/bin:$PATH # 生效 [root@hadoop-01 java]# source /etc/profile Maven安装 [root@hadoop-01 ~]# cd /opt/software/ [root@hadoop-01 software]# rz #上传apache-maven-3.3.9-bin.zip [root@hadoop-01 software]# ll total 8432 -rw-r--r--. 1 root root 8617253 Aug 20 12:35 apache-maven-3.3.9-bin.zip # 解压 [root@hadoop-01 software]# unzip apache-maven-3.3.9-bin.zip # 设置环境变量 [root@hadoop-01 software]# vi /etc/profile export MAVEN_HOME=/opt/software/apache-maven-3.3.9 export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot; export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH # 生效 [root@hadoop-01 software]# source /etc/profile # 查看 [root@hadoop-01 software]# mvn -version Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00) Maven home: /opt/software/apache-maven-3.3.9 Java version: 1.8.0_45, vendor: Oracle Corporation Java home: /usr/java/jdk1.8.0_45/jre Default locale: en_US, platform encoding: UTF-8 OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot; Findbugs安装 [root@hadoop-01 ~]# cd /opt/software/ [root@hadoop-01 software]# rz #上传findbugs-1.3.9.zip # 解压 [root@hadoop-01 software]# unzip findbugs-1.3.9.zip # 设置环境变量 [root@hadoop-01 software]# vi /etc/profile export FINDBUGS_HOME=/opt/software/findbugs-1.3.9 export PATH=$FINDBUGS_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH # 生效 [root@hadoop-01 software]# source /etc/profile # 查看 [root@hadoop-01 software]# findbugs -version 1.3.9 protobuf安装 [root@hadoop-01 ~]# cd /opt/software/ [root@hadoop-01 software]# rz #上传protobuf-2.5.0.tar.gz # 解压 [root@hadoop-01 software]# tar -xzvf protobuf-2.5.0.tar.gz [root@hadoop-01 software]# cd protobuf-2.5.0 [root@hadoop-01 protobuf-2.5.0]# yum install -y gcc gcc-c++ make cmake [root@hadoop-01 protobuf-2.5.0]# ./configure --prefix=/usr/local/protobuf [root@hadoop-01 protobuf-2.5.0]# make &amp;&amp; make install # 设置环境变量 [root@hadoop-01 java]# vi /etc/profile export PROTOC_HOME=/usr/local/protobuf export PATH=$PROTOC_HOME/bin:$FINDBUGS_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH # 生效 [root@hadoop-01 protobuf-2.5.0]# source /etc/profile # 查看 [root@hadoop-01 protobuf-2.5.0]# protoc --version libprotoc 2.5.0 其他依赖 yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake 编译 [root@hadoop-01 sourcecode]# cd hadoop-2.8.1-src [root@hadoop-01 hadoop-2.8.1-src]# mvn clean package -Pdist,native -DskipTests -Dtar 编译好的tar包路径 /opt/sourcecode/hadoop-2.8.1-src/hadoop-dist/target/hadoop-2.8.1.tar.gz 配置本地Maven仓库 window/linux: cd /opt/software/apache-maven-3.3.9/conf 1.vi setting.xml &lt;localRepository&gt;D:\software\apache-maven-3.3.9\repository&lt;/localRepository&gt; 2.创建D:\software\apache-maven-3.3.9\repository 提醒 1、有时候编译过程中会出现下载某个包的时间太久，这是由于连接网站的过程中会出现假死， 此时按ctrl+c，重新运行编译命令。 2、如果出现缺少了某个文件的情况，则要先清理maven(使用命令 mvn clean) 再重新编译。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL：4、MySQL知识点第1节]]></title>
    <url>%2F2017%2F12%2F09%2Fmysql-4-notes.html</url>
    <content type="text"><![CDATA[1、MySQL默认配置文件是在哪里？ /etc/my.cnf 2、赋予权限的最后一个命令 flush privileges 3、允许所有IP可以访问，用什么表示 % 4、插入一条语句的语法 insert into 表名 values(字段1，字段2……) 例如： 添加全部的数据： insert into user values(1,&#39;name&#39;,23); 添加指定某个字段的数据： insert into user(name,age) values(&#39;name&#39;,12); 5、更新一条数据的语法 update 表名 set 要修改的字段=修改的内容 where 字段=值 例如： 修改指定的数据： update user set name=&#39;wewe&#39; where id=1; 修改全部的数据： update user set name=&#39;wewe&#39;; 6、删除一条的语句 delete from 表名 where 字段=值 例如： 删除全部的数据： delete from user; 删除指定的某个数据： delete from user where id=1; 7、查询一张表的语句 select * from 表名 where 字段=值; 例如： 查询全部的数据： select * from user; 查询符合某个字段的信息： select * from user where id=1; 查询指定字段的数据： select name,age from user; 8、排序的语法 order by xxx desc/asc 例如： 如果不指定顺序，默认为升序： select * from user order by id; 降序： select * from user order by id desc; 升序： select * from user order by id asc; 9、降序是什么 降序是desc 例如： select * from user order by id desc; 10、聚合函数有哪些 求和： sum() 求数量：count() 求平均：avg() 例如： 求数量： select COUNT(category_id) from tb_content; 求和： select SUM(category_id) from tb_content; 求平均： select AVG(category_id) from tb_content; 11、聚合语法是什么，也就是分组语法 select 列1,列2……,sum(字段) from 表名 group by 列1,列2…… having sum(字段) &gt; 某个数量; 12、分组语法谨记一个点是什么 不是聚合函数的字段要出现在group by 后面 13、分组语法有个取多少行的语法是什么？比如工资和大于5000 limit 例如： select id from tb_content group by id having sum(money) &gt; 5000 LIMIT 2; 14、left join哪个表数据最全？谁去匹配？匹配不上，用什么表示 左边的表最全，右边的表去匹配，匹配不上用null表示]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第9节]]></title>
    <url>%2F2017%2F12%2F08%2Flinux-notes-9.html</url>
    <content type="text"><![CDATA[1、查看文件大小的两个命令 du -sh 或者 ll -h 2、文件夹的大小的命令 du -sh 3、隐藏文件什么标识开始 以.开头 4、使用什么参数查看 ll -a 或者 ls a 5、环境变量分为全局和个人，那么分别是什么文件 全局：/etc/profile 个人：.bash_profile 或者 .bashrc 6、生效环境变量文件的命令 source 文件 7、切换用户的命令，带有环境变量参数执行 su - 用户 8、想要临时有root权限是哪个命令 sudo 9、修改哪个文件 /etc/sudoers 10、添加用户和用户组的命令 添加用户： useradd 用户名 添加用户组： groupadd 用户组名 11、分别对应在哪个文件里有记录 cd /etc/passwd 12、一个用户可以有多个用户组吗？ 可以 13、crontab的查看和编辑的参数是什么 查看：crontab -l 编辑：crontab -e 14、后台运行一个命令，有哪些命令可以做 &amp; 、 nohup 、 screen 15、screen的创建和查看、进入，分别参数 创建：screen -S 查看：screen -list 进入：screen -r 16、返回上一次命令 cd - 17、ps和netstat什么作用 ps查看进程及pid netstat查看端口ip进程]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第8节]]></title>
    <url>%2F2017%2F12%2F08%2Flinux-notes-8.html</url>
    <content type="text"><![CDATA[1、怎样给一个普通用户开root权限 sudo 2、修改哪个文件 /etc/sudoers 3、切换用户，带环境变量的命令 su - 用户 4、讲讲带R的命令和r的命令 带R的命令：chown chmod 带r的命令：rm cp 5、级联创建文件夹 mkdir -p /xxx/xxx 6、rwx分别代表什么 r：可读 w：可写： x：可执行 7、软连接是相当于Windows的快捷方式，那么Linux的语法是什么 ln -s 实际路径 软连接路径(最好使用绝对路径) 8、什么叫绝对路径？什么叫相对路径？ 绝对路径：cd /xxx/xxx/aaa 相对路径：cd aaa在/xxx/xxx路径下 9、Linux系统之间传输文件夹、文件用什么命令 传输文件： scp xxx.log root@xxx.xxx.xxx.xxxIP地址:/xxx/xxx 传输文件夹： scp -r /xxx root@xxx.xxx.xxx.xxxIP地址:/xxx/xxx 10、一般解压一个压缩包，要注意什么 查看权限 11、机器卡了我们要看哪个进程占用内存和cpu较大，命令是什么 top 12、查看端口号命令 netstat -nlp | grep xxx]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第7节]]></title>
    <url>%2F2017%2F12%2F08%2Flinux-notes-7.html</url>
    <content type="text"><![CDATA[1、vi的三种模式 编辑模式、命令行模式、尾行模式 2、文件清空内容一般是哪些命令 dG 3、强制保存退出快捷键 wq! 4、说说所学到的关于文件的命令 创建文件：touch xxx 编辑：vi xxx 或者 echo &#39;xxx&#39; &gt; xxx 查看：tail -F 或者 cat 或者 more 删除：rm xxx 拷贝：cp xxx xxx 改名和移动： mv xxx xxx 解压：tar -zxvf xxx 5、-r是什么命令 递归 6、多人合作的后台命令 screen 7、多人合作的后台命令参数有哪些 -list：查看 screen -list -S：创建 screen -S xxx -r进入 screen -r xxx/id 8、后台命令有哪些 &amp; 或者 nohup 或者 screen 9、定时job的命令 crontab 10、五个星代表什么意思 * * * * * 分 时 日 月 周 11、每隔十分钟怎么写 */10 * * * * xxx 12、上传下载我们要事先执行什么命令安装什么包 执行：yum -y install lrzsz 需要安装lrzsz包 13、要卸载名字xxx的RPM，不知道全名 查询xxx：rpm -qa | grep xxx 卸载：rpm -e -nodeps xxx 14、查看进程，杀死进程命令 kill -9 $(pgrep -f xxx) 15、查看端口号命令 netstat -nlp | grep xxx 16、强制删除/xxx文件夹命令，强制和文件夹的参数 rm -rf /*** 17、打你通过web监控或者你在使用过程中发现此电脑太卡，那么哪些命令去查看系统负载还能看到哪些进程占的资源 top:能够看出进程是使用多少内存和CPU，也有该进程的pid 18、能够查看文件夹大小的命令 du -sh 或者 ll -h 19、查看文件的命令 ll 或者 ls -l 20、切换用户命令 su - xxx 21、临时获取root权限 sudo 22、获取root权限修改哪个文件 /etc/sudoers 23、同事告诉我那个电脑上装了xxx软件，但是没启动，我也不知道路径，该怎么办？假如xxx启动，我还能使用哪个命令去查看 未启动： find / -name &quot;*xxx*&quot; 或者 history | grep xxx 启动： ps -ef | grepxxx 24、-R参数的两个命令 chmod 或者 chown 25、最后一个知识点补充 比如当我们使用root用户去解压命令时，解压后的文件夹的所属用户和用户组会变化，而不是root。]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL：3、MySQL数据库设置远程访问权限]]></title>
    <url>%2F2017%2F12%2F08%2Fmysql-3-grant-basic.html</url>
    <content type="text"><![CDATA[支持远程访问如果你想连接你的mysql的时候发生这个错误： ERROR 1130: Host ‘192.168.1.111’ is not allowed to connect to this MySQL server 改表法可能是你的帐号不允许从远程登陆，只能在localhost。这个时候只要在localhost的那台电脑，登入mysql后，更改 “mysql” 数据库里的 “user” 表里的 “host” 项，从”localhost”改为”%” mysql -u root -p mysql&gt;use mysql; mysql&gt;update user set host = &#39;%&#39; where user = &#39;root&#39;; mysql&gt;select host, user from user; 授权法例如，你想root使用123456从任何主机连接到mysql服务器的话。 GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123546&#39; WITH GRANT OPTION; 如果你想允许用户root从ip为192.168.1.111的主机连接到mysql服务器，并使用123456作为密码 GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;192.168.1.111&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION; MySQL数据库设置远程访问权限方法小结很多情况下我们需要远程连接mysql数据库，那么就可以参考下面的权限设置 MySQL基础知识第一期，如何远程访问MySQL数据库设置权限方法总结，讨论访问单个数据库，全部数据库，指定用户访问，设置访问密码，指定访问主机。 设置访问单个数据库权限说明：设置用户名为root，密码为空，可访问数据库test mysql&gt;grant all privileges on test.* to &#39;root&#39;@&#39;%&#39;; 设置访问全部数据库权限说明：设置用户名为root，密码为空，可访问所有数据库 mysql&gt;grant all privileges on *.* to &#39;root&#39;@&#39;%&#39;; 设置指定用户名访问权限说明：设置指定用户名为user1，密码为空，可访问所有数据库 mysql&gt;grant all privileges on *.* to &#39;user1&#39;@&#39;%&#39;; 设置密码访问权限说明：设置指定用户名为user1，密码为user1，可访问所有数据库 mysql&gt;grant all privileges on *.* to &#39;user1&#39;@&#39;%&#39; IDENTIFIED BY &#39;user1&#39;; 设置指定可访问主机权限说明：设置指定用户名为user1，可访问所有数据库，只有10.2.1.11这台机器有权限访问 mysql&gt;grant all privileges on *.* to &#39;user1&#39;@&#39;10.2.1.11&#39;;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL：2、MySQL基础语法]]></title>
    <url>%2F2017%2F12%2F08%2Fmysql-2-mysql-basic-grammar.html</url>
    <content type="text"><![CDATA[Linux命令补充 Windows和Linux之间是rz sz scp： A机器（Linux）将文件或者文件夹传到B机器（Linux） scp xxx.log root@xxx.xxx.xxx.xxxIP地址:/xxx/xxx scp -r /xxx root@xxx.xxx.xxx.xxxIP地址:/xxx/xxx A机器用xxx用户发送： scp xxx.log IP地址:/xxx/xxx 等价于 scp xxx@IP地址:/xxxx/xxxx 软连接： 路径： 绝对路径：cd /xxx/xxx/xxx 相对路径：cd xxxx ln -s 实际路径 软连接路径(最好使用绝对路径) MySQL的基本概念 database db ：数据库 table ： 表 db1：t1, t2, t3 db2：t2, t3, t4 字段类型 http://www.runoob.com/mysql/mysql-data-types.html 整数型：int 小数型：float/double 字符：char 字符串：varchar 时间：timestamp 常规命令 使用某个数据库 use 数据库名; 查看数据库下面所有的表 show tables; 查看某个表的表结构 show create table 表名; 创建数据库 create database 数据库名; 创建表 create table 数据库名.表名(字段 类型,……) 例如： create table user( id int, name varchar(128), memory double, sex char(1), do varchar(100), cretime timestamp )CHARSET=utf8; 删除表 drop table 表名; 插入数据 insert into 数据库名.表名(列名) values(对应的值); 例如： insert into user(id,name,memory,sex,do,cretime) values(1,&#39;小米&#39;,10.22,&#39;b&#39;,&#39;在打游戏&#39;,&#39;2017-12-11 00:00:00&#39;); insert into user values(1,&#39;小米&#39;,10.22,&#39;b&#39;,&#39;在打游戏&#39;,&#39;2017-12-11 00:00:00&#39;); insert into user(id, name) values(1,&#39;小米&#39;); 查询 select 字段 from 数据库名.表名; 例如 select * from user; select * from user where id=3; *：查询所有的列 更新 update 数据库名.表名 set 字段名称=新的值 例如： update user set sex=&#39;g&#39; where id=1;修改id为1的数据 update user set sex=&#39;g&#39;;修改全部的行 删除 delete from 数据库名.表名 例如： delete from user;删除所有的数据，慎用 delete from user where id=3;删除id为3的数据 排序 order by xxx desc | asc 例如： select * from user order by cretime select * from user order by cretime desc; select * from user order by cretime asc; 只取多少行数据 limit n 例如： select * from user limit 2; 聚合语法 select 列1,列2……,sum(memory) from user group by 列1,列2…… having sum(memory) &gt; 3000 聚合函数 count() : 求数量 sum() : 求和 avg() ： 求平均 字段别名 as xxx 等价SQL：使用子查询语法 select * from(select dept, sum(sal) as sum_sal from salary group by dept) t where t.sum_sql &gt; 5000; 两张表关联： 左连接： A left join B on A.字段=B.字段 工作中用的最多 A表数据最全 &lt;-- B表补全 右连接： A right join B on A.字段=B.字段 A表补全 --&gt; B表数据最全 内连接： A inner join B on A.字段=B.字段 慎用 注意点： 只要满足on条件，有几行算几行 例如： select a.* b.deptno,b.dname from emp a left join dept b on a.deptno=b.deptno; select a.* b.deptno,b.dname from emp a right join dept b on a.deptno=b.deptno; 创建db，user create database 数据库名; grant all privileges on 数据库名.* to 某个用户名@&#39;%&#39; identified by &#39;密码&#39;; flush privileges; 注意点 只要涉及权限修改，必须执行flush privileges; % 允许所有的IP都可以访问（权限危险） 192.168.%.% 创建用户并授权，同时限制只能在某个IP或者IP段上的机器才能访问 谨记： flush privileges; 或者重启MySQL服务 补充点 1.登录 mysql -uroot -p123456 -h127.0.0.1 2.dbeaver(企业使用的软件) mysqladmin环境变量 vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs export MYSQL_BASE=/usr/local/mysql export PATH=${MYSQL_BASE}/bin:$PATH PS1=`uname -n`&quot;:&quot;&#39;$USER&#39;&quot;:&quot;&#39;$PWD&#39;&quot;:&gt;&quot;; export PS1]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第6节]]></title>
    <url>%2F2017%2F12%2F07%2Flinux-notes-6.html</url>
    <content type="text"><![CDATA[1、上传下载的命令 上传：rz 从Windows上传到Linux 下载：sz 从Linux下载到Windows 2、实现要安装一个包，包名叫什么 包名：lrzsz 安装：yum -y install lrzsz 3、杀死关于xxx的所有进程 kill -9 $(pgrep -f xxx) 4、讲了三个后台执行的命令，哪三个 &amp; 和 nohup 和 screen 5、定时启动job的命令是什么 crontab 6、查看和编辑是什么参数 查看：crontab -l 参数：-l 编辑：crontab -e 参数：-e 7、五个*分别代表什么 * * * * * 分 时 日 月 周 8、想要注释掉一个定时的job，加什么符号 在job任务前面加# 例如： # * * * * * echo &#39;date&#39; &gt;&gt; date.log 9、写一个每隔十分钟的表达式 */10 * * * * xxxjob任务 10、shell中=前后不能有什么 shell中=前后不能有空格 11、调试模式，加什么参数 调试模式加-x参数 例如： #!/bin/bash -x 12、创建完shell脚本，那么执行是报权限问题，该怎么办 修改权限： chmod +x xxx.sh 或者 chmod 744 xxx.sh 13、多人合作的一个后台会话命令是什么 screen -S xxx 14、创建的参数 创建的参数时-S 例如： screen -S xxx 15、进入的参数 进入的参数时-r 例如： screen -r xxx/id 16、退出会话的快捷键’ 退出快捷键： ctrl + a + d 17、查看该用户下有哪些会话，什么参数 查看参数：-list 例如： screen -list 18、vi三种模式 编辑模式、命令行模式、尾行模式 19、快捷键有哪些？ 删除当行：dd 删除多行：dG 回到第一行：gg 退出：Ctrl + z 退出编辑模式：冒号 回到行首：$ 20、强制保存退出 wq! 21、rw–wx—的数字是多少 630 22、部署MySQL数据库时，一般默认配置文件在哪？ /usr/local 23、给MySQL创建一个用户和用户组 创建用户组： groupadd -g 101 dba 创建用户： useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin 24、用创建的用户和用户组来管理MySQL，那么必须对那个配置和/usr/local/mysql文件夹赋予权限和用户以及用户组，对不对？ 这句话是对的 25、查看一个xxx名字的RPM包存在不存在，什么命令 rpm -qa | grep xxx 26、假如存在，怎么不验证依赖关系去卸载 rpm -e -nodeps xxx 27、MySQL安装过程中，我们用root设置了MySQL开机自启动，那么请问部署完，我是不是可以使用service mysql status这样的命令？ 可以 28、假如我不知道这个服务是哪个用户管理，怎么才能发现这个用户呢? 用root或者sudo权限的用户，去这个服务的目录，使用ls -l查看，不就可以看该mysql文件夹的所属用户了 29、场景：你和别人工作交接，他走了没告诉你很详细的内容，然后xxx服务挂了，需要去启动，你只知道xxx服务的名称，请问怎么知道文件夹路径？领导让你打开那台电脑的网页，该怎么办？ find / -name &quot;*xxx*&quot; netstat -nlp | grep xxx]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第5节]]></title>
    <url>%2F2017%2F12%2F07%2Flinux-notes-5.html</url>
    <content type="text"><![CDATA[1、强制保存退出命令 q! 2、无论光标在哪，跳到第一行的行首的快捷键 gg 3、删除当前行和以下行 dG 4、创建文件的哪几种命令 vi xxx touch xxx echo &quot;xxx&quot; &gt; xxx 5、创建文件夹的命令和级联参数 mkdir -p xxx/xxx 6、给文件重命名的命令 mv xxx xxx 7、两个大R参数的命令是什么 chmod 和 chown 8、rwx数字分别是多少 r：4 w：2 x：1 9、rwxrwx—数字是多少 770 10、三组分别是什么意思 第一组：当前用户 第二组：当前用户组的所有成员 第三组：其他用户 11、给普通用户赋予sudo权限，在什么文件加什么内容 /etc/sudores xxx ALL=(root) NOPASSWD:ALL 12、切换用户和执行环境变量的参数 su - root 13、全局环境变量文件在哪？个人的在哪？ 全局变量： /etc/profile 个人： .bash_profile 或者 .bashrc 14、查看文件内容的命令有哪些？ tail -F xxx cat xxx more xxx 15、实时查看文件内容的命令和参数 tail -F xxx 16、生效环境变量文件有哪几种命令 source /etc/profile . .bash_profile 17、tar.gz解压，压缩命令 解压： tar -zxvf xxx.tar.gz 压缩： tar -czf xxx.tar.gz xxx 18、查看文件大小的是哪两种命令 du -sh xxx ll -h xxx 19、查看文件夹的大小的一个命令是什么 du -sh xxx 20、隐藏文件的标识是什么，加什么参数查看 以.开始 查看：ll -a 21、切换到上一次目录的命令 cd - 22、上一层命令 cd ../ 23、切到自己家目录的命令 cd ~ 或者 cd 24、查看IP的命令 ifconfig 25、内存的命令 free -m/-g 26、磁盘的命令 df -h 27、系统负载情况的命令 top 28、查看xxx进程是否存在 ps -ef | grep xxx 29、杀死xxx进程 kill -9 xxx 或者 kill -9 $(pgrep -f xxx) 30、查看端口号命令 netstat -nlp | grep xxx 31、RPM安装命令，查看本机是否已经安装xxx的RPM，怎样卸载xxx的RPM包 RPM安装： yum install xxx 或者 yum -y install xxx 查看是否安装RPM： rpm -qa | grep xxx 卸载RPM： rpm -e xxx 或者 rpm --nodeps -e xxx 32、卸载时不校验RPM软件包的依赖性，参数是什么 --nodeps参数 33、别名的命令是什么 alias xxx=&#39;cd xxx&#39; 34、设置别名也有临时的和永久的设置两种方式 临时： alias xxx=&#39;cd xxx&#39; 设置jh别名 永久： alias xxx=&#39;cd xxx&#39; 配置到环境变量文件，执行生效命令 35、查看命令帮助的常用的两种命令 man xxx xxx --help 36、强制删除文件夹/xx rm -rf /xxx]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第4节]]></title>
    <url>%2F2017%2F12%2F07%2Flinux-notes-4.html</url>
    <content type="text"><![CDATA[1、出现R参数的是哪两个命令？ chmod 和 chown 2、出现r参数的命令 rm -rf xxx cp -r xxx xxx 3、rwx数字分别代表多少 r：4 w：2 x：1 4、rw-rw-r–数字多少 664 5、第一组第二组第三组，什么意思？简述。 第一组：当前用户对这个文件拥有什么权限 第二组：当前用户组的所有用户对这个文件拥有什么权限 第三组：其他用户组的所有用户对这个文件拥有什么权限 6、创建文件哪几种命令 vi xxx touch xxx echo &quot;xxx&quot; &gt; xxx 7、创建文件夹的命令和级联创建参数 mkdir -p xxx/xxxx 8、移动的命令 mv xxx xxx 9、复制的命令 cp xxxx xxxx 10、复制一个文件夹，加什么参数 -r参数 11、切换到一个文件夹的命令 cd xxx 12、上一次和上一层的命令 上一层：cd ../ 上一次：cd - 13、家目录在哪？root用户和普通用户 root用户： /root 普通用户： /home/用户名 14、添加一个用户的命令 useradd 用户名 15、修改哪个文件时赋予sudo权限的 vi /etc/sudoers xxx ALL=(root) NOPASSWD:ALL 16、切换用户的命令 su 用户名 17、带环境变量的执行的参数是什么 -参数 例如：su -root 18、vi三种模式的快捷键 进入编辑模式：i 进入命令行模式：esc 进入尾行模式：Shift + 冒号 19、跳到第一行行首 gg 20、最后一行的行首 G 21、跳到某行的行尾 $ 22、删除当前行 dd 23、删除当前行及以下行 dG 24、保存退出和强制退出 保存退出：wq 强制退出：q! 25、查看当前目录是什么 pwd 26、ls -l等价是什么 ll 27、自动补齐命令的快捷键 tab 28、实时查看文件内容的命令和参数 tail -F 29、全局环境变量文件在哪，怎么生效 /etc/profile source /etc/profile 30、个人环境变量文件是什么，怎么生效 .bash_profile 或者 .bashrc . .bash_profile 或者 . .bashrc 31、隐藏文件什么标识，怎么查看 以.开头 ll -a .xxx 32、怎样查看进程是否存在 ps -ef | grep xxx 33、怎么杀死该进程 kill -9 xxx 34、某个服务xxx的端口号怎么查看 netstat -nlp | grep xxx 35、安装RPM包的命令 yum install xxx 或者 yum -y install xxx 36、查看当前系统是否安装xxx名字的RPM rpm -qa | grep xxx 37、卸载命令和不验证依赖关系的参数 rpm --nodeps -e xxx 38、tar.gz解压压缩命令 解压： tar -zxvf xxx.tar.gz 压缩： tar -czf xxx.tar.gz xxx 39、一般命令帮助是哪两种方式 xxx --help man xxx 40、查看文件大小的命令 ll -h xxx du -sh xxx 41、启动ntpd服务、查看ntpd服务、停止ntpd服务 启动ntpd服务：service ntpd start 查看ntpd服务：service ntpd status 停止ntpd服务：service ntpd stop 42、慎用的删除命令 rm -rf /* 千万不可以使用！！！！]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第3节]]></title>
    <url>%2F2017%2F12%2F07%2Flinux-notes-3.html</url>
    <content type="text"><![CDATA[1、切换用户的命令？ su 用户 2、-表示什么意思？ 执行切换到的用户的环境变量，进入家目录 3、想要临时获取root权限（最大系统权限）的命令是什么？ sudo 4、在哪个文件去修改，让一个普通用户变为sudo的用户？ vi /etc/sudoers 用户名 ALL=(root) NOPASSWD:ALL 5、全局环境变量文件在哪？ /etc/proflie 6、个人环境变量文件在哪？ ~/.bash_profile 或者 ~/.bashrc 7、生效全局的命令，一般是什么？ source xxx 8、生效个人的是什么？ . xxx 9、实时查看一个文件内容的命令和参数？ tail -F xxx** 10、vi的三种模式，是哪三种？ 编辑模式、命令行模式、尾行模式 11、命令模式进行编辑模式，按什么键？从编辑模式退出命令是什么键？从命令模式进入尾行模式是什么键？ 从命令模式进入编辑模式：i 从编辑模式退出命令模式：esc 从命令模式进入尾行模式：Shift + 冒号 12、跳到第一行第一个字母的快捷键？ gg 13、跳到最后一行第一个字母？ G 14、跳到一行的最后一个字母？ $ 15、删除当前行？ dd 16、删除当前行及以下所有行？ dG 17、保存退出 wq 18、强制退出 q! 19、添加用户，用户组的命令？删除用户、用户组的命令？ 添加用户: useradd 用户名 添加用户组：groupadd 用户组 删除用户：userdel 用户名 删除用户组：groupdel 用户组 20、将一个用户添加到另外一个用户组的命令 usermod -a -G 用户组 用户 21、权限 读写执英文简写？数字分别是多少？ 读：r 数字为：4 写：w 数字为：2 执行：x 数字为：1 22、rwx—r–数字是多少？ 704 23、第一组、第二组、第三组分布什么意思？简单描述？ 第一组：三位rwx ，root这个用户对这个文件有读写执行权限 第二组：三位rw- ，root这个组的所有用户对这个文件只有读写权限 第三组：三位r-- ，其他用户组的所有用户对这个文件只有只读权限 24、在Linux命令学习中，带参数是-R的是哪两个命令？ chmod 和 chown 25、请列举带-r的命令 rm -rf /xxx cp -r xxx xxx 26、修改一个文件或者文件夹的用户组和用户的命令是什么？ chown -R xxx:xxx /xxx chown xxx:xxx /xxx 27、创建文件有哪几种方式？ vi xxx touch xxx echo &quot;xxx&quot; &gt; xxx 28、级联创建文件夹 mkdir -p xxx/xxx 29、一个命令的结果作为另外一个命令的输入，是什么参数？ 管道符： | 30、老板让你们打开电脑的xxx服务的web，请问，IP和端口号怎么获取？ netstat -nlp | grep xxx 31、查看磁盘 df -h 32、查看内存 free -m/-g m : 表示多少兆 g : 表示多少g 33、安装RPM软件的命令？ yum install xxx 或者 yum -y install xxx 34、查看RPM安装有哪些 rpm --qa | grep xxx 35、卸载RPM rpm -e xxx 或者 rpm --nodeps -e xxx 36、不验证依赖关系的参数 --nodeps 37、隐藏文件什么标识 以.开头 38、怎么查看隐藏文件 ll -a 39、ll等价什么 ls -l 40、别名怎么设置 临时： alias xxx=&#39;cd xxx&#39; 永久： alias xxx=&#39;cd xxx&#39; 配置到环境变量文件，执行生效命令 41、切到上一层目录 cd ../ 42、切到上一次目录 cd - 43、查看历史命令 history 44、执行历史命令中的一条 !第多少行数字 例如：!142 45、搜索的命令 which xxx locate xxx find /xxx -name &quot;*xxx*&quot; 46、在/var目录搜索出现过的log字母的文件或者文件夹 find /var -name &quot;*.log&quot; 47、解压tar.gz的命令 tar -zxvf xxx.tar.gz 48、压缩tar.gz tar -czf xxx.tar.gz xxx 49、自动补齐shell命令，该怎么做？ tab键 50、在当前目录搜索？ . 当前目录]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第2节]]></title>
    <url>%2F2017%2F12%2F07%2Flinux-notes-2.html</url>
    <content type="text"><![CDATA[1、vi命令的三种模式，使用什么命令切换？ i进入编辑模式 --&gt; esc退出编辑模式，进入命令模式 --&gt; shift + 冒号进入尾行模式 2、vi命令，假如一个记事本，按什么键跳到最后一行行首？什么快捷键跳到第一行行首？按什么快捷键删除当前行？按什么快捷键删除当前行及以下行？ 跳到最后一行行首：命令模式下冒号 + $ 例如： :$ 跳到第一行行首：命令模式下冒号 + 0 或者 1 例如： :0 或者 :1 删除当前行：命令模式下 dd 例如：dd 跳到当前行及以下行：命令模式下 n（是一个数字）dd==** 例如：删除包含当前行以下的6行 6dd 3、保存退出，强制退出，命令？ 保存退出：wq 强制退出：q! 4、隐藏文件以什么为开头？ 隐藏文件以.开头 例如：.ssh 5、怎么查看隐藏文件？ ls -a 6、创建文件夹，想要级联创建，加什么参数？ 加-p参数 例如：mkdir -p /data/aaa 7、创建一个空文件？ touch xxx 例如：touch aaa.txt 8、vi可不可以创建文件？ 可以创建文件 例如： vi aaa.txt 9、root家目录？ /root 例如：cd /root 10、其他用户的家目录在哪？ /home/xxx 例如：cd /home/admin 11、怎么切换到自己的家目录，哪两个命令？ cd 或者 cd ~ 12、切换到上一层目录？ cd ../ 13、切换到上一次目录？ cd - 14、哪两个命令可以帮助？ man xxx 或者 xxx --help 例如：man ls 或者 ls --help 15、ll等价于什么命令？ ls -l 16、看一个文件夹里的文件大小，ll后面加什么参数？ 加-h参数 例如：ll -h 17、怎么修改hostname（临时和永久）？修改完临时的，是不是要把当前终端退出，重新进？ 永久修改： vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=admin(hostname名字) 修改完之后重启 临时修改 hostname admin 修改之后重新进入 需要退出当前终端，重新进入]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux知识点第1节]]></title>
    <url>%2F2017%2F12%2F07%2Flinux-notes-1.html</url>
    <content type="text"><![CDATA[1、鼠标在Windows桌面怎么进入CentOS？ 单击左键 2、怎么从CentOS桌面退出到Windows桌面？ ctrl + alt 3、我们常用的Linux系统是哪两个？ Ubuntu 和 CentOS 4、查看当前目录的命令？ pwd 5、进入目录的命令是什么？ cd xxx 例如：cd /home 6、root的家目录在哪里？ /root 7、怎样进入当前用户的家目录，两种方法？ cd 或者 cd ~ 8、波浪线是代表什么？ 家目录 9、回到上一层目录？ cd ../ 10、怎么回退到上一次目录？ cd - 11、查看当前目录有哪些文件和文件夹？ ls 12、查看文件和文件夹详细信息的两个命令？ ls -l 或者 ll 13、隐藏文件，以什么为开始？ 以.开头 例如：.ssh 14、加什么参数能够查看隐藏文件？ 加-a参数 例如：ls -la 或者 ls -l -a 或者 ll -a 15、查看文件的大小的参数？ 加-h参数 例如：ls -lh 或者 ll -h 16、文件夹或者文件按照时间排序的命令？ ll -rt 或者 ls -lrt 17、怎么查看命令帮助？ =man xxx 或者 xxx --help 例如：man ls 或者 ls --help 18、创建文件夹的命令？ mkdir 例如：mkdir data 19、级联创建的参数？ 加-p参数 例如：mkdir -p /data/aaa 20、创建文件的一个命令是什么？ touch xxx.xxx 例如：touch aaa.txt 21、查看文件内容的常用的两个命令？ cat xxx 或者 more xxx 例如：cat aaa.txt 或者 more aaa.txt 22、怎么编辑一个文件，先按什么键进入编辑模式？再按什么键退出编辑模式，进入命令模式？然后按什么键进入尾行模式？ vi xxx --&gt; i键 --&gt; esc键 --&gt; shift + 冒号键 例如：vi aaa.txt --&gt; i进入编辑模式 --&gt; esc退出编辑模式 --&gt; shift + 冒号进入尾行模式 23、强制退出的命令？ q! 例如：vi aaa.txt --&gt; i进入编辑模式 --&gt; esc退出编辑模式 --&gt; shift + 冒号进入尾行模式 --&gt; q!编辑错误，强制退出]]></content>
      <categories>
        <category>Linux知识点</category>
      </categories>
      <tags>
        <tag>Linux知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL：1、MySQL二进制部署]]></title>
    <url>%2F2017%2F12%2F07%2Fmysql-1-mysql-binary-deployment.html</url>
    <content type="text"><![CDATA[下载mysql并检查MD5 [root@hadoop-01 ~]# cd /usr/local [root@hadoop-01 local]# wget https://downloads.mariadb.com/archives/mysql-5.6/mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz [root@hadoop-01 local]# wget https://downloads.mariadb.com/archives/mysql-5.6/mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz.md5 [root@hadoop-01 local]# vi mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz.md5 61affe944eff55fcf51b31e67f25dc10 mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz [root@hadoop-01 local]# md5sum mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz 61affe944eff55fcf51b31e67f25dc10 mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz 检查是否已安装 [root@hadoop-01 local]# ps -ef|grep mysqld 解压重命名 [root@hadoop-01 local]# tar xzvf mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz [root@hadoop-01 local]# mv mysql-5.6.23-linux-glibc2.5-x86_64 mysql 创建用户组 [root@hadoop-01 local]# groupadd -g 101 dba [root@hadoop-01 local]# useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin [root@hadoop-01 local]# id mysqladmin uid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root) [root@hadoop-01 local]# passwd mysqladmin # copy 环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量 [root@hadoop-01 local]# cp /etc/skel/.* /usr/local/mysql 创建配置文件 /etc/my.cnf(640) [root@hadoop-01 mysql]# vi /etc/my.cnf 删除内容，并将以下内容复制进去 [client] port = 3306 socket = /usr/local/mysql/data/mysql.sock [mysqld] port = 3306 socket = /usr/local/mysql/data/mysql.sock skip-external-locking key_buffer_size = 256M sort_buffer_size = 2M read_buffer_size = 2M read_rnd_buffer_size = 4M query_cache_size= 32M max_allowed_packet = 16M myisam_sort_buffer_size=128M tmp_table_size=32M table_open_cache = 512 thread_cache_size = 8 wait_timeout = 86400 interactive_timeout = 86400 max_connections = 600 # Try number of CPU&#39;s*2 for thread_concurrency thread_concurrency = 32 #isolation level and default engine default-storage-engine = INNODB transaction-isolation = READ-COMMITTED server-id = 1 basedir = /usr/local/mysql datadir = /usr/local/mysql/data pid-file = /usr/local/mysql/data/hostname.pid #open performance schema log-warnings sysdate-is-now binlog_format = MIXED log_bin_trust_function_creators=1 log-error = /usr/local/mysql/data/hostname.err log-bin=/usr/local/mysql/arch/mysql-bin #other logs #general_log =1 #general_log_file = /usr/local/mysql/data/general_log.err #slow_query_log=1 #slow_query_log_file=/usr/local/mysql/data/slow_log.err #for replication slave #log-slave-updates #sync_binlog = 1 #for innodb options innodb_data_home_dir = /usr/local/mysql/data/ innodb_data_file_path = ibdata1:500M:autoextend innodb_log_group_home_dir = /usr/local/mysql/arch innodb_log_files_in_group = 2 innodb_log_file_size = 200M # rember change innodb_buffer_pool_size = 2048M innodb_additional_mem_pool_size = 50M innodb_log_buffer_size = 16M innodb_lock_wait_timeout = 100 #innodb_thread_concurrency = 0 innodb_flush_log_at_trx_commit = 1 innodb_locks_unsafe_for_binlog=1 #innodb io features: add for mysql5.5.8 performance_schema innodb_read_io_threads=4 innodb-write-io-threads=4 innodb-io-capacity=200 #purge threads change default(0) to 1 for purge innodb_purge_threads=1 innodb_use_native_aio=on #case-sensitive file names and separate tablespace innodb_file_per_table = 1 lower_case_table_names=1 [mysqldump] quick max_allowed_packet = 16M [mysql] no-auto-rehash [mysqlhotcopy] interactive-timeout [myisamchk] key_buffer_size = 256M sort_buffer_size = 256M read_buffer = 2M write_buffer = 2M 指定文件用户组、修改权限、尝试首次安装 [root@hadoop-01 local]# chown mysqladmin:dba /etc/my.cnf [root@hadoop-01 local]# chmod 640 /etc/my.cnf [root@hadoop-01 etc]# ll my.cnf -rw-r----- 1 mysqladmin dba 2201 Aug 25 23:09 my.cnf [root@hadoop-01 local]# chown -R mysqladmin:dba /usr/local/mysql [root@hadoop-01 local]# chmod -R 755 /usr/local/mysql [root@hadoop-01 local]# su - mysqladmin [mysqladmin@hadoop-01 ~]# pwd /usr/local/mysql [mysqladmin@hadoop-01 ~]# mkdir arch [mysqladmin@hadoop-01 ~]# scripts/mysql_install_db ###import Installing MySQL system tables..../bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory #缺少libaio.so 包 [root@hadoop-01 local]# yum -y install libaio 再次安装 [mysqladmin@hadoop-01 ~]# scripts/mysql_install_db --user=mysqladmin --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data 配置mysql服务自启动 [root@hadoop-01 ~]# cd /usr/local/mysql #将服务文件拷贝到init.d下，并重命名为mysql [root@hadoop-01 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql #赋予可执行权限 [root@hadoop-01 mysql]# chmod +x /etc/rc.d/init.d/mysql #删除服务 [root@hadoop-01 mysql]# chkconfig --del mysql #添加服务 [root@hadoop-01 mysql]# chkconfig --add mysql [root@hadoop-01 mysql]# chkconfig --level 345 mysql on 查看进程 [root@hadoop-01 mysql]# su - mysqladmin [mysqladmin@hadoop-01 ~]# pwd /usr/local/mysql [mysqladmin@hadoop-01 ~]# rm -rf my.cnf [mysqladmin@hadoop-01 ~]# bin/mysqld_safe &amp; [mysqladmin@hadoop-01 ~]# ps -ef|grep mysqld [mysqladmin@hadoop-01 ~]# netstat -tulnp | grep mysql [root@hadoop-01 local]# service mysql status 登录mysql [mysqladmin@hadoop-01 ~]# bin/mysql mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | test | +--------------------+ 修改密码 mysql&gt; use mysql; mysql&gt; update user set password=password(&#39;password&#39;) where user=&#39;root&#39;; 配置环境变量 [mysqladmin@hadoop-01 ~]$ vi ./.bash_profile # insert export MYSQL_HOME=/usr/local/mysql export PATH=$MYSQL_HOME/bin:$PATH [mysqladmin@hadoop-01 ~]$ source ./.bash_profile 重新初始化安装 [mysqladmin@rzdatahadoop001 ~]$ rm -rf data [mysqladmin@rzdatahadoop001 ~]$ rm -rf arch/* [mysqladmin@rzdatahadoop001 ~]$ scripts/mysql_install_db --user=mysqladmin --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux：4、Linux常用命令进阶]]></title>
    <url>%2F2017%2F12%2F06%2Flinux-4-linux-command.html</url>
    <content type="text"><![CDATA[Windows和Linux交互 下载安装包： yum install -y lrzsz 上传 rz 下载 sz xxxx文件名 上传和下载时，怎样指定默认的路径？ 在CRT里面点击会话窗口--&gt; 然后根据连接点击右键属性--&gt; X/Y/Zmodem里面的上传和下载 crontab作业调度 查看当前用户下哪些作业 crontab -l 添加、编辑 crontab -e 每分钟执行一次日期添加 * * * * * echo &#39;date&#39; &gt;&gt; date.log 格式： * * * * * command 分 时 日 月 周 命令 第1列：表示分钟1~59 每分钟用*或者*/1表示 第2列：表示小时1~23（0表示0点） 第3列：表示日期1~31 第4列表示月份1~12 第5列表示星期0~6（0表示星期天） 第6列表示要执行的命令 每隔十分钟执行，在12月06号22点开始，每周都执行 */10 22 06 12 * commad 禁用 crontab -e 在任务前面加# 补充 crontab 每隔十秒执行 */1 * * * * shell脚本 休眠 sleep 后台执行命令 &amp; nohup 工作中，例如： 执行： ./xxx.sh &amp; 查看： pa -ef | grep xxx 杀死： kill -9 $(pgrep -f xxx) pgrep -f test：打印pid 有时候会失效，需要结合nohup nohup ./xxx.sh &amp; 每次执行一条命令的结果会存放到nohup.out 重定向日志到指定的文件 nohup ./xxx.sh &gt; xxx.log 2&gt;&amp;1 &amp; screen（需要安装：yum -y install screen） screen: screen -list : 查看会话 screen -S xxx : 建立一个后台的会话 screen -r xxx/id：进入会话 ctrl + a + d:退出会话 detached 分离 shell脚本 文件名: xxx.sh 第一行：#!/bin/bash (-x) 如果不加-x就是不进入调试模式，如果加了-x就是进入调试模式 =前后不能有空格 案例 # vi test.sh #!/bin/bash step=1 #间隔的秒数，不能大于60 =前后不能有空格 for (( i = 0; i &lt; 60; i=(i+step) )); do echo `date` &gt;&gt; date.log sleep $step done exit 执行shell脚本 当前目录下:./xxx.sh 注意点 1、=前后不能有空格 2、第一行加 -x，是进入调试模式 3、chmod +x xxx 或者 chmod 744 xxx 4、./xxx.sh]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-常用命令汇总]]></title>
    <url>%2F2017%2F12%2F04%2Flinux-frequently-used-command-.html</url>
    <content type="text"><![CDATA[Linux 命令 pwd 查看当前目录 cd cd 和 cd ~ 进用户的家目录 标识是~ /home/用户名称 cd /etc 进目录 cd ../ 退回上一层目录 cd ../../ 退回上两层目录 cd - 退回上一次的目录 hostname hostname 查看机器名称 hostname xxx 设置机器名称（临时） hostname -i 查看机器的IP ls ls 文件或文件夹名 ls -l 文件或文件夹详细列表 ll 等价 ls -l ll -a 隐藏文件 以.为开头 ll -h 文件大小 查看文件内容 cat test.log 直接输出文件内容 more test.log 一页页往下翻 less test.log 一页页往上翻 tail -F xxx.log 实时查看log文件 tail -f xxx.log 查看log文件 # 注： 参数区别： -F==》-f --retry 查看文件(文件夹)的大小 du -sh filename vi vi test.txt 进入命令模式 gg 第一行第一个字母 G 最后一行第一个字母 shift+$ 行的最后一个字母 dd 删除当前行 dG 删除光标以下的所有行 ndd 删除光标所在的向下n行 i 插入--&gt; 编辑模式 ECS 退出编辑模式--&gt;尾行模式 尾行模式: :q 退出 vi 编辑器 :w 保存修改的内容 :wq 保存并退出 :q! 强制退出，当对文本内容作了修改而不想要保存时 :w! 强制保存，当没有文本的写权限时 :set number或:set nu 显示行号 :set nonumber或:set nonu 取消显示行号 :/内容/ 或/内容 查找指定内容 //n将光标移动到下一个目标 //N上一个 :n 跳转到第n行 alias 别名 http://blog.51cto.com/aixecc/787590 临时设置，重启后无效 alias 别名=&#39;原命令 参数&#39; 例：alias vi=&#39;vim&#39; 永久设置，需要重启后才生效 更改~/.bashrc或/etc/bashrc touch 创建一个空白的文件 mkdir 创建文件夹 mkdir xxx mkdir -p xxx1[表情]x2 rm 删除文件或文件夹 rm 文件名称 rm -f 文件名称 rm -r 文件夹 rm -rf 文件夹 -r 指的是递归 (目录) -f 指的是强制 不提示 ps/netstat [root@hadoop-01 ~]# ps -ef|grep ssh 查看当前的进程的端口号 [root@hadoop-01 ~]# netstat -nlp|grep 1450 [root@hadoop-01 ~]# ps -ef|grep 1450 根据端口号查看进程 netstat -nlp|grep 22 搜索 find / -name &#39;name&#39; 全局搜索 locate java which java echo echo &quot;xxxx&quot; 输出 echo &quot;xxxx&quot; &gt; test.log 覆盖 echo &quot;xxxx&quot; &gt;&gt; test.log 追加 wget 下载 wget http://ftp.iij.ad.jp/pub/db/mysql/Downloads/MySQL-5.6/mysql-5.6.35-linux-glibc2.5-x86_64.tar.gz yum yum -y install xxx 搜索、卸载rpm rpm -qa |grep xxx rpm --nodeps -e xxx # 注: --nodeps 不验证包的依赖关系 强行卸载 mv、cp mv 移动 cp 复制 mv test.log test.log20170813 mv 文件1/文件夹 文件11/文件夹 cp test.log20170813 test.log20170814 cp -r 文件1/文件夹 文件11/文件夹 tar、zip tar -czf test.tar.gz test.log 压缩 tar -xzvf test.tar.gz 解压 zip(压缩)/unzip(解压) 磁盘和内存 查看磁盘大小及其使用情况 [root@hadoop-01 ~]# df -h 查看内存大小及其使用情况 [root@hadoop-01 ~]# free -m 用户和用户组 用户 [root@hadoop-01 ~]# ll /usr/sbin/user* -rwxr-x---. 1 root root 103096 Dec 8 2011 /usr/sbin/useradd -rwxr-x---. 1 root root 69560 Dec 8 2011 /usr/sbin/userdel -rwxr-x---. 1 root root 98680 Dec 8 2011 /usr/sbin/usermod 创建用户 [root@hadoop-01 ~]# useradd dwz01 创建已存在的用户 [root@hadoop-01 ~]# useradd dwz useradd: user &#39;dwz&#39; already exists [root@hadoop-01 ~]# id dwz01 uid=515(dwz01) gid=515(dwz01) groups=515(dwz01) 给用户设置密码 [root@hadoop-01 ~]# passwd dwz01 查看已有的用户 [root@hadoop-01 ~]# ll /home/ total 8 drwx------. 27 dwz dwz 4096 Aug 13 14:31 dwz drwx------. 4 dwz01 dwz01 4096 Aug 16 21:38 dwz01 用户组 [root@hadoop-01 ~]# ll /usr/sbin/group* -rwxr-x---. 1 root root 54968 Dec 8 2011 /usr/sbin/groupadd -rwxr-x---. 1 root root 46512 Dec 8 2011 /usr/sbin/groupdel -rwxr-x---. 1 root root 50800 Dec 8 2011 /usr/sbin/groupmems -rwxr-x---. 1 root root 61360 Dec 8 2011 /usr/sbin/groupmod 创建用户组 [root@hadoop-01 ~]# groupadd bigdata 用户组的文件 [root@hadoop-01 ~]# cat /etc/group 将dwz用户添加到bigdata组 [root@hadoop-01 ~]# usermod -a -G bigdata dwz 给一个普通用户添加sudo权限 vi /etc/sudoers #在 root ALL=(ALL) ALL 下面添加一行 dwz ALL=(ALL) ALL 切换用户su [root@hadoop-01 ~]# su - dwz [dwz@hadoop-01 ~]$ su - Password: #输入root密码，切到root 修改文件权限 chmod 750 test.log rwxr-x--- chmod -R 777 test chown dwz:dwz test.log chown -R dwz:dwz test -R 递归 文件夹 软连接 ln -s 原始目录 目标目录]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux：3、Linux常用命令]]></title>
    <url>%2F2017%2F12%2F03%2Flinux-3-linux-command.html</url>
    <content type="text"><![CDATA[查看用户目录 ll /usr/sbin/user* 查看用户组目录 ll /usr/sbin/group* 查看用户 ll /usr/sbin/user* 查看用户组 ll /usr/sbin/group* 添加用户 useradd xxx 删除之后如果再次创建的时候会提示家目录已经存在 当前创建的用户： id 用户 gid：主组 groups：所有组 删除用户 userdel 用户 创建用户组 groupadd 用户组 把用户添加到用户组 usermod -a -G 用户组 用户 修改用户的密码 passwd 用户 怎样切换用户 su 用户 su - 用户 - ：执行切换用户的环境变量，进入家目录 退出当前用户 su su - 临时获取root的最大权限 sudo 修改某个用户具有root的权限 vi /etc/sudoers xxx ALL=(root) NOPASSWD:ALL 怎样查看进程 ps -ef | grep ntpd ntp ：进程运行user 1393：pid 1：父进程id 杀死进程 kill -9 进程的pid 怎样查看端口号 netstat -nlp | grep ntp 案例 打开某个xxx服务的web页面：ip+port http://ip:root ps -ef | grep xxx --&gt; pid --&gt; netstat -nlp | grep pid 等价于 netstat -nlp | grep xxx 0.0.0.0 代表当前机器的ip 权限 读 r 4 写 w 2 执行 x 1 - ： 无权限，占位 -rw-r--r--： 第一位：- 文件 d文件夹 l连接 第一组：rw- root这个用户对这个文件 只有读写权限 第二组：r-- root这个组的所有用户对这个文件只有读权限 第三组：r-- 其他用户组的所有用户对这个文件只有读权限 chown：修改文件夹或文件的所属的用户及用户组 chown [-R] xxx:xxx /xxx chown xxx:xxx /root/xxx.log chmod：修改文件夹或文件的权限 chmod -R 777 /xxx chmod 644 /root/xxx.log chmod +x /root/xxx.log 所有的都加上可执行操作 rw-r--r-- ===&gt; 644 rwxrwxrwx ===&gt; 777 r-x ===&gt; 5 文件夹： -R 递归,只出现在chown和chmod 补充：rm -r -f /xxx 安装rpm yum install httpd yum -y install httpd 可选的 启动： service httpd start | status | stop | restart 查看端口号： netstat -nlp | grep 3606 搜索rpm yum search xxx 搜索、卸载rpm 过滤： rpm --qa | grep http rpm -e xxx rpm --nodes -e xxx --nodeps 不验证包的依赖关系，强行卸载 命令或某个文件在哪？ –&gt;搜索 which xxx 定位命令 locate xxx find / -name &quot;*xxx*&quot; 查看磁盘大小 df -h 查看内存大小 free -m/-g -m：多少兆，一般是-m -g：多少g 实时打印当前机器的负载情况（系统情况查看） top 怎样下载软件包 wget http下载地址 压缩包解压 tar -zxvf xxx.tar.gz unzip xxx.zip 文件压缩 tar -czf jpg.tar.gz *.xxx 最常用的 频率最高的 zip ***.zip *.xxx 压缩解压 tar 压缩解压 zip 压缩 unzip 解压 tar命令 解包：tar zxvf FileName.tar 打包：tar czvf FileName.tar DirName gz命令 解压1：gunzip FileName.gz 解压2：gzip -d FileName.gz 压缩：gzip FileName .tar.gz 和 .tgz 解压：tar zxvf FileName.tar.gz 压缩：tar zcvf FileName.tar.gz DirName 压缩多个文件：tar zcvf FileName.tar.gz DirName1 DirName2 DirName3 ... bz2命令 解压1：bzip2 -d FileName.bz2 解压2：bunzip2 FileName.bz2 压缩： bzip2 -z FileName .tar.bz2 解压：tar jxvf FileName.tar.bz2 压缩：tar jcvf FileName.tar.bz2 DirName bz命令 解压1：bzip2 -d FileName.bz 解压2：bunzip2 FileName.bz 压缩：未知 .tar.bz 解压：tar jxvf FileName.tar.bz Z命令 解压：uncompress FileName.Z 压缩：compress FileName .tar.Z 解压：tar Zxvf FileName.tar.Z 压缩：tar Zcvf FileName.tar.Z DirName zip命令 解压：unzip FileName.zip 压缩：zip FileName.zip DirName 注意 如果进入创建的用户hadoop之后是-bash-4.1$，但是希望显示的是[hadoop@admin ~]，那么执行以下内容： [root@admin ~]# cp -pr /etc/skel/.bash* /home/hadoop/ [root@admin ~]# chown -R hadoop /home/hadoop/ [root@admin ~]# su - hadoop [hadoop@admin ~]$]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux：2、Linux常用命令]]></title>
    <url>%2F2017%2F12%2F02%2Flinux-2-linux-command.html</url>
    <content type="text"><![CDATA[查看IP ifconfig 或者 hostname -i（需要配置文件之后才可以使用） ipconfig（Windows） 关闭防火墙 Service iptables status chkconfig iptables off 配置静态IP地址 vi /etc/sysconfig/network-scripts/ifcfg-eth0 ONBOOT=yes NM_CONTROLLED=no BOOTPROTO=&quot;static&quot; IPADDR=192.168.137.200 NETMASK=255.255.255.0 GATEWAY=192.168.137.2 DNS1=10.64.0.10 保存退出执行Service network restart 实时查看文件内容 tail -F xxx.log echo &quot;1&quot; &gt; xxx.log echo &quot;2&quot; &gt;&gt; xxx.log &gt;:覆盖（慎用） &gt;&gt;:追加 -F参数 ==&gt; -f --retry 重命名 mv xxx1 xxx2 复制 cp xxx1 xxx2 以前的文件不动，产生新的文件 提醒： xxx.default xxx.xml cp xxx.xml xxx.xml20171205 (养成备份习惯) 输出打印 echo &quot;123&quot; alias alias 查看有哪些别名 临时： alias jh=&#39;cd /home/jepson&#39; 设置jh别名 =前后没有空格 永久： alias jh=&#39;cd /home/jepson&#39; 配置到环境变量文件，执行生效命令 cd jh 环境变量配置 全局： /etc/profile 个人： ~/.bash_profile 或者 ~/.bashrc 1.假设没有.bash_profile,怎么办？ 2.注意.bash_profile的权限问题 生效： source /etc/profile . .bash_profile 或者 source .bash_profile 删除文件 rm 删除时需要询问 rm -f xxx强制删除文件，不询问 rm -r -f 或者 rm -rf 递归，强制删除文件夹 慎用：rm -rf / (手工) shell脚本 [root@rzdatahadoop001 ~]# mkdir /home/jepson/001 [root@rzdatahadoop001 ~]# jpath=&quot;/home/jepson/001&quot; [root@rzdatahadoop001 ~]# echo $jpath /home/jepson/001 [root@rzdatahadoop001 ~]# touch /home/jepson/001/xxx.log [root@rzdatahadoop001 ~]# echo $jpath /home/jepson/001 [root@rzdatahadoop001 ~]# rm -rf $jpath/* [root@rzdatahadoop001 ~]# ll /home/jepson/001/ total 0 #逻辑错误导致jpath复制为&quot;&quot; [root@rzdatahadoop001 ~]# touch /home/jepson/001/xxx.log [root@rzdatahadoop001 ~]# jpath=&quot;&quot; [root@rzdatahadoop001 ~]# echo $jpath [root@rzdatahadoop001 ~]# rm -rf $jpath/* ==&gt;rm -rf /* 怎样查看历史命令 history 管道符 | 怎样过滤 grep 例如：history | grep xxx vi 三种模式： 命令模式 gg 第一行第一个字母 G 最后一行第一个字母 shift+$ 行的最后一个字母 dd 删除当前行 dG 删除光标以下的所有行 ndd 删除光标所在的向下n行 i 插入--&gt; 编辑模式 编辑模式: : ECS 退出--&gt;尾行模式 尾行模式: : :q 退出vi编辑器 :w 保存修改的内容 :wq 保存并退出 :q! 强制退出，当对文本内容作了修改而不想要保存时 :w! 强制保存，当没有文本的写权限时 :set number 或 :set nu 显示行号 :set nonumber 或 :set nonu 取消显示行号 :/内容/ 或 /内容 查找指定内容 //n将光标移动到下一个目标 //N上一个 :n 跳转到第n行 假如文件内容满满的，我需要重新覆盖文件 ctrl + a，Ctrl + c --&gt; gg + dg --&gt;i,右键粘贴 --&gt;esc --&gt;shift + : ---&gt;wq]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见错误：1、CentOS克隆导致网卡eth0变成eth1、及修改网卡名]]></title>
    <url>%2F2017%2F12%2F01%2Ferror-1-centos-clone-eth0-eth1.html</url>
    <content type="text"><![CDATA[为什么eth0会变成eth1?很多Linux distribution使用udev动态管理设备文件，并根据设备的信息对其进行持久化命名。 udev会在系统引导的过程中识别网卡，将mac地址和网卡名称对应起来记录在udev的规则脚本中。 而对于新的虚拟机，VMware会自动为虚拟机的网卡生成MAC地址，当你克隆或者重装虚拟机软件时， 由于你使用的是以前系统虚拟硬盘的信息，而该系统中已经有eth0的信息，对于这个新的网卡， udev会自动将其命名为eth1（累加的原则），所以在你的系统启动后，你使用ifconfig看到的网卡名为eth1。 解决方案1、修改主机名（注：此处根据个人需要，不修改也行，此处我是用于搭建集群，修改主机名做区分） vi /etc/sysconfig/network 修改主机名hostname的值 2、vi /etc/udev/rules.d/70-persistent-net.rules # PCI device 0x8086:0x100f (e1000) SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR{address}==&quot;00:0c:29:5a:6c:73&quot;, ATTR{type}==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth0&quot; # PCI device 0x8086:0x100f (e1000) SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR{address}==&quot;00:0c:29:e5:6b:c4&quot;, ATTR{type}==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth1&quot; 3、删除第一个eth0行信息 ，将第二个eth1改为eth0 ，最终如下 # PCI device 0x8086:0x100f (e1000) SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR{address}==&quot;00:0c:29:e5:6b:c4&quot;, ATTR{type}==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth0&quot; 4、vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改HWADDR为step3中的00:0c:29:e5:6b:c4 修改IPADDR为新的一个ip即可 DEVICE=eth0 HWADDR=00:0C:29:95:4A:D3 TYPE=Ethernet UUID=ad391919-c736-4d3d-b24d-1d78427e7c6e ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=static IPADDR=192.168.8.111 NETMASK=255.255.255.0 GATEWAY=192.168.8.1 DNS1=4.4.4.4 DNS2=8.8.8.8 5、重启机器 reboot]]></content>
      <categories>
        <category>常见错误</category>
      </categories>
      <tags>
        <tag>常见错误</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux：1、Linux部署及常用命令]]></title>
    <url>%2F2017%2F11%2F29%2Flinux-1-linux-deployment-command.html</url>
    <content type="text"><![CDATA[vmware安装CentOS CentOS修改主机名 编辑 /etc/sysconfig/network [root@localhost ~]# vi /etc/sysconfig/network networkNETWORKING=yes HOSTNAME=hadoop-01 编辑 /etc/hosts [root@localhost~]# vi/etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4: :1 localhost localhost.localdomain localhost6 localhost6.localdomain6 # 添加如下： 192.168.137.130 hadoop-01 Linux命令 按一次Tab键自动补全，2次打印所有当前相关的 pwd 显示当前用户所在的目录 家目录 root /root admin：/home/admin xxx： /home/xxx cd cd : 切换到当前用户的家目录 cd ~： 切换到当前用户的家目录 cd /tmp：切换到指定目录 cd -： 切换到上一次的目录 cd ../: 退到上一层目录 clear 清空桌面 ls man ls：查看ls所有的命令 ls -l（等价于ll）：打印当前目录的所有文件夹和文件的名称、权限、日期 ls -l -a/ -la（等价于ll -a）：打印隐藏文件，隐藏文件的标识是以 .为开始 ls -lh（等价于ll -h）：主要是查看文件的大小 ls -lrt：查看当前目录，按照时间排序 r：排序 t：修改时间 ls filename：查看文件名字 怎么查看命令帮助 man ls --&gt; ctrl + z ls --help mkdir mkdir rzdata01 创建单级文件夹 mkdir -p rzdata01/aaa/bbb：创建级联文件夹 touch touch filename 查看文件内容 cat filename：打印所有的内容 more filename：一页一页的翻（空格键翻页） vi vi filename ---&gt; i键 ---&gt;进入到编辑模式，增加删除修改文件 ---&gt; esc键(退出编辑模式) --&gt;shift + 冒号键 ---&gt;进入尾行模式 ---&gt;按wq键保存退出 wq：保存退出 q!：强制退出，修改失效 修改hostname 修改永久性hostname： vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=hadoop1 修改之后重启 修改临时名字： hostname hadoop1 修改之后重新进入]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>

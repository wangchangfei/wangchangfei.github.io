<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Spark,"><link rel="alternate" href="/atom.xml" title="Changfei's Tech Blog" type="application/atom+xml"><meta name="description" content="编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行"><meta name="keywords" content="Spark"><meta property="og:type" content="article"><meta property="og:title" content="Spark：4、Spark RDD 编程（Transformation）"><meta property="og:url" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation.html"><meta property="og:site_name" content="Changfei&#39;s Tech Blog"><meta property="og:description" content="编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-1.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-2.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-3.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-4.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-5.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-6.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-8.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-9.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-10.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-11.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-12.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-13.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-14.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-15.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-16.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-17.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-18.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-19.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-20.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-21.png"><meta property="og:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-22.png"><meta property="og:updated_time" content="2018-11-20T02:07:46.771Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Spark：4、Spark RDD 编程（Transformation）"><meta name="twitter:description" content="编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行"><meta name="twitter:image" content="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation/spark-1.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation.html"><title>Spark：4、Spark RDD 编程（Transformation） | Changfei's Tech Blog</title></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Changfei's Tech Blog</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">在纷杂的世界安静的写代码</h1></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation.html"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="CoderF"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="Changfei's Tech Blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Spark：4、Spark RDD 编程（Transformation）</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-09T21:22:23+08:00">2018-02-09</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a></span></span> <span id="/2018/02/09/spark-4-spark-rdd-program-transformation.html" class="leancloud_visitors" data-flag-title="Spark：4、Spark RDD 编程（Transformation）"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数&#58;</span><span class="leancloud-visitors-count"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">4,865</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">25</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。</p><p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-1.png"><a id="more"></a><h2 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h2><p>在Spark中创建RDD的创建方式大概可以分为三种：</p><p>（1）从集合中创建RDD；</p><p>（2）从外部存储创建RDD；</p><p>（3）从其他RDD创建。</p><p>1) 由一个已经存在的Scala集合创建，集合并行化。</p><p>例如</p><pre><code class="scala">val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))
</code></pre><p>而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</p><pre><code class="scala">def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T]

def makeRDD[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T]


def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T]
</code></pre><p>我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p><pre><code class="scala">def makeRDD[T: ClassTag](
    seq: Seq[T],
    numSlices: Int = defaultParallelism): RDD[T] = withScope {
  parallelize(seq, numSlices)
}
</code></pre><p>我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])].</p><p>Spark文档的说明是：</p><p>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.</p><p>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p><pre><code class="scala">scala&gt; val test1= sc.parallelize(List(1,2,3))
test1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21

scala&gt; val test2 = sc.makeRDD(List(1,2,3))
test2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21

scala&gt; val seq = List((1, List(&quot;slave01&quot;)), (2, List(&quot;slave02&quot;)))
seq: List[(Int, List[String])] = List((1,List(slave01)), (2,List(slave02)))

scala&gt; val test3 = sc.makeRDD(seq)
test3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23

scala&gt; test3.preferredLocations(test3.partitions(1))
res26: Seq[String] = List(slave02)

scala&gt; test3.preferredLocations(test3.partitions(0))
res27: Seq[String] = List(slave01)

scala&gt; test1.preferredLocations(test1.partitions(0))
res28: Seq[String] = List()
</code></pre><p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下</p><pre><code class="scala">def parallelize[T: ClassTag](
    seq: Seq[T],
    numSlices: Int = defaultParallelism): RDD[T] = withScope {
  assertNotStopped()
  new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
}

def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {
  assertNotStopped()
  val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap
  new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)
}
</code></pre><p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><p>2) 由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p><pre><code class="scala">scala&gt; val test = sc.textFile(&quot;hdfs://hadoop001:9000/RELEASE&quot;)
test: org.apache.spark.rdd.RDD[String] = hdfs://hadoop001:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24
</code></pre><h2 id="RDD算子操作"><a href="#RDD算子操作" class="headerlink" title="RDD算子操作"></a>RDD算子操作</h2><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。</p><h3 id="常用算子：Transformation"><a href="#常用算子：Transformation" class="headerlink" title="常用算子：Transformation"></a>常用算子：Transformation</h3><p>1) map(func)</p><p>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成。<br><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-2.png"></p><pre><code class="scala">scala&gt; var source  = sc.parallelize(1 to 10)
source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24

scala&gt; source.collect()
res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala&gt; val mapadd = source.map(_ * 2)
mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26

scala&gt; mapadd.collect()
res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)
</code></pre><p>2) filter(func)</p><p>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-3.png"><pre><code class="scala">scala&gt; var sourceFilter = sc.parallelize(Array(&quot;xiaoming&quot;,&quot;xiaojiang&quot;,&quot;xiaohe&quot;,&quot;dazhi&quot;))
sourceFilter: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24

scala&gt; val filter = sourceFilter.filter(_.contains(&quot;xiao&quot;))
filter: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at &lt;console&gt;:26

scala&gt; sourceFilter.collect()
res9: Array[String] = Array(xiaoming, xiaojiang, xiaohe, dazhi)

scala&gt; filter.collect()
res10: Array[String] = Array(xiaoming, xiaojiang, xiaohe)
</code></pre><p>3) flatMap(func)</p><p>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-4.png"><pre><code class="scala">scala&gt; val sourceFlat = sc.parallelize(1 to 5)
sourceFlat: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24

scala&gt; sourceFlat.collect()
res11: Array[Int] = Array(1, 2, 3, 4, 5)

scala&gt; val flatMap = sourceFlat.flatMap(1 to _)
flatMap: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26

scala&gt; flatMap.collect()
res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)
</code></pre><p>4) mapPartitions(func)</p><p>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-5.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;),(&quot;lucy&quot;,&quot;female&quot;)))
rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24

scala&gt; :paste
// Entering paste mode (ctrl-D to finish)

def partitionsFun(iter : Iterator[(String,String)]) : Iterator[String] = {
  var woman = List[String]()
  while (iter.hasNext){
    val next = iter.next()
    next match {
       case (_,&quot;female&quot;) =&gt; woman = next._1 :: woman
       case _ =&gt;
    }
  }
  woman.iterator
}

// Exiting paste mode, now interpreting.

partitionsFun: (iter: Iterator[(String, String)])Iterator[String]

scala&gt; val result = rdd.mapPartitions(partitionsFun)
result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at mapPartitions at &lt;console&gt;:28

scala&gt; result.collect()
res13: Array[String] = Array(kpop, lucy)
</code></pre><p>5) mapPartitionsWithIndex(func)</p><p>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-6.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;),(&quot;lucy&quot;,&quot;female&quot;)))
rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24

scala&gt; :paste
// Entering paste mode (ctrl-D to finish)

def partitionsFun(index : Int, iter : Iterator[(String,String)]) : Iterator[String] = {
  var woman = List[String]()
  while (iter.hasNext){
    val next = iter.next()
    next match {
       case (_,&quot;female&quot;) =&gt; woman = &quot;[&quot;+index+&quot;]&quot;+next._1 :: woman
       case _ =&gt;
    }
  }
  woman.iterator
}

// Exiting paste mode, now interpreting.

partitionsFun: (index: Int, iter: Iterator[(String, String)])Iterator[String]

scala&gt; val result = rdd.mapPartitionsWithIndex(partitionsFun)
result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at mapPartitionsWithIndex at &lt;console&gt;:28

scala&gt; result.collect()
res14: Array[String] = Array([0]kpop, [3]lucy)
</code></pre><p>6) sample(withReplacement, fraction, seed)</p><p>以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24

scala&gt; rdd.collect()
res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala&gt; var sample1 = rdd.sample(true,0.4,2)
sample1: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[21] at sample at &lt;console&gt;:26

scala&gt; sample1.collect()
res16: Array[Int] = Array(1, 2, 2, 7, 7, 8, 9)

scala&gt; var sample2 = rdd.sample(false,0.2,3)
sample2: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[22] at sample at &lt;console&gt;:26

scala&gt; sample2.collect()
res17: Array[Int] = Array(1, 9)
</code></pre><p>7) takeSample</p><p>和Sample的区别是：takeSample返回的是最终的结果集合。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-8.png"><p>8) union(otherDataset)</p><p>对源RDD和参数RDD求并集后返回一个新的RDD。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-9.png"><pre><code class="scala">scala&gt; val rdd1 = sc.parallelize(1 to 5)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24

scala&gt; val rdd2 = sc.parallelize(5 to 10)
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24

scala&gt; val rdd3 = rdd1.union(rdd2)
rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[25] at union at &lt;console&gt;:28

scala&gt; rdd3.collect()
res18: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10)
</code></pre><p>9) intersection(otherDataset)</p><p>对源RDD和参数RDD求交集后返回一个新的RDD。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-10.png"><pre><code class="scala">scala&gt; val rdd1 = sc.parallelize(1 to 7)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24

scala&gt; val rdd2 = sc.parallelize(5 to 10)
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:24

scala&gt; val rdd3 = rdd1.intersection(rdd2)
rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at intersection at &lt;console&gt;:28

scala&gt; rdd3.collect()
[Stage 15:=============================&gt;                       (2 + 2)                      res19: Array[Int] = Array(5, 6, 7)
</code></pre><p>10) distinct([numTasks]))</p><p>对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-11.png"><pre><code class="scala">scala&gt; val distinctRdd = sc.parallelize(List(1,2,1,5,2,9,6,1))
distinctRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24

scala&gt; val unionRDD = distinctRdd.distinct()
unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[37] at distinct at &lt;console&gt;:26

scala&gt; unionRDD.collect()
[Stage 16:&gt; (0 + 4) [Stage 16:=============================&gt;                            (2 + 2)                                                                             res20: Array[Int] = Array(1, 9, 5, 6, 2)

scala&gt; val unionRDD = distinctRdd.distinct(2)
unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at distinct at &lt;console&gt;:26

scala&gt; unionRDD.collect()
res21: Array[Int] = Array(6, 2, 1, 9, 5)
</code></pre><p>11) partitionBy</p><p>对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区，否则会生成ShuffleRDD。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-12.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;),(4,&quot;ddd&quot;)),4)
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24

scala&gt; rdd.partitions.size
res24: Int = 4

scala&gt; var rdd2 = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))
rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[45] at partitionBy at &lt;console&gt;:26

scala&gt; rdd2.partitions.size
res25: Int = 2
</code></pre><p>12) reduceByKey(func, [numTasks])</p><p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-13.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((&quot;female&quot;,1),(&quot;male&quot;,5),(&quot;female&quot;,5),(&quot;male&quot;,2)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[46] at parallelize at &lt;console&gt;:24

scala&gt; val reduce = rdd.reduceByKey((x,y) =&gt; x+y)
reduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[47] at reduceByKey at &lt;console&gt;:26

scala&gt; reduce.collect()
res29: Array[(String, Int)] = Array((female,6), (male,7))
</code></pre><p>13) groupByKey</p><p>groupByKey也是对每个key进行操作，但只生成一个sequence。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-14.png"><pre><code class="scala">scala&gt; val words = Array(&quot;one&quot;, &quot;two&quot;, &quot;two&quot;, &quot;three&quot;, &quot;three&quot;, &quot;three&quot;)
words: Array[String] = Array(one, two, two, three, three, three)

scala&gt; val wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1))
wordPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at &lt;console&gt;:26

scala&gt; val group = wordPairsRDD.groupByKey()
group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[5] at groupByKey at &lt;console&gt;:28

scala&gt; group.collect()
res1: Array[(String, Iterable[Int])] = Array((two,CompactBuffer(1, 1)), (one,CompactBuffer(1)), (three,CompactBuffer(1, 1, 1)))

scala&gt; group.map(t =&gt; (t._1, t._2.sum))
res2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at &lt;console&gt;:31

scala&gt; res2.collect()
res3: Array[(String, Int)] = Array((two,2), (one,1), (three,3))

scala&gt; val map = group.map(t =&gt; (t._1, t._2.sum))
map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at map at &lt;console&gt;:30

scala&gt; map.collect()
res4: Array[(String, Int)] = Array((two,2), (one,1), (three,3))
</code></pre><p>14) combineByKey<a href="createCombiner: V =&gt; C, mergeValue: (C, V" target="_blank" rel="noopener">C</a> =&gt; C, mergeCombiners: (C, C) =&gt; C)</p><p>对相同K，把V合并成一个集合.</p><p>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</p><p>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</p><p>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-15.png"><pre><code class="scala">scala&gt; val scores = Array((&quot;Fred&quot;, 88), (&quot;Fred&quot;, 95), (&quot;Fred&quot;, 91), (&quot;Wilma&quot;, 93), (&quot;Wilma&quot;, 95), (&quot;Wilma&quot;, 98))
scores: Array[(String, Int)] = Array((Fred,88), (Fred,95), (Fred,91), (Wilma,93), (Wilma,95), (Wilma,98))

scala&gt; val input = sc.parallelize(scores)
input: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26

scala&gt; val combine = input.combineByKey(
     |     (v)=&gt;(v,1),
     |     (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1),
     |     (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))
combine: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[53] at combineByKey at &lt;console&gt;:28

scala&gt; val result = combine.map{
     |     case (key,value) =&gt; (key,value._1/value._2.toDouble)}
result: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[54] at map at &lt;console&gt;:30

scala&gt; result.collect()
res33: Array[(String, Double)] = Array((Wilma,95.33333333333333), (Fred,91.33333333333333))
</code></pre><p>15) aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U)</p><p>在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。<br>seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</p><p>例如：List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8))，分一个分区，以key为1的分区为例，0先和3比较得3，3在和2比较得3，3在和4比较得4，所以整个key为1的组最终结果为（1，4），同理，key为2的最终结果为（2，3），key为3的为（3，8）.<br>如果分三个分区，前两个是一个分区，中间两个是一个分区，最后两个是一个分区，第一个分区的最终结果为（1，3），第二个分区为（1，4）（2，3），最后一个分区为（3，8），combine后为 (3,8), (1,7), (2,3)</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-16.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)))
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24

scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_)
agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[13] at aggregateByKey at &lt;console&gt;:26

scala&gt; agg.collect()
res7: Array[(Int, Int)] = Array((3,8), (1,7), (2,3))

scala&gt; agg.partitions.size
res8: Int = 3

scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1)
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24

scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect()
agg: Array[(Int, Int)] = Array((1,4), (3,8), (2,3))
</code></pre><p>16) foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]<br>aggregateByKey的简化操作，seqop和combop相同。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)))
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[91] at parallelize at &lt;console&gt;:24

scala&gt; val agg = rdd.foldByKey(0)(_+_)
agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[92] at foldByKey at &lt;console&gt;:26

scala&gt; agg.collect()
res61: Array[(Int, Int)] = Array((3,14), (1,9), (2,3))
</code></pre><p>17) sortByKey([ascending], [numTasks])<br>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((3,&quot;aa&quot;),(6,&quot;cc&quot;),(2,&quot;bb&quot;),(1,&quot;dd&quot;)))
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24

scala&gt; rdd.sortByKey(true).collect()
res9: Array[(Int, String)] = Array((1,dd), (2,bb), (3,aa), (6,cc))

scala&gt; rdd.sortByKey(false).collect()
res10: Array[(Int, String)] = Array((6,cc), (3,aa), (2,bb), (1,dd))
</code></pre><p>18) sortBy(func,[ascending], [numTasks])</p><p>与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List(1,2,3,4))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24

scala&gt; rdd.sortBy(x =&gt; x).collect()
res11: Array[Int] = Array(1, 2, 3, 4)

scala&gt; rdd.sortBy(x =&gt; x%3).collect()
res12: Array[Int] = Array(3, 4, 1, 2)
</code></pre><p>19) join(otherDataset, [numTasks])</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-17.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24

scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))
rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24

scala&gt; rdd.join(rdd1).collect()
res13: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6)))
</code></pre><p>20) cogroup(otherDataset, [numTasks])</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD。</w></v></p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-18.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24

scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))
rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24

scala&gt; rdd.cogroup(rdd1).collect()
res14: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))

scala&gt; val rdd2 = sc.parallelize(Array((4,4),(2,5),(3,6)))
rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[41] at parallelize at &lt;console&gt;:24

scala&gt; rdd.cogroup(rdd2).collect()
res15: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))

scala&gt; val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))
rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24

scala&gt; rdd3.cogroup(rdd2).collect()
[Stage 36:&gt;                                                         (0 + 0)                                                                             res16: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(d, a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))
</code></pre><p>21) cartesian(otherDataset)</p><p>笛卡尔积</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-19.png"><pre><code class="scala">scala&gt; val rdd1 = sc.parallelize(1 to 3)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24

scala&gt; val rdd2 = sc.parallelize(2 to 5)
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at &lt;console&gt;:24

scala&gt; rdd1.cartesian(rdd2).collect()
res17: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (2,2), (2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5))
</code></pre><p>22) pipe(command, [envVars])</p><p>对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。</p><p>Shell脚本</p><pre><code class="shell">#!/bin/sh
echo &quot;AA&quot;
while read LINE; do
   echo &quot;&gt;&gt;&gt;&quot;${LINE}
done
</code></pre> <font color="red">shell脚本需要集群中的所有节点都能访问到。</font><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;),1)
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[50] at parallelize at &lt;console&gt;:24

scala&gt; rdd.pipe(&quot;/home/bigdata/pipe.sh&quot;).collect()
res18: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)

scala&gt; val rdd = sc.parallelize(List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;),2)
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:24

scala&gt; rdd.pipe(&quot;/home/bigdata/pipe.sh&quot;).collect()
res19: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, AA, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)

pipe.sh:
#!/bin/sh
echo &quot;AA&quot;
while read LINE; do
   echo &quot;&gt;&gt;&gt;&quot;${LINE}
done
</code></pre><p>23) coalesce(numPartitions)</p><p>缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[54] at parallelize at &lt;console&gt;:24

scala&gt; rdd.partitions.size
res20: Int = 4

scala&gt; val coalesceRDD = rdd.coalesce(3)
coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[55] at coalesce at &lt;console&gt;:26

scala&gt; coalesceRDD.partitions.size
res21: Int = 3
</code></pre><p>24) repartition(numPartitions)</p><p>根据分区数，从新通过网络随机洗牌所有数据。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at &lt;console&gt;:24

scala&gt; rdd.partitions.size
res22: Int = 4

scala&gt; val rerdd = rdd.repartition(2)
rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[60] at repartition at &lt;console&gt;:26

scala&gt; rerdd.partitions.size
res23: Int = 2

scala&gt; val rerdd = rdd.repartition(4)
rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[64] at repartition at &lt;console&gt;:26

scala&gt; rerdd.partitions.size
res24: Int = 4
</code></pre><p>25) repartitionAndSortWithinPartitions(partitioner)</p><p>repartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高。</p><p>26) glom</p><p>将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-20.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[65] at parallelize at &lt;console&gt;:24

scala&gt; rdd.glom().collect()
res25: Array[Array[Int]] = Array(Array(1, 2, 3, 4), Array(5, 6, 7, 8), Array(9, 10, 11, 12), Array(13, 14, 15, 16))
</code></pre><p>27) mapValues</p><p>针对于(K,V)形式的类型只对V进行操作。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-21.png"><pre><code class="scala">scala&gt; val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))
rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[67] at parallelize at &lt;console&gt;:24

scala&gt; rdd3.mapValues(_+&quot;|||&quot;).collect()
res26: Array[(Int, String)] = Array((1,a|||), (1,d|||), (2,b|||), (3,c|||))
</code></pre><p>28) subtract</p><p>计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来。</p> <img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-22.png"><pre><code class="scala">scala&gt; val rdd = sc.parallelize(3 to 8)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[70] at parallelize at &lt;console&gt;:24

scala&gt; val rdd1 = sc.parallelize(1 to 5)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[71] at parallelize at &lt;console&gt;:24

scala&gt; rdd.subtract(rdd1).collect()
res27: Array[Int] = Array(8, 6, 7)
</code></pre></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>您的支持将鼓励我继续创作！</div> <button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'> <span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"> <img id="wechat_qr" src="/images/wechatpay.png" alt="CoderF 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"> <img id="alipay_qr" src="/images/alipay.png" alt="CoderF 支付宝"><p>支付宝</p></div></div></div></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/02/08/spark-3-spark-rdd.html" rel="next" title="Spark：3、Spark RDD"><i class="fa fa-chevron-left"></i> Spark：3、Spark RDD</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/2018/02/10/spark-5-spark-rdd-program-action.html" rel="prev" title="Spark：5、Spark RDD 编程（Action）">Spark：5、Spark RDD 编程（Action）<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div class="bdsharebuttonbox"><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_more" data-cmd="more"></a><a class="bds_count" data-cmd="count"></a></div><script>window._bd_share_config={common:{bdText:"",bdMini:"2",bdMiniList:!1,bdPic:""},share:{bdSize:"16",bdStyle:"0"},image:{viewList:["tsina","douban","sqq","qzone","weixin","twi","fbook"],viewText:"分享到：",viewSize:"16"}}</script><script>with(document)(getElementsByTagName("head")[0]||body).appendChild(createElement("script")).src="/static/api/js/share.js?cdnversion="+~(-new Date/36e5)</script></div></div></div><div class="comments" id="comments"><div id="vcomments"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="CoderF"><p class="site-author-name" itemprop="name">CoderF</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">69</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">12</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">12</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://www.jianshu.com/u/cf9e217dcf5c" target="_blank" title="简书"><i class="fa fa-fw fa-heartbeat"></i> 简书</a></span><span class="links-of-author-item"><a href="https://github.com/wangchangfei" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://weibo.com/u/5870480318?is_all=1" target="_blank" title="微博"><i class="fa fa-fw fa-weibo"></i> 微博</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#编程模型"><span class="nav-number">1.</span> <span class="nav-text">编程模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建RDD"><span class="nav-number">2.</span> <span class="nav-text">创建RDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD算子操作"><span class="nav-number">3.</span> <span class="nav-text">RDD算子操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#常用算子：Transformation"><span class="nav-number">3.1.</span> <span class="nav-text">常用算子：Transformation</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">CoderF</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">91.1k</span></div><link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.2.0/styles/solarized-light.min.css"><script src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/languages/scala.min.js"></script><script>hljs.initHighlightingOnLoad()</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine@1.1.4/dist/Valine.min.js"></script><script type="text/javascript">new Valine({av:AV,el:"#vcomments",verify:!1,notify:!1,app_id:"x8P6mpIr7TWIoaIwTFLtvUjw-gzGzoHsz",app_key:"PkMk480kBPrRXPvH5GF36VpI",placeholder:"说点什么吧！"})</script><script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("x8P6mpIr7TWIoaIwTFLtvUjw-gzGzoHsz","PkMk480kBPrRXPvH5GF36VpI")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),t.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script></body></html><script type="text/javascript" src="/js/src/love.js"></script>
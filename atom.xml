<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Changfei&#39;s Tech Blog</title>
  
  <subtitle>在纷杂的世界安静的写代码</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.coderfei.com/"/>
  <updated>2018-11-20T02:07:46.755Z</updated>
  <id>http://www.coderfei.com/</id>
  
  <author>
    <name>CoderF</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark：11、Spark SQL 数据源</title>
    <link href="http://www.coderfei.com/2018/02/18/spark-11-spark-sql-datasource.html"/>
    <id>http://www.coderfei.com/2018/02/18/spark-11-spark-sql-datasource.html</id>
    <published>2018-02-18T12:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.755Z</updated>
    
    <content type="html"><![CDATA[<h2 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载/保存方法"></a>通用加载/保存方法</h2><p>1) 手动指定选项</p><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</p><p>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项<code>spark.sql.sources.default</code>，可修改默认数据源格式。</p><pre><code class="scala">scala&gt; val df = spark.read.load(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; df.select(&quot;name&quot;).write.save(&quot;names.parquet&quot;)</code></pre><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（<code>例如：org.apache.spark.sql.parquet</code>），如果数据源格式为内置格式，则只需要指定简称<font color="red">json, parquet, jdbc, orc, libsvm, csv, text</font>来指定数据的格式。</p><p>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><pre><code class="scala">scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;hdfs://hadoop001:9000/people.json&quot;)peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]          scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;hdfs://hadoop001:9000/namesAndAges.parquet&quot;)scala&gt;</code></pre><p>除此之外，可以直接运行SQL在文件上：</p><pre><code class="scala">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&quot;)sqlDF.show()</code></pre><a id="more"></a><p>2) 文件保存选项</p><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><tr><th width="35%," bgcolor="yellow">Scala/Java</th><th width="30%," bgcolor="yellow">Any Language</th>&lt;th width=”35%”, bgcolor=yellow&gt; Meaning </tr><tr><td> SaveMode.ErrorIfExists(default) </td><td> “error”(default) </td> <td> 如果文件存在，则报错 </td></tr><tr><td> SaveMode.Append </td><td> “append” </td><td> 追加 </td></tr><tr><td> SaveMode.Overwrite </td><td> “overwrite” </td><td> 覆写 </td></tr><tr><td> SaveMode.Ignore </td><td> “ignore” </td><td> 数据存在，则忽略 </td></tr></table><h2 id="Parquet文件"><a href="#Parquet文件" class="headerlink" title="Parquet文件"></a>Parquet文件</h2><p>1) Parquet读写</p><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。</p><pre><code class="scala">// Encoders for most common types are automatically provided by importing spark.implicits._import spark.implicits._val peopleDF = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)// DataFrames can be saved as Parquet files, maintaining the schema informationpeopleDF.write.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)// Read in the parquet file created above// Parquet files are self-describing so the schema is preserved// The result of loading a Parquet file is also a DataFrameval parquetFileDF = spark.read.parquet(&quot;hdfs://hadoop001:9000/people.parquet&quot;)// Parquet files can also be used to create a temporary view and then used in SQL statementsparquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;)val namesDF = spark.sql(&quot;SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19&quot;)namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()// +------------+// |       value|// +------------+// |Name: Justin|// +------------+</code></pre><p>2) 解析分区信息</p><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><pre><code>path└── to    └── table        ├── gender=male        │   ├── ...        │   │        │   ├── country=US        │   │   └── data.parquet        │   ├── country=CN        │   │   └── data.parquet        │   └── ...        └── gender=female            ├── ...            │            ├── country=US            │   └── data.parquet            ├── country=CN            │   └── data.parquet            └── ...</code></pre><p>通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下：</p><pre><code>root|-- name: string (nullable = true)|-- age: long (nullable = true)|-- gender: string (nullable = true)|-- country: string (nullable = true)</code></pre><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：</p><p><code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认值为true。</p><p>如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><p>3) Schema合并</p><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0</p><p>开始默认关闭了该功能。可以通过下面两种方式开启该功能：</p><p>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true。</p><p>设置全局SQL选项spark.sql.parquet.mergeSchema为true。</p><pre><code class="scala">// sqlContext from the previous example is used in this example.// This is used to implicitly convert an RDD to a DataFrame.import spark.implicits._// Create a simple DataFrame, stored into a partition directoryval df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)df1.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=1&quot;)// Create another DataFrame in a new partition directory,// adding a new column and dropping an existing columnval df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)df2.write.parquet(&quot;hdfs://hadoop001:9000/data/test_table/key=2&quot;)// Read the partitioned tableval df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;hdfs://hadoop001:9000/data/test_table&quot;)df3.printSchema()// The final schema consists of all 3 columns in the Parquet files together// with the partitioning column appeared in the partition directory paths.// root// |-- single: int (nullable = true)// |-- double: int (nullable = true)// |-- triple: int (nullable = true)// |-- key : int (nullable = true)</code></pre><h2 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h2><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。 </p><font color="red">若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。</font><p>需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><pre><code class="scala">import java.io.Fileimport org.apache.spark.sql.Rowimport org.apache.spark.sql.SparkSessioncase class Record(key: Int, value: String)// warehouseLocation points to the default location for managed databases and tablesval warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePathval spark = SparkSession.builder().appName(&quot;Spark Hive Example&quot;).config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation).enableHiveSupport().getOrCreate()import spark.implicits._import spark.sqlsql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)sql(&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;)// Queries are expressed in HiveQLsql(&quot;SELECT * FROM src&quot;).show()// +---+-------+// |key|  value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Aggregation queries are also supported.sql(&quot;SELECT COUNT(*) FROM src&quot;).show()// +--------+// |count(1)|// +--------+// |    500 |// +--------+// The results of SQL queries are themselves DataFrames and support all normal functions.val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)// The items in DataFrames are of type Row, which allows you to access each column by ordinal.val stringsDS = sqlDF.map {case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;}stringsDS.show()// +--------------------+// |               value|// +--------------------+// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// ...// You can also use DataFrames to create temporary views within a SparkSession.val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))recordsDF.createOrReplaceTempView(&quot;records&quot;)// Queries can then join DataFrame data with data stored in Hive.sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()// +---+------+---+------+// |key| value|key| value|// +---+------+---+------+// |  2| val_2|  2| val_2|// |  4| val_4|  4| val_4|// |  5| val_5|  5| val_5|// ...</code></pre><p>1) 内嵌Hive应用</p><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf : </p><pre><code>spark.sql.warehouse.dir=</code></pre><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。</p><p>2) 外部Hive应用</p><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><p>a  将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</p><p>b  打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</p><pre><code class="shell">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</code></pre><h2 id="JSON数据集"><a href="#JSON数据集" class="headerlink" title="JSON数据集"></a>JSON数据集</h2><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><pre><code class="json">{&quot;name&quot;:&quot;Michael&quot;}{&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30}{&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19}</code></pre><pre><code class="scala">// Primitive types (Int, String, etc) and Product types (case classes) encoders are// supported by importing this when creating a Dataset.import spark.implicits._// A JSON dataset is pointed to by path.// The path can be either a single text file or a directory storing text filesval path = &quot;examples/src/main/resources/people.json&quot;val peopleDF = spark.read.json(path)// The inferred schema can be visualized using the printSchema() methodpeopleDF.printSchema()// root//  |-- age: long (nullable = true)//  |-- name: string (nullable = true)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView(&quot;people&quot;)// SQL statements can be run by using the sql methods provided by sparkval teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)teenagerNamesDF.show()// +------+// |  name|// +------+// |Justin|// +------+// Alternatively, a DataFrame can be created for a JSON dataset represented by// a Dataset[String] storing one JSON object per stringval otherPeopleDataset = spark.createDataset(&quot;&quot;&quot;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot; :: Nil)val otherPeople = spark.read.json(otherPeopleDataset)otherPeople.show()// +---------------+----+// |        address|name|// +---------------+----+// |[Columbus,Ohio]| Yin|// +---------------+----+</code></pre><h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p><p>注意，需要将相关的数据库驱动放到spark的类路径下。</p><pre><code class="shell">$ bin/spark-shell --master spark://hadoop001:7077 --jars mysql-connector-java-5.1.27-bin.jar</code></pre><pre><code class="scala">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods// Loading data from a JDBC sourceval jdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot; rddtable&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).load()val connectionProperties = new Properties()connectionProperties.put(&quot;user&quot;, &quot;root&quot;)connectionProperties.put(&quot;password&quot;, &quot;hive&quot;)val jdbcDF2 = spark.read.jdbc(&quot;jdbc:mysql://hadoop001:3306/rdd&quot;, &quot;rddtable&quot;, connectionProperties)// Saving data to a JDBC sourcejdbcDF.write.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://hadoop001:3306/rdd&quot;).option(&quot;dbtable&quot;, &quot;rddtable2&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;hive&quot;).save()jdbcDF2.write.jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)// Specifying create table column data types on writejdbcDF.write.option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;).jdbc(&quot;jdbc:mysql://hadoop001:3306/mysql&quot;, &quot;db&quot;, connectionProperties)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;通用加载-保存方法&quot;&gt;&lt;a href=&quot;#通用加载-保存方法&quot; class=&quot;headerlink&quot; title=&quot;通用加载/保存方法&quot;&gt;&lt;/a&gt;通用加载/保存方法&lt;/h2&gt;&lt;p&gt;1) 手动指定选项&lt;/p&gt;
&lt;p&gt;Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。&lt;/p&gt;
&lt;p&gt;Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项&lt;code&gt;spark.sql.sources.default&lt;/code&gt;，可修改默认数据源格式。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;scala&amp;gt; val df = spark.read.load(&amp;quot;hdfs://hadoop001:9000/namesAndAges.parquet&amp;quot;)
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala&amp;gt; df.select(&amp;quot;name&amp;quot;).write.save(&amp;quot;names.parquet&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（&lt;code&gt;例如：org.apache.spark.sql.parquet&lt;/code&gt;），如果数据源格式为内置格式，则只需要指定简称&lt;font color=&quot;red&quot;&gt;json, parquet, jdbc, orc, libsvm, csv, text&lt;/font&gt;来指定数据的格式。&lt;/p&gt;
&lt;p&gt;可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;scala&amp;gt; val peopleDF = spark.read.format(&amp;quot;json&amp;quot;).load(&amp;quot;hdfs://hadoop001:9000/people.json&amp;quot;)
peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]          

scala&amp;gt; peopleDF.write.format(&amp;quot;parquet&amp;quot;).save(&amp;quot;hdfs://hadoop001:9000/namesAndAges.parquet&amp;quot;)
scala&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;除此之外，可以直接运行SQL在文件上：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;val sqlDF = spark.sql(&amp;quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/namesAndAges.parquet`&amp;quot;)
sqlDF.show()
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：10、Spark SQL 解析</title>
    <link href="http://www.coderfei.com/2018/02/16/spark-10-spark-sql-analysis.html"/>
    <id>http://www.coderfei.com/2018/02/16/spark-10-spark-sql-analysis.html</id>
    <published>2018-02-16T13:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.754Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SparkSession的概念"><a href="#SparkSession的概念" class="headerlink" title="SparkSession的概念"></a>SparkSession的概念</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><pre><code class="scala">import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName(&quot;Spark SQL basic example&quot;).config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;).getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._</code></pre><p>SparkSession.builder 用于创建一个SparkSession。</p><p>import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。</p><p>如果需要Hive支持，则需要以下创建语句：</p><pre><code class="scala">import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName(&quot;Spark SQL basic example&quot;).config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;).enableHiveSupport().getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._</code></pre><a id="more"></a><h2 id="DataFrames的创建"><a href="#DataFrames的创建" class="headerlink" title="DataFrames的创建"></a>DataFrames的创建</h2><p>在Spark SQL中SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。</p><p>1) 从Spark数据源进行创建：</p><pre><code class="scala">val df = spark.read.json(&quot;examples/src/main/resources/people.json&quot;) // Displays the content of the DataFrame to stdoutdf.show()// +----+-------+// | age|   name|// +----+-------+// |null|Michael|// |  30|   Andy|// |  19| Justin|// +----+-------+</code></pre><p>2)  将RDD转换为DataFrame：</p><pre><code class="scala">/**MichaelAndy, 30Justin, 19**/scala&gt; val peopleRdd = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;)peopleRdd: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[18] at textFile at &lt;console&gt;:24scala&gt; val peopleDF3 = peopleRdd.map(_.split(&quot;,&quot;)).filter(arr =&gt; arr.length == 2).map(paras =&gt; (paras(0),paras(1).trim().toInt)).toDF(&quot;name&quot;,&quot;age&quot;)peopleDF3: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; peopleDF3.show()+-------+---+|   name|age|+-------+---+|Michael| 29||   Andy| 30|| Justin| 19|+-------+---+</code></pre><h2 id="DataFrame的常用操作"><a href="#DataFrame的常用操作" class="headerlink" title="DataFrame的常用操作"></a>DataFrame的常用操作</h2><p>1) DSL风格语法</p><pre><code class="scala">// This import is needed to use the $-notationimport spark.implicits._// Print the schema in a tree formatdf.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Select only the &quot;name&quot; columndf.select(&quot;name&quot;).show()// +-------+// |   name|// +-------+// |Michael|// |Andy|// | Justin|// +-------+// Select everybody, but increment the age by 1df.select($&quot;name&quot;, $&quot;age&quot; + 1).show()// +-------+---------+// | name|(age + 1)|// +-------+---------+// |Michael| null|// | Andy|   31|// | Justin|   20|// +-------+---------+// Select people older than 21df.filter($&quot;age&quot; &gt; 21).show()// +---+----+// |age|name|// +---+----+// | 30|Andy|// +---+----+// Count people by agedf.groupBy(&quot;age&quot;).count().show()// +----+-----+// | age|count|// +----+-----+// |  19|    1|// |null|    1|// |  30|    1|// +----+-----+</code></pre><p>2) SQL风格语法</p><pre><code class="scala">// Register the DataFrame as a SQL temporary viewdf.createOrReplaceTempView(&quot;people&quot;)val sqlDF = spark.sql(&quot;SELECT * FROM people&quot;)sqlDF.show()// +----+-------+// | age|   name|// +----+-------+// |null|Michael|// |  30|   Andy|// |  19| Justin|// +----+-------+// Register the DataFrame as a global temporary viewdf.createGlobalTempView(&quot;people&quot;)// Global temporary view is tied to a system preserved database `global_temp`spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show()// +----+-------+// | age|   name|// +----+-------+// |null|Michael|// |  30|   Andy|// |  19| Justin|// +----+-------+// Global temporary view is cross-sessionspark.newSession().sql(&quot;SELECT * FROM global_temp.people&quot;).show()// +----+-------+// | age|   name|// +----+-------+// |null|Michael|// |  30|   Andy|// |  19| Justin|// +----+-------+</code></pre><p>临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people。</p><h2 id="DataSet的创建"><a href="#DataSet的创建" class="headerlink" title="DataSet的创建"></a>DataSet的创建</h2><p>Dataset是具有强类型的数据集合，需要提供对应的类型信息。</p><pre><code class="scala">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,// you can use custom classes that implement the Product interfacecase class Person(name: String, age: Long)// Encoders are created for case classesval caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS()caseClassDS.show()// +----+---+// |name|age|// +----+---+// |Andy| 32|// +----+---+// Encoders for most common types are automatically provided by importing spark.implicits._val primitiveDS = Seq(1, 2, 3).toDS()primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by nameval path = &quot;hdfs://hadoop001:9000/people.json&quot;val peopleDS = spark.read.json(path).as[Person]peopleDS.show()// +----+-------+// | age|   name|// +----+-------+// |null|Michael|// |  30|   Andy|// |  19| Justin|// +----+-------+</code></pre><h2 id="DataSet与RDD的转换"><a href="#DataSet与RDD的转换" class="headerlink" title="DataSet与RDD的转换"></a>DataSet与RDD的转换</h2><p>Spark SQL支持通过两种方式将存在的RDD转换为Dataset，转换的过程中需要让Dataset获取RDD中的Schema信息，主要有两种方式，一种是通过反射来获取RDD中的Schema信息。这种方式适合于列名已知的情况下。第二种是通过编程接口的方式将Schema信息应用于RDD，这种方式可以处理那种在运行时才能知道列的方式。</p><p>1) 通过反射获取Schema    </p><p>SparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。</p><pre><code class="scala">// For implicit conversions from RDDs to DataFramesimport spark.implicits._// Create an RDD of Person objects from a text file, convert it to a Dataframeval peopleDF = val peopleDF = spark.sparkContext.textFile(&quot;hdfs://hadoop001:9000/people2.txt&quot;).map(_.split(&quot;,&quot;)).filter(arr =&gt; arr.length == 2).map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)).toDF()// Register the DataFrame as a temporary viewpeopleDF.createOrReplaceTempView(&quot;people&quot;)// SQL statements can be run by using the sql methods provided by Sparkval teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)// The columns of a row in the result can be accessed by field index  ROW objectteenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()// +------------+// |       value|// +------------+// |Name: Justin|// +------------+// or by field nameteenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;)).show()// +------------+// |       value|// +------------+// |Name: Justin|// +------------+// No pre-defined encoders for Dataset[Map[K,V]], define explicitlyimplicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]// Primitive types and case classes can be also defined as// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect()// Array(Map(&quot;name&quot; -&gt; &quot;Justin&quot;, &quot;age&quot; -&gt; 19))</code></pre><p>2) 通过编程设置Schema</p><p>如果case类不能够提前定义，可以通过下面三个步骤定义一个DataFrame</p><p>创建一个多行结构的RDD;</p><p>创建用StructType来表示的行结构信息。</p><p>通过SparkSession提供的createDataFrame方法来应用Schema。</p><pre><code class="scala">import org.apache.spark.sql.types._// Create an RDDval peopleRDD = spark.sparkContext.textFile(&quot;hdfs://hadoop001:9000/people2.txt&quot;)// The schema is encoded in a string,应该是动态通过程序生成的val schemaString = &quot;name age&quot;// Generate the schema based on the string of schema   Array[StructFiled]val fields = schemaString.split(&quot; &quot;).map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))val filed = schemaString.split(&quot; &quot;).map(filename=&gt; filename match{ case &quot;name&quot; =&gt; StructField(filename,StringType,nullable = true); case &quot;age&quot;=&gt;StructField(filename, IntegerType,nullable = true)})val schema = StructType(fields)// Convert records of the RDD (people) to Rowsimport org.apache.spark.sql._val rowRDD = peopleRDD.map(_.split(&quot;,&quot;)).filter(arr =&gt; arr.length == 2).map(attributes =&gt; Row(attributes(0), attributes(1).trim))// Apply the schema to the RDDval peopleDF = spark.createDataFrame(rowRDD, schema)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView(&quot;people&quot;)// SQL can be run over a temporary view created using DataFramesval results = spark.sql(&quot;SELECT name FROM people&quot;)// The results of SQL queries are DataFrames and support all the normal RDD operations// The columns of a row in the result can be accessed by field index or by field nameresults.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()// +-------------+// |        value|// +-------------+// |Name: Michael|// |   Name: Andy|// | Name: Justin|// +-------------+</code></pre><h2 id="类型之间的转换总结"><a href="#类型之间的转换总结" class="headerlink" title="类型之间的转换总结"></a>类型之间的转换总结</h2><p>RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换。</p><p>1) DataFrame/Dataset转RDD：</p><pre><code class="scala">val rdd1=testDF.rddval rdd2=testDS.rdd</code></pre><p>2) RDD转DataFrame：</p><pre><code class="scala">import spark.implicits._val testDF = rdd.map {line=&gt;      (line._1,line._2)    }.toDF(&quot;col1&quot;,&quot;col2&quot;)</code></pre><p>一般用元组把一行的数据写在一起，然后在toDF中指定字段名。</p><p>3) RDD转Dataset：</p><pre><code class="scala">import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = rdd.map {line=&gt;      Coltest(line._1,line._2)    }.toDS</code></pre><p>可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可</p><p>4）Dataset转DataFrame：</p><p>这个也很简单，因为只是把case class封装成Row：</p><pre><code class="scala">import spark.implicits._val testDF = testDS.toDFDataFrame转Dataset：import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = testDF.as[Coltest]</code></pre><p>这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。</p><p>在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。</p><h2 id="UDF函数"><a href="#UDF函数" class="headerlink" title="UDF函数"></a>UDF函数</h2><p>1) UDF函数的创建</p><pre><code class="scala">scala&gt; val df = spark.read.json(&quot;hdfs://hadoop001:9000/people.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; df.show()+----+-------+| age|   name|+----+-------+|null|Michael||  30|   Andy||  19| Justin|+----+-------+scala&gt; spark.udf.register(&quot;addName&quot;, (x:String)=&gt; &quot;Name:&quot;+x)res5: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))scala&gt; df.createOrReplaceTempView(&quot;people&quot;)scala&gt; spark.sql(&quot;Select addName(name), age from people&quot;).show()+-----------------+----+|UDF:addName(name)| age|+-----------------+----+|     Name:Michael|null||        Name:Andy|  30||      Name:Justin|  19|+-----------------+----+</code></pre><p>2) UDAF函数的创建</p><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。</p><p>1) 弱类型用户自定义聚合函数</p><p>通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。</p><pre><code class="scala">import org.apache.spark.sql.expressions.MutableAggregationBufferimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.types._import org.apache.spark.sql.Rowimport org.apache.spark.sql.SparkSessionobject MyAverage extends UserDefinedAggregateFunction {// 聚合函数输入参数的数据类型 def inputSchema: StructType = StructType(StructField(&quot;inputColumn&quot;, LongType) :: Nil)// 聚合缓冲区中值得数据类型 def bufferSchema: StructType = {StructType(StructField(&quot;sum&quot;, LongType) :: StructField(&quot;count&quot;, LongType) :: Nil)}// 返回值的数据类型 def dataType: DataType = DoubleType// 对于相同的输入是否一直返回相同的输出。 def deterministic: Boolean = true// 初始化def initialize(buffer: MutableAggregationBuffer): Unit = {// 存工资的总额buffer(0) = 0L// 存工资的个数buffer(1) = 0L}// 相同Execute间的数据合并。 def update(buffer: MutableAggregationBuffer, input: Row): Unit = {if (!input.isNullAt(0)) {buffer(0) = buffer.getLong(0) + input.getLong(0)buffer(1) = buffer.getLong(1) + 1}}// 不同Execute间的数据合并 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)}// 计算最终结果def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)}// 注册函数spark.udf.register(&quot;myAverage&quot;, MyAverage)val df = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;)df.createOrReplaceTempView(&quot;employees&quot;)df.show()// +-------+------+// |   name|salary|// +-------+------+// |Michael|  3000|// |   Andy|  4500|// | Justin|  3500|// |  Berta|  4000|// +-------+------+val result = spark.sql(&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;)result.show()// +--------------+// |average_salary|// +--------------+// |        3750.0|// +--------------+</code></pre><p>2) 强类型用户自定义函数</p><p>通过继承Aggregator来实现强类型自定义聚合函数，同样是求平均工资。</p><pre><code class="scala">import org.apache.spark.sql.expressions.Aggregatorimport org.apache.spark.sql.Encoderimport org.apache.spark.sql.Encodersimport org.apache.spark.sql.SparkSession// 既然是强类型，可能有case类case class Employee(name: String, salary: Long)case class Average(var sum: Long, var count: Long)object MyAverage extends Aggregator[Employee, Average, Double] {// 定义一个数据结构，保存工资总数和工资总个数，初始都为0def zero: Average = Average(0L, 0L)// Combine two values to produce a new value. For performance, the function may modify `buffer`// and return it instead of constructing a new objectdef reduce(buffer: Average, employee: Employee): Average = {buffer.sum += employee.salarybuffer.count += 1buffer}// 聚合不同execute的结果def merge(b1: Average, b2: Average): Average = {b1.sum += b2.sumb1.count += b2.countb1}// 计算输出def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count// 设定之间值类型的编码器，要转换成case类// Encoders.product是进行scala元组和case类转换的编码器 def bufferEncoder: Encoder[Average] = Encoders.product// 设定最终输出值的编码器def outputEncoder: Encoder[Double] = Encoders.scalaDouble}import spark.implicits._val ds = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;).as[Employee]ds.show()// +-------+------+// |   name|salary|// +-------+------+// |Michael|  3000|// |   Andy|  4500|// | Justin|  3500|// |  Berta|  4000|// +-------+------+// Convert the function to a `TypedColumn` and give it a nameval averageSalary = MyAverage.toColumn.name(&quot;average_salary&quot;)val result = ds.select(averageSalary)result.show()// +--------------+// |average_salary|// +--------------+// |        3750.0|// +--------------+</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;SparkSession的概念&quot;&gt;&lt;a href=&quot;#SparkSession的概念&quot; class=&quot;headerlink&quot; title=&quot;SparkSession的概念&quot;&gt;&lt;/a&gt;SparkSession的概念&lt;/h2&gt;&lt;p&gt;在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;import org.apache.spark.sql.SparkSession

val spark = SparkSession
.builder()
.appName(&amp;quot;Spark SQL basic example&amp;quot;)
.config(&amp;quot;spark.some.config.option&amp;quot;, &amp;quot;some-value&amp;quot;)
.getOrCreate()

// For implicit conversions like converting RDDs to DataFrames
import spark.implicits._
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SparkSession.builder 用于创建一个SparkSession。&lt;/p&gt;
&lt;p&gt;import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。&lt;/p&gt;
&lt;p&gt;如果需要Hive支持，则需要以下创建语句：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;import org.apache.spark.sql.SparkSession

val spark = SparkSession
.builder()
.appName(&amp;quot;Spark SQL basic example&amp;quot;)
.config(&amp;quot;spark.some.config.option&amp;quot;, &amp;quot;some-value&amp;quot;)
.enableHiveSupport()
.getOrCreate()

// For implicit conversions like converting RDDs to DataFrames
import spark.implicits._
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：9、Spark SQL 使用</title>
    <link href="http://www.coderfei.com/2018/02/15/spark-9-spark-sql-use.html"/>
    <id>http://www.coderfei.com/2018/02/15/spark-9-spark-sql-use.html</id>
    <published>2018-02-15T12:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.810Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark-SQL-查询"><a href="#Spark-SQL-查询" class="headerlink" title="Spark SQL 查询"></a>Spark SQL 查询</h2><h3 id="命令行查询"><a href="#命令行查询" class="headerlink" title="命令行查询"></a>命令行查询</h3><p>打开Spark shell</p><p>创建如下JSON文件，注意JSON的格式：</p><pre><code class="json">{&quot;name&quot;:&quot;Michael&quot;}{&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30}{&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19}</code></pre><pre><code class="scala">scala&gt; import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.SparkSessionscala&gt; val spark = SparkSession.builder().config(sparkconf).getOrCreate()18/05/11 11:23:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@128d397cscala&gt; val rdd=spark.sparkContext.parallelize(Seq((&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)))rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:29scala&gt; import spark.implicits._import spark.implicits._scala&gt; val df = spark.read.json(&quot;hdfs://hadoop001:9000/people.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]                scala&gt; df.show+----+-------+| age|   name|+----+-------+|null|Michael||  30|   Andy||  19| Justin|+----+-------+scala&gt; df.filter($&quot;age&quot; &gt; 21).show+---+----+|age|name|+---+----+| 30|Andy|+---+----+scala&gt; df.createOrReplaceTempView(&quot;persons&quot;)scala&gt; spark.sql(&quot;select * from persons&quot;)res3: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; spark.sql(&quot;select * from persons&quot;).show+----+-------+| age|   name|+----+-------+|null|Michael||  30|   Andy||  19| Justin|+----+-------+scala&gt; spark.sql(&quot;select * from persons where age &gt; 21&quot;).show+---+----+|age|name|+---+----+| 30|Andy|+---+----+scala&gt;</code></pre><a id="more"></a><h3 id="IDEA创建SparkSQL程序"><a href="#IDEA创建SparkSQL程序" class="headerlink" title="IDEA创建SparkSQL程序"></a>IDEA创建SparkSQL程序</h3><p>IDEA中程序的打包和运行方式都和SparkCore类似，Maven依赖中需要添加新的依赖项：</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.coderfei&lt;/groupId&gt;    &lt;artifactId&gt;CoreWordCount&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;            &lt;version&gt;2.11.8&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;            &lt;version&gt;2.1.1&lt;/version&gt;            &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;2.7.2&lt;/version&gt;            &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;            &lt;version&gt;2.1.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;            &lt;version&gt;2.1.1&lt;/version&gt;            &lt;scope&gt;provided&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;                &lt;version&gt;2.15.2&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;scala-compile-first&lt;/id&gt;                        &lt;goals&gt;                            &lt;goal&gt;compile&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;includes&gt;                                &lt;include&gt;**/*.scala&lt;/include&gt;                            &lt;/includes&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                    &lt;execution&gt;                        &lt;id&gt;scala-test-compile&lt;/id&gt;                        &lt;goals&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>代码如下：</p><pre><code class="scala">package mainimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionobject DataFrameSyllabus {  def main(args: Array[String]): Unit = {    //创建SparkConf()并设置App名称    val sparkconf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;test&quot;).set(&quot;spark.port.maxRetries&quot;,&quot;1000&quot;)    val spark = SparkSession.builder().config(sparkconf).getOrCreate()    // For implicit conversions like converting RDDs to DataFrames    import spark.implicits._    val df = spark.read.json(&quot;hdfs://hadoop001:9000/people.json&quot;)    // Displays the content of the DataFrame to stdout    df.show()    df.filter($&quot;age&quot; &gt; 21).show()    df.createOrReplaceTempView(&quot;persons&quot;)    spark.sql(&quot;SELECT * FROM persons where age &gt; 21&quot;).show()    spark.stop()  }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Spark-SQL-查询&quot;&gt;&lt;a href=&quot;#Spark-SQL-查询&quot; class=&quot;headerlink&quot; title=&quot;Spark SQL 查询&quot;&gt;&lt;/a&gt;Spark SQL 查询&lt;/h2&gt;&lt;h3 id=&quot;命令行查询&quot;&gt;&lt;a href=&quot;#命令行查询&quot; class=&quot;headerlink&quot; title=&quot;命令行查询&quot;&gt;&lt;/a&gt;命令行查询&lt;/h3&gt;&lt;p&gt;打开Spark shell&lt;/p&gt;
&lt;p&gt;创建如下JSON文件，注意JSON的格式：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;json&quot;&gt;{&amp;quot;name&amp;quot;:&amp;quot;Michael&amp;quot;}
{&amp;quot;name&amp;quot;:&amp;quot;Andy&amp;quot;, &amp;quot;age&amp;quot;:30}
{&amp;quot;name&amp;quot;:&amp;quot;Justin&amp;quot;, &amp;quot;age&amp;quot;:19}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;scala&amp;gt; import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

scala&amp;gt; val spark = SparkSession.builder().config(sparkconf).getOrCreate()
18/05/11 11:23:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@128d397c

scala&amp;gt; val rdd=spark.sparkContext.parallelize(Seq((&amp;quot;a&amp;quot;, 1), (&amp;quot;b&amp;quot;, 1), (&amp;quot;a&amp;quot;, 1)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at &amp;lt;console&amp;gt;:29

scala&amp;gt; import spark.implicits._
import spark.implicits._

scala&amp;gt; val df = spark.read.json(&amp;quot;hdfs://hadoop001:9000/people.json&amp;quot;)
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]                

scala&amp;gt; df.show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala&amp;gt; df.filter($&amp;quot;age&amp;quot; &amp;gt; 21).show
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+


scala&amp;gt; df.createOrReplaceTempView(&amp;quot;persons&amp;quot;)

scala&amp;gt; spark.sql(&amp;quot;select * from persons&amp;quot;)
res3: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala&amp;gt; spark.sql(&amp;quot;select * from persons&amp;quot;).show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala&amp;gt; spark.sql(&amp;quot;select * from persons where age &amp;gt; 21&amp;quot;).show
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

scala&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：8、Spark SQL 概述</title>
    <link href="http://www.coderfei.com/2018/02/13/spark-8-spark-sql-summary.html"/>
    <id>http://www.coderfei.com/2018/02/13/spark-8-spark-sql-summary.html</id>
    <published>2018-02-13T14:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SparkSQL是什么"><a href="#SparkSQL是什么" class="headerlink" title="SparkSQL是什么"></a>SparkSQL是什么</h2><img src="/2018/02/13/spark-8-spark-sql-summary/spark-1.png">    <p>SparkSQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。</p><p>我们已经学习了Hive，它是将HiveSQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有SparkSQL的应运而生，它是将SparkSQL转换成RDD，然后提交到集群执行，执行效率非常快！</p><a id="more"></a><p>1) 易整合</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-2.png">    <p>2) 统一的数据访问方式</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-3.png">    <p>3) 兼容Hive</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-4.png">    <p>4) 标准的数据连接</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-5.png">    <img src="/2018/02/13/spark-8-spark-sql-summary/spark-6.png">    <p>SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。</p><h2 id="RDD、DataFrames、DataSet"><a href="#RDD、DataFrames、DataSet" class="headerlink" title="RDD、DataFrames、DataSet"></a>RDD、DataFrames、DataSet</h2><img src="/2018/02/13/spark-8-spark-sql-summary/spark-7.png">    <p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：</p><p>RDD(Spark1.0)—&gt;Dataframe(Spark1.3)—&gt;Dataset(Spark1.6)</p><p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。</p><p>在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。</p><h3 id="三者的详细说明"><a href="#三者的详细说明" class="headerlink" title="三者的详细说明"></a>三者的详细说明</h3><p>1) RDD</p><p>a  RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。</p><p>b  RDD的最大好处就是简单，API的人性化程度很高。</p><p>c  RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。</p><p>RDD 例子如下:</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-8.png">    <p>2) DataFame</p><p>与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrameAPI提供的是一套高层的关系操作，比函数式的RDDAPI要更加友好，门槛更低。</p><p>由于与R和Pandas的DataFrame类似，SparkDataFrame很好地继承了传统单机数据分析的开发体验。</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-9.png">    <p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得SparkSQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。<br>DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。</p><p>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待</p><p>DataFrame也是懒执行的。</p><p>性能上比RDD要高，主要有两方面原因：</p><p>定制化内存管理：</p><p>数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-10.png">    <p>优化的执行计划：</p><p>查询计划通过Spark catalyst optimiser进行优化。</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-11.png">    <p>比如这个例子：</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-12.png">    <p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而SparkSQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p><p>得到的优化执行计划在转换成物理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。</p><p>对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。</p><p>Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错。</p><p>3)  DataSet</p><p>1)  是DataframeAPI的一个扩展，是Spark最新的数据抽象</p><p>2)  用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</p><p>3)  Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</p><p>4)  样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。</p><p>5)  Dataframe是Dataset的特列，DataFrame=Dataset[Row]，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。</p><p>6)  DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].<br>DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-13.png">    <p>RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。</p><img src="/2018/02/13/spark-8-spark-sql-summary/spark-14.png">    <h2 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h2><p>1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</p><p>2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。</p><pre><code class="scala">import org.apache.spark.sql.SparkSessionimport org.apache.spark.{SparkConf, SparkContext}val sparkconf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;test&quot;).set(&quot;spark.port.maxRetries&quot;,&quot;1000&quot;)val spark = SparkSession.builder().config(sparkconf).getOrCreate()val rdd=spark.sparkContext.parallelize(Seq((&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)))// map不运行rdd.map{line=&gt;  println(&quot;运行&quot;)  line._1}</code></pre><p>3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p><p>4、三者都有partition的概念</p><p>5、三者有许多共同的函数，如filter，排序等</p><p>6、在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。</p><pre><code class="scala">import spark.implicits._</code></pre><p>7、DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型</p><p>DataFrame:</p><pre><code class="scala">testDF.map{      case Row(col1:String,col2:Int)=&gt;        println(col1);println(col2)        col1      case _=&gt;        &quot;&quot;   }</code></pre><p>Dataset:</p><pre><code class="scala">case class Coltest(col1:String,col2:Int)extends Serializable     testDS.map{      case Coltest(col1:String,col2:Int)=&gt;        println(col1);println(col2)        col1      case _=&gt;        &quot;&quot;   }</code></pre><h2 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h2><p>RDD:</p><p>1、RDD一般和sparkmlib同时使用</p><p>2、RDD不支持sparksql操作</p><p>DataFrame:</p><p>1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如：</p><pre><code class="scala">testDF.foreach{  line =&gt;    val col1=line.getAs[String](&quot;col1&quot;)    val col2=line.getAs[String](&quot;col2&quot;)}</code></pre><p>每一列的值没法直接访问</p><p>2、DataFrame与Dataset一般不与spark ml同时使用</p><p>3、DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如：</p><pre><code class="scala">dataDF.createOrReplaceTempView(&quot;tmp&quot;)spark.sql(&quot;select  ROW,DATE from tmp where DATE is not null order by DATE&quot;).show(100,false)</code></pre><p>4、DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然</p><pre><code class="scala">//保存val saveoptions = Map(&quot;header&quot; -&gt; &quot;true&quot;, &quot;delimiter&quot; -&gt; &quot;\t&quot;, &quot;path&quot; -&gt; &quot;hdfs://master01:9000/test&quot;)datawDF.write.format(&quot;com.coderf.spark.csv&quot;).mode(SaveMode.Overwrite).options(saveoptions).save()//读取val options = Map(&quot;header&quot; -&gt; &quot;true&quot;, &quot;delimiter&quot; -&gt; &quot;\t&quot;, &quot;path&quot; -&gt; &quot;hdfs://master01:9000/test&quot;)val datarDF= spark.read.options(options).format(&quot;com.coderf.spark.csv&quot;).load()</code></pre><p>利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。</p><p>Dataset:</p><p>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。</p><p>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段</p><p>而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。</p><pre><code class="scala">case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型/** rdd (&quot;a&quot;, 1) (&quot;b&quot;, 1) (&quot;a&quot;, 1)**/val test: Dataset[Coltest]=rdd.map{line=&gt;      Coltest(line._1,line._2)    }.toDStest.map{      line=&gt;        println(line.col1)        println(line.col2)    }</code></pre><p>可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;SparkSQL是什么&quot;&gt;&lt;a href=&quot;#SparkSQL是什么&quot; class=&quot;headerlink&quot; title=&quot;SparkSQL是什么&quot;&gt;&lt;/a&gt;SparkSQL是什么&lt;/h2&gt;&lt;img src=&quot;/2018/02/13/spark-8-spark-sql-summary/spark-1.png&quot;&gt;    
&lt;p&gt;SparkSQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。&lt;/p&gt;
&lt;p&gt;我们已经学习了Hive，它是将HiveSQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有SparkSQL的应运而生，它是将SparkSQL转换成RDD，然后提交到集群执行，执行效率非常快！&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：7、Spark RDD依赖与DAG</title>
    <link href="http://www.coderfei.com/2018/02/12/spark-7-spark-rdd-dependency-dag.html"/>
    <id>http://www.coderfei.com/2018/02/12/spark-7-spark-rdd-dependency-dag.html</id>
    <published>2018-02-12T13:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RDD的依赖关系"><a href="#RDD的依赖关系" class="headerlink" title="RDD的依赖关系"></a>RDD的依赖关系</h2><p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p><img src="/2018/02/12/spark-7-spark-rdd-dependency-dag/spark-1.png">    <p>1) 窄依赖</p><p>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用</p><p>2) 宽依赖</p><p>宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle</p><p>3) Lineage</p><p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><img src="/2018/02/12/spark-7-spark-rdd-dependency-dag/spark-2.png">    <a id="more"></a><p>下面例子说明了“窄”，“宽”依赖的不同现象：</p><pre><code class="scala">scala&gt; val text = sc.textFile(&quot;README.md&quot;)text: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; val words = text.flatMap(_.split(&quot; &quot;))words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at &lt;console&gt;:26scala&gt; words.map((_,1))res0: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:29scala&gt; res0.reduceByKey(_+_)res1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:31scala&gt; res1.dependenciesres2: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@6cfe48a4)scala&gt; res0.dependenciesres3: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@6c9e24c4)</code></pre><h2 id="DAG的生成"><a href="#DAG的生成" class="headerlink" title="DAG的生成"></a>DAG的生成</h2><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。</p><img src="/2018/02/12/spark-7-spark-rdd-dependency-dag/spark-3.png">    <h2 id="RDD的周边对应关系"><a href="#RDD的周边对应关系" class="headerlink" title="RDD的周边对应关系"></a>RDD的周边对应关系</h2><img src="/2018/02/12/spark-7-spark-rdd-dependency-dag/spark-4.png">    <p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。<br>当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。</p><p>随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。</p><p>随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p><p>1)  每个节点可以起一个或多个Executor。</p><p>2)  每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</p><p>3)  每个Task执行的结果就是生成了目标RDD的一个partiton。</p><p>注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。</p><p>而 Task被执行的并发度 = Executor数目 * 每个Executor核数。</p><p>至于partition的数目：</p><p>1)  对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</p><p>2)  在Map阶段partition数目保持不变。</p><p>3)  在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。<br>RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。<br>申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。</p><p>比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。<br>如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。</p><p>如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;RDD的依赖关系&quot;&gt;&lt;a href=&quot;#RDD的依赖关系&quot; class=&quot;headerlink&quot; title=&quot;RDD的依赖关系&quot;&gt;&lt;/a&gt;RDD的依赖关系&lt;/h2&gt;&lt;p&gt;RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。&lt;/p&gt;
&lt;img src=&quot;/2018/02/12/spark-7-spark-rdd-dependency-dag/spark-1.png&quot;&gt;    
&lt;p&gt;1) 窄依赖&lt;/p&gt;
&lt;p&gt;窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用&lt;/p&gt;
&lt;p&gt;2) 宽依赖&lt;/p&gt;
&lt;p&gt;宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle&lt;/p&gt;
&lt;p&gt;3) Lineage&lt;/p&gt;
&lt;p&gt;RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。&lt;/p&gt;
&lt;img src=&quot;/2018/02/12/spark-7-spark-rdd-dependency-dag/spark-2.png&quot;&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：6、Spark RDD持久化与检查点机制</title>
    <link href="http://www.coderfei.com/2018/02/11/spark-6-spark-rdd-cache-checkpoint.html"/>
    <id>http://www.coderfei.com/2018/02/11/spark-6-spark-rdd-cache-checkpoint.html</id>
    <published>2018-02-11T13:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.793Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h2><p>1)  RDD的缓存</p><p>Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。</p><p>2)  RDD缓存方式</p><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。<br>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><img src="/2018/02/11/spark-6-spark-rdd-cache-checkpoint/spark-1.png">    <a id="more"></a><p>通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。</p><pre><code class="scala">scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at makeRDD at &lt;console&gt;:25scala&gt; val nocache = rdd.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;)nocache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at map at &lt;console&gt;:27scala&gt; val cache = rdd.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;)cache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at map at &lt;console&gt;:27scala&gt; cache.cacheres24: cache.type = MapPartitionsRDD[21] at map at &lt;console&gt;:27scala&gt; nocache.collectres25: Array[String] = Array(1[1505479375155], 2[1505479374674], 3[1505479374674], 4[1505479375153], 5[1505479375153], 6[1505479374675], 7[1505479375154], 8[1505479375154], 9[1505479374676], 10[1505479374676])scala&gt; nocache.collectres26: Array[String] = Array(1[1505479375679], 2[1505479376157], 3[1505479376157], 4[1505479375680], 5[1505479375680], 6[1505479376159], 7[1505479375680], 8[1505479375680], 9[1505479376158], 10[1505479376158])scala&gt; nocache.collectres27: Array[String] = Array(1[1505479376743], 2[1505479377218], 3[1505479377218], 4[1505479376745], 5[1505479376745], 6[1505479377219], 7[1505479376747], 8[1505479376747], 9[1505479377218], 10[1505479377218])scala&gt; cache.collectres28: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.collectres29: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.collectres30: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])cache.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)</code></pre><img src="/2018/02/11/spark-6-spark-rdd-cache-checkpoint/spark-2.png">    <p>在存储级别的末尾加上“_2”来把持久化数据存为两份。</p><img src="/2018/02/11/spark-6-spark-rdd-cache-checkpoint/spark-3.png">    <p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>注意：使用 Tachyon可以实现堆外缓存。</p><h2 id="RDD检查点机制"><a href="#RDD检查点机制" class="headerlink" title="RDD检查点机制"></a>RDD检查点机制</h2><p>Spark中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。</p><p>cache 和 checkpoint 是有显著区别的， 缓存把 RDD 计算出来然后放在内存中，但是RDD 的依赖链（相当于数据库中的redo 日志），也不能丢掉，当某个点某个 executor 宕了，上面cache 的RDD就会丢掉，需要通过依赖链重放计算出来，不同的是，checkpoint是把 RDD 保存在 HDFS中， 是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链， 是通过复制实现的高容错。</p><p>如果存在以下场景，则比较适合使用检查点机制：</p><p>1) DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</p><p>2) 在宽依赖上做Checkpoint获得的收益更大。</p><p>为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p><pre><code class="scala">scala&gt; val data = sc.parallelize(1 to 100 , 5)data: org.apache.spark.rdd.RDD[Int] =ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:12scala&gt; sc.setCheckpointDir(&quot;hdfs://hadoop001:9000/checkpoint&quot;)scala&gt; data.checkpointscala&gt; data.countscala&gt; val ch1 = sc.parallelize(1 to 2)ch1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:25scala&gt; val ch2 = ch1.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;)ch2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at map at &lt;console&gt;:27scala&gt; val ch3 = ch1.map(_.toString+&quot;[&quot;+System.currentTimeMillis+&quot;]&quot;)ch3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[37] at map at &lt;console&gt;:27scala&gt; ch3.checkpointscala&gt; ch2.collectres62: Array[String] = Array(1[1505480940726], 2[1505480940243])scala&gt; ch2.collectres63: Array[String] = Array(1[1505480941957], 2[1505480941480])scala&gt; ch2.collectres64: Array[String] = Array(1[1505480942736], 2[1505480942257])scala&gt; ch3.collectres65: Array[String] = Array(1[1505480949080], 2[1505480948603])scala&gt; ch3.collectres66: Array[String] = Array(1[1505480948683], 2[1505480949161])scala&gt; ch3.collectres67: Array[String] = Array(1[1505480948683], 2[1505480949161])</code></pre><h3 id="checkpoint写流程"><a href="#checkpoint写流程" class="headerlink" title="checkpoint写流程"></a>checkpoint写流程</h3><p>RDD checkpoint 过程中会经过以下几个状态：</p><p>[ Initialized → marked for checkpointing → checkpointing in progress → checkpointed ]</p><p>转换流程如下：</p><img src="/2018/02/11/spark-6-spark-rdd-cache-checkpoint/spark-4.png">    <p>1)  data.checkpoint 这个函数调用中， 设置的目录中， 所有依赖的 RDD 都会被删除， 函数必须在 job 运行之前调用执行， 强烈建议 RDD 缓存在内存中（又提到一次，千万要注意，即checkpoint()之前，先cache()），否则保存到文件的时候需要从头计算。初始化RDD的 checkpointData 变量为 ReliableRDDCheckpointData。  这时候标记为 Initialized 状态</p><p>2)  在所有 job action 的时候，runJob 方法中都会调用 rdd.doCheckpoint , 这个会向前递归调用所有的依赖的RDD，看看需不需要 checkpoint。如果需要，然后调用  checkpointData.get.checkpoint()，里面标记状态为CheckpointingInProgress，里面调用具体实现类的 ReliableRDDCheckpointData的doCheckpoint 方法。</p><p>3)  doCheckpoint -&gt; writeRDDToCheckpointDirectory， 注意这里会把 job 再运行一次， 如果已经cache 了，就可以直接使用缓存中的 RDD 了， 就不需要重头计算一遍了（怎么又说了一遍），这时候直接把RDD，输出到 hdfs，每个分区一个文件，会先写到一个临时文件，如果全部输出完，进行rename，如果输出失败，就回滚delete。</p><p>4)  标记 状态为 Checkpointed， markCheckpointed方法中清除所有的依赖， 怎么清除依赖的呢， 就是把RDD 变量的强引用设置为 null，垃圾回收了，会触发 ContextCleaner 里面的监听，清除实际 BlockManager 缓存中的数据。</p><h3 id="checkpoint读流程"><a href="#checkpoint读流程" class="headerlink" title="checkpoint读流程"></a>checkpoint读流程</h3><p>如果一个RDD我们已经checkpoint了那么是什么时候用呢，checkpoint将RDD持久化到HDFS或本地文件夹，如果不被手动remove掉，是一直存在的，也就是说可以被下一个driverprogram使用。比如sparkstreaming挂掉了，重启后就可以使用之前checkpoint的数据进行recover,当然在同一个driverprogram也可以使用。我们讲下在同一个driverprogram中是怎么使用checkpoint数据的。</p><p>具体细节如下：</p><p>如果一个RDD被checkpoint了，那么这个RDD中对分区和依赖的处理都是使用的RDD内部的checkpointRDD变量，具体实现是ReliableCheckpointRDD类型。这个是在checkpoint写流程中创建的。依赖和获取分区方法中先判断是否已经checkpoint，如果已经checkpoint了，就斩断依赖，使用ReliableCheckpointRDD，来处理依赖和获取分区。<br>如果没有，才往前回溯依赖。依赖就是没有依赖，因为已经斩断了依赖，获取分区数据就是读取checkpoint到hdfs目录中不同分区保存下来的文件。</p><font color="red">如果在 Scala 中出现了 NotSerializableException，通常问题就在于我们传递了一个不可序列 化的类中的函数或字段。</font>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;RDD的持久化&quot;&gt;&lt;a href=&quot;#RDD的持久化&quot; class=&quot;headerlink&quot; title=&quot;RDD的持久化&quot;&gt;&lt;/a&gt;RDD的持久化&lt;/h2&gt;&lt;p&gt;1)  RDD的缓存&lt;/p&gt;
&lt;p&gt;Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。&lt;/p&gt;
&lt;p&gt;2)  RDD缓存方式&lt;/p&gt;
&lt;p&gt;RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。&lt;br&gt;但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。&lt;/p&gt;
&lt;img src=&quot;/2018/02/11/spark-6-spark-rdd-cache-checkpoint/spark-1.png&quot;&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：5、Spark RDD 编程（Action）</title>
    <link href="http://www.coderfei.com/2018/02/10/spark-5-spark-rdd-program-action.html"/>
    <id>http://www.coderfei.com/2018/02/10/spark-5-spark-rdd-program-action.html</id>
    <published>2018-02-10T14:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常用算子：Action"><a href="#常用算子：Action" class="headerlink" title="常用算子：Action"></a>常用算子：Action</h2><p>1) reduce(func)</p><p>通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。</p><pre><code class="scala">scala&gt; val rdd1 = sc.makeRDD(1 to 10,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.reduce(_+_)res50: Int = 55scala&gt; val rdd2 = sc.makeRDD(Array((&quot;a&quot;,1),(&quot;a&quot;,3),(&quot;c&quot;,3),(&quot;d&quot;,5)))rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[86] at makeRDD at &lt;console&gt;:24scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))res51: (String, Int) = (adca,12)]</code></pre><p>2) collect()</p><p>在驱动程序中，以数组的形式返回数据集的所有元素。</p><img src="/2018/02/10/spark-5-spark-rdd-program-action/spark-23.png">    <pre><code class="scala">scala&gt; var rdd1 = sc.makeRDD(1 to 10, 2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.collect()res0: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</code></pre><a id="more"></a><p>3) count()</p><p>返回RDD的元素个数。</p><pre><code class="scala">scala&gt; var rdd2 = sc.makeRDD(1 to 10, 2)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:24scala&gt; rdd2.count()res1: Long = 10</code></pre><p>4) first()</p><p>返回RDD的第一个元素(类似于take(1))。</p><pre><code class="scala">scala&gt; var rdd3 = sc.makeRDD(1 to 10, 2)rdd3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at makeRDD at &lt;console&gt;:24scala&gt; rdd3.first()res2: Int = 1</code></pre><p>5) take(n)</p><p>返回一个由数据集的前n个元素组成的数组。</p><pre><code class="scala">scala&gt; var rdd4 = sc.makeRDD(1 to 10, 2)rdd4: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at makeRDD at &lt;console&gt;:24scala&gt; rdd4.take(5)res3: Array[Int] = Array(1, 2, 3, 4, 5)</code></pre><p>6) takeSample(withReplacement,num, [seed])</p><p>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。</p><pre><code class="scala">scala&gt; var rdd5 = sc.parallelize(1 to 10, 2)rdd5: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at &lt;console&gt;:24scala&gt; rdd5.collectres4: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; rdd5.takeSample(true, 5, 3)res5: Array[Int] = Array(3, 5, 5, 9, 7)</code></pre><p>7) takeOrdered(n)</p><p>返回前几个的排序。</p><pre><code class="scala">scala&gt; val rdd6 = sc.makeRDD(Seq(10, 4, 2, 12, 3))rdd6: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at makeRDD at &lt;console&gt;:24scala&gt; rdd6.top(2)res6: Array[Int] = Array(12, 10)scala&gt; rdd6.takeOrdered(2)res7: Array[Int] = Array(2, 3)scala&gt; rdd6.takeOrdered(4)res8: Array[Int] = Array(2, 3, 4, 10)</code></pre><p>8) aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</p><p>aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</p><img src="/2018/02/10/spark-5-spark-rdd-program-action/spark-24.png">    <pre><code class="scala">scala&gt; var rdd7 = sc.makeRDD(1 to 10, 2)rdd7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at makeRDD at &lt;console&gt;:24scala&gt; rdd7.aggregate(1)(     | {(x: Int, y: Int) =&gt; x + y},     | {(a: Int, b: Int) =&gt; a + b})res9: Int = 58scala&gt; rdd7.aggregate(1)(     | {(x: Int, y: Int) =&gt; x * y},     | {(a: Int, b: Int) =&gt; a + b})res10: Int = 30361</code></pre><p>9) fold(num)(func)</p><p>折叠操作，aggregate的简化操作，seqop和combop一样。</p><pre><code class="scala">scala&gt; var rdd8 = sc.makeRDD(1 to 4,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[90] at makeRDD at &lt;console&gt;:24scala&gt; rdd8.aggregate(1)(     | {(x : Int,y : Int) =&gt; x + y},     | {(a : Int,b : Int) =&gt; a + b}     | )res59: Int = 13scala&gt; rdd8.fold(1)(_+_)res60: Int = 13</code></pre><p>10) saveAsTextFile(path)</p><p>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。</p><pre><code class="scala">scala&gt; val rdd8 = sc.parallelize(1 to 10, 2)rdd8: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at parallelize at &lt;console&gt;:24scala&gt; rdd8.saveAssaveAsObjectFile   saveAsTextFilescala&gt; rdd8.saveAsTextFile(&quot;hdfs://hadoop001:9000/my_rdd&quot;)</code></pre><p>11) saveAsObjectFile(path)</p><p>用于将RDD中的元素序列化成对象，存储到文件中。</p><pre><code class="scala">scala&gt; val rdd10 = sc.parallelize(List((1, 3), (1, 2), (1, 4), (2, 3), (3, 6), (3, 8)))rdd10: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; rdd10.saveAsObjectFile(&quot;hdfs://hadoop001:9000/my_obj&quot;)</code></pre><p>12)  countByKey()</p><p>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at parallelize at &lt;console&gt;:24scala&gt; rdd.countByKey()res63: scala.collection.Map[Int,Long] = Map(3 -&gt; 2, 1 -&gt; 3, 2 -&gt; 1)</code></pre><p>13) foreach(func)</p><p>在数据集的每一个元素上，运行函数func进行更新。</p><img src="/2018/02/10/spark-5-spark-rdd-program-action/spark-25.png">    <pre><code class="scala">scala&gt; var rdd = sc.makeRDD(1 to 10,2)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[107] at makeRDD at &lt;console&gt;:24scala&gt; var sum = sc.accumulator(0)warning: there were two deprecation warnings; re-run with -deprecation for detailssum: org.apache.spark.Accumulator[Int] = 0scala&gt; rdd.foreach(sum+=_)scala&gt; sum.valueres68: Int = 55scala&gt; rdd.collect().foreach(println)12345678910</code></pre><h2 id="RDD中数值元素的统计操作"><a href="#RDD中数值元素的统计操作" class="headerlink" title="RDD中数值元素的统计操作"></a>RDD中数值元素的统计操作</h2><p>Spark 对包含数值数据的 RDD 提供了一些描述性的统计操作。Spark 的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用 stats() 时通过一次遍历数据计算出来，并以StatsCounter对象返回。</p><img src="/2018/02/10/spark-5-spark-rdd-program-action/spark-26.png">    <pre><code class="scala">scala&gt; var rdd1 = sc.makeRDD(1 to 100)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[42] at makeRDD at &lt;console&gt;:32scala&gt; rdd1.sum()res34: Double = 5050.0scala&gt; rdd1.max()res35: Int = 100</code></pre><h2 id="RDD算子中接受的函数注意事项"><a href="#RDD算子中接受的函数注意事项" class="headerlink" title="RDD算子中接受的函数注意事项"></a>RDD算子中接受的函数注意事项</h2><p>Spark 的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。 在 Scala 中，我们可以把定义的内联函数、方法的引用或静态方法传递给 Spark，就像 Scala 的其他函数式 API 一样。我们还要考虑其他一些细节，比如所传递的函数及其引用 的数据需要是可序列化的(实现了 Java 的 Serializable 接口)。 传递一个对象的方法或者字段时，会包含对整个对象的引用。</p><pre><code class="scala">class SearchFunctions(val query: String) extends java.io.Serializable{  def isMatch(s: String): Boolean = {    s.contains(query)  }  def getMatchesFunctionReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = {    // 问题:&quot;isMatch&quot;表示&quot;this.isMatch&quot;，因此我们要传递整个&quot;this&quot;     rdd.filter(isMatch)  }  def getMatchesFieldReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = { // 问题:&quot;query&quot;表示&quot;this.query&quot;，因此我们要传递整个&quot;this&quot; rdd.filter(x =&gt; x.contains(query))   }  def getMatchesNoReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = {    // 安全:只把我们需要的字段拿出来放入局部变量中 val query_ = this.query    rdd.filter(x =&gt; x.contains(query_))  } }</code></pre><font color="red">如果在 Scala 中出现了 NotSerializableException，通常问题就在于我们传递了一个不可序列 化的类中的函数或字段。</font><h2 id="不同RDD类型的转换"><a href="#不同RDD类型的转换" class="headerlink" title="不同RDD类型的转换"></a>不同RDD类型的转换</h2><p>有些函数只能用于特定类型的 RDD，比如 mean() 和 variance() 只能用在数值 RDD 上， 而 join() 只能用在键值对 RDD 上。在 Scala 和 Java 中，这些函数都没有定义在标准的 RDD 类中，所以要访问这些附加功能，必须要确保获得了正确的专用 RDD 类。<br>在 Scala 中，将 RDD 转为有特定函数的 RDD(比如在 RDD[Double] 上进行数值操作)是 由隐式转换来自动处理的。</p><img src="/2018/02/10/spark-5-spark-rdd-program-action/spark-27.png">    ]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;常用算子：Action&quot;&gt;&lt;a href=&quot;#常用算子：Action&quot; class=&quot;headerlink&quot; title=&quot;常用算子：Action&quot;&gt;&lt;/a&gt;常用算子：Action&lt;/h2&gt;&lt;p&gt;1) reduce(func)&lt;/p&gt;
&lt;p&gt;通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;scala&amp;gt; val rdd1 = sc.makeRDD(1 to 10,2)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at makeRDD at &amp;lt;console&amp;gt;:24

scala&amp;gt; rdd1.reduce(_+_)
res50: Int = 55

scala&amp;gt; val rdd2 = sc.makeRDD(Array((&amp;quot;a&amp;quot;,1),(&amp;quot;a&amp;quot;,3),(&amp;quot;c&amp;quot;,3),(&amp;quot;d&amp;quot;,5)))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[86] at makeRDD at &amp;lt;console&amp;gt;:24

scala&amp;gt; rdd2.reduce((x,y)=&amp;gt;(x._1 + y._1,x._2 + y._2))
res51: (String, Int) = (adca,12)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2) collect()&lt;/p&gt;
&lt;p&gt;在驱动程序中，以数组的形式返回数据集的所有元素。&lt;/p&gt;
&lt;img src=&quot;/2018/02/10/spark-5-spark-rdd-program-action/spark-23.png&quot;&gt;    
&lt;pre&gt;&lt;code class=&quot;scala&quot;&gt;scala&amp;gt; var rdd1 = sc.makeRDD(1 to 10, 2)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &amp;lt;console&amp;gt;:24

scala&amp;gt; rdd1.collect()
res0: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：4、Spark RDD 编程（Transformation）</title>
    <link href="http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation.html"/>
    <id>http://www.coderfei.com/2018/02/09/spark-4-spark-rdd-program-transformation.html</id>
    <published>2018-02-09T13:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.771Z</updated>
    
    <content type="html"><![CDATA[<h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。</p><p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-1.png">    <a id="more"></a><h2 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h2><p>在Spark中创建RDD的创建方式大概可以分为三种：</p><p>（1）从集合中创建RDD；</p><p>（2）从外部存储创建RDD；</p><p>（3）从其他RDD创建。</p><p>1)    由一个已经存在的Scala集合创建，集合并行化。</p><p>例如</p><pre><code class="scala">val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))</code></pre><p>而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</p><pre><code class="scala">def parallelize[T: ClassTag](      seq: Seq[T],      numSlices: Int = defaultParallelism): RDD[T]def makeRDD[T: ClassTag](      seq: Seq[T],      numSlices: Int = defaultParallelism): RDD[T]def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T]</code></pre><p>我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p><pre><code class="scala">def makeRDD[T: ClassTag](    seq: Seq[T],    numSlices: Int = defaultParallelism): RDD[T] = withScope {  parallelize(seq, numSlices)}</code></pre><p>我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])].</p><p>Spark文档的说明是：</p><p>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.</p><p>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p><pre><code class="scala">scala&gt; val test1= sc.parallelize(List(1,2,3))test1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21scala&gt; val test2 = sc.makeRDD(List(1,2,3))test2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21scala&gt; val seq = List((1, List(&quot;slave01&quot;)), (2, List(&quot;slave02&quot;)))seq: List[(Int, List[String])] = List((1,List(slave01)), (2,List(slave02)))scala&gt; val test3 = sc.makeRDD(seq)test3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23scala&gt; test3.preferredLocations(test3.partitions(1))res26: Seq[String] = List(slave02)scala&gt; test3.preferredLocations(test3.partitions(0))res27: Seq[String] = List(slave01)scala&gt; test1.preferredLocations(test1.partitions(0))res28: Seq[String] = List()</code></pre><p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下</p><pre><code class="scala">def parallelize[T: ClassTag](    seq: Seq[T],    numSlices: Int = defaultParallelism): RDD[T] = withScope {  assertNotStopped()  new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())}def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {  assertNotStopped()  val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap  new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)}</code></pre><p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><p>2)    由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p><pre><code class="scala">scala&gt; val test = sc.textFile(&quot;hdfs://hadoop001:9000/RELEASE&quot;)test: org.apache.spark.rdd.RDD[String] = hdfs://hadoop001:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</code></pre><h2 id="RDD算子操作"><a href="#RDD算子操作" class="headerlink" title="RDD算子操作"></a>RDD算子操作</h2><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。</p><h3 id="常用算子：Transformation"><a href="#常用算子：Transformation" class="headerlink" title="常用算子：Transformation"></a>常用算子：Transformation</h3><p>1) map(func)</p><p>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成。<br><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-2.png">    </p><pre><code class="scala">scala&gt; var source  = sc.parallelize(1 to 10)source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24scala&gt; source.collect()res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; val mapadd = source.map(_ * 2)mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26scala&gt; mapadd.collect()res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</code></pre><p>2) filter(func)</p><p>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-3.png">    <pre><code class="scala">scala&gt; var sourceFilter = sc.parallelize(Array(&quot;xiaoming&quot;,&quot;xiaojiang&quot;,&quot;xiaohe&quot;,&quot;dazhi&quot;))sourceFilter: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val filter = sourceFilter.filter(_.contains(&quot;xiao&quot;))filter: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at &lt;console&gt;:26scala&gt; sourceFilter.collect()res9: Array[String] = Array(xiaoming, xiaojiang, xiaohe, dazhi)scala&gt; filter.collect()res10: Array[String] = Array(xiaoming, xiaojiang, xiaohe)</code></pre><p>3) flatMap(func)</p><p>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。    </p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-4.png">    <pre><code class="scala">scala&gt; val sourceFlat = sc.parallelize(1 to 5)sourceFlat: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; sourceFlat.collect()res11: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val flatMap = sourceFlat.flatMap(1 to _)flatMap: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26scala&gt; flatMap.collect()res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)</code></pre><p>4) mapPartitions(func)</p><p>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-5.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;),(&quot;lucy&quot;,&quot;female&quot;)))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(iter : Iterator[(String,String)]) : Iterator[String] = {  var woman = List[String]()  while (iter.hasNext){    val next = iter.next()    next match {       case (_,&quot;female&quot;) =&gt; woman = next._1 :: woman       case _ =&gt;    }  }  woman.iterator}// Exiting paste mode, now interpreting.partitionsFun: (iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitions(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at mapPartitions at &lt;console&gt;:28scala&gt; result.collect()res13: Array[String] = Array(kpop, lucy)</code></pre><p>5) mapPartitionsWithIndex(func)</p><p>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-6.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;),(&quot;lucy&quot;,&quot;female&quot;)))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(index : Int, iter : Iterator[(String,String)]) : Iterator[String] = {  var woman = List[String]()  while (iter.hasNext){    val next = iter.next()    next match {       case (_,&quot;female&quot;) =&gt; woman = &quot;[&quot;+index+&quot;]&quot;+next._1 :: woman       case _ =&gt;    }  }  woman.iterator}// Exiting paste mode, now interpreting.partitionsFun: (index: Int, iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitionsWithIndex(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at mapPartitionsWithIndex at &lt;console&gt;:28scala&gt; result.collect()res14: Array[String] = Array([0]kpop, [3]lucy)</code></pre><p>6) sample(withReplacement, fraction, seed)</p><p>以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。</p>    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24scala&gt; rdd.collect()res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; var sample1 = rdd.sample(true,0.4,2)sample1: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[21] at sample at &lt;console&gt;:26scala&gt; sample1.collect()res16: Array[Int] = Array(1, 2, 2, 7, 7, 8, 9)scala&gt; var sample2 = rdd.sample(false,0.2,3)sample2: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[22] at sample at &lt;console&gt;:26scala&gt; sample2.collect()res17: Array[Int] = Array(1, 9)</code></pre><p>7) takeSample</p><p>和Sample的区别是：takeSample返回的是最终的结果集合。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-8.png">    <p>8) union(otherDataset)</p><p>对源RDD和参数RDD求并集后返回一个新的RDD。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-9.png">    <pre><code class="scala">scala&gt; val rdd1 = sc.parallelize(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.union(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[25] at union at &lt;console&gt;:28scala&gt; rdd3.collect()res18: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10)</code></pre><p>9) intersection(otherDataset)</p><p>对源RDD和参数RDD求交集后返回一个新的RDD。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-10.png">    <pre><code class="scala">scala&gt; val rdd1 = sc.parallelize(1 to 7)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.intersection(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at intersection at &lt;console&gt;:28scala&gt; rdd3.collect()[Stage 15:=============================&gt;                       (2 + 2)                      res19: Array[Int] = Array(5, 6, 7)</code></pre><p>10) distinct([numTasks]))</p><p>对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-11.png">    <pre><code class="scala">scala&gt; val distinctRdd = sc.parallelize(List(1,2,1,5,2,9,6,1))distinctRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24scala&gt; val unionRDD = distinctRdd.distinct()unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[37] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()[Stage 16:&gt; (0 + 4) [Stage 16:=============================&gt;                            (2 + 2)                                                                             res20: Array[Int] = Array(1, 9, 5, 6, 2)scala&gt; val unionRDD = distinctRdd.distinct(2)unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()res21: Array[Int] = Array(6, 2, 1, 9, 5)</code></pre><p>11) partitionBy</p><p>对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区，否则会生成ShuffleRDD。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-12.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;),(4,&quot;ddd&quot;)),4)rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres24: Int = 4scala&gt; var rdd2 = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[45] at partitionBy at &lt;console&gt;:26scala&gt; rdd2.partitions.sizeres25: Int = 2</code></pre><p>12) reduceByKey(func, [numTasks])</p><p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-13.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((&quot;female&quot;,1),(&quot;male&quot;,5),(&quot;female&quot;,5),(&quot;male&quot;,2)))rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[46] at parallelize at &lt;console&gt;:24scala&gt; val reduce = rdd.reduceByKey((x,y) =&gt; x+y)reduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[47] at reduceByKey at &lt;console&gt;:26scala&gt; reduce.collect()res29: Array[(String, Int)] = Array((female,6), (male,7))</code></pre><p>13) groupByKey</p><p>groupByKey也是对每个key进行操作，但只生成一个sequence。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-14.png">    <pre><code class="scala">scala&gt; val words = Array(&quot;one&quot;, &quot;two&quot;, &quot;two&quot;, &quot;three&quot;, &quot;three&quot;, &quot;three&quot;)words: Array[String] = Array(one, two, two, three, three, three)scala&gt; val wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1))wordPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at &lt;console&gt;:26scala&gt; val group = wordPairsRDD.groupByKey()group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[5] at groupByKey at &lt;console&gt;:28scala&gt; group.collect()res1: Array[(String, Iterable[Int])] = Array((two,CompactBuffer(1, 1)), (one,CompactBuffer(1)), (three,CompactBuffer(1, 1, 1)))scala&gt; group.map(t =&gt; (t._1, t._2.sum))res2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at &lt;console&gt;:31scala&gt; res2.collect()res3: Array[(String, Int)] = Array((two,2), (one,1), (three,3))scala&gt; val map = group.map(t =&gt; (t._1, t._2.sum))map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at map at &lt;console&gt;:30scala&gt; map.collect()res4: Array[(String, Int)] = Array((two,2), (one,1), (three,3))</code></pre><p>14) combineByKey<a href="createCombiner: V =&gt; C, mergeValue: (C, V" target="_blank" rel="noopener">C</a> =&gt; C, mergeCombiners: (C, C) =&gt; C)</p><p>对相同K，把V合并成一个集合.</p><p>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</p><p>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</p><p>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-15.png">    <pre><code class="scala">scala&gt; val scores = Array((&quot;Fred&quot;, 88), (&quot;Fred&quot;, 95), (&quot;Fred&quot;, 91), (&quot;Wilma&quot;, 93), (&quot;Wilma&quot;, 95), (&quot;Wilma&quot;, 98))scores: Array[(String, Int)] = Array((Fred,88), (Fred,95), (Fred,91), (Wilma,93), (Wilma,95), (Wilma,98))scala&gt; val input = sc.parallelize(scores)input: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26scala&gt; val combine = input.combineByKey(     |     (v)=&gt;(v,1),     |     (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1),     |     (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))combine: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[53] at combineByKey at &lt;console&gt;:28scala&gt; val result = combine.map{     |     case (key,value) =&gt; (key,value._1/value._2.toDouble)}result: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[54] at map at &lt;console&gt;:30scala&gt; result.collect()res33: Array[(String, Double)] = Array((Wilma,95.33333333333333), (Fred,91.33333333333333))</code></pre><p>15) aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U)</p><p>在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。<br>seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</p><p>例如：List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8))，分一个分区，以key为1的分区为例，0先和3比较得3，3在和2比较得3，3在和4比较得4，所以整个key为1的组最终结果为（1，4），同理，key为2的最终结果为（2，3），key为3的为（3，8）.<br>如果分三个分区，前两个是一个分区，中间两个是一个分区，最后两个是一个分区，第一个分区的最终结果为（1，3），第二个分区为（1，4）（2，3），最后一个分区为（3，8），combine后为 (3,8), (1,7), (2,3)</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-16.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)))rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_)agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[13] at aggregateByKey at &lt;console&gt;:26scala&gt; agg.collect()res7: Array[(Int, Int)] = Array((3,8), (1,7), (2,3))scala&gt; agg.partitions.sizeres8: Int = 3scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect()agg: Array[(Int, Int)] = Array((1,4), (3,8), (2,3))</code></pre><p>16) foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]<br>aggregateByKey的简化操作，seqop和combop相同。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)))rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[91] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.foldByKey(0)(_+_)agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[92] at foldByKey at &lt;console&gt;:26scala&gt; agg.collect()res61: Array[(Int, Int)] = Array((3,14), (1,9), (2,3))</code></pre><p>17) sortByKey([ascending], [numTasks])<br>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((3,&quot;aa&quot;),(6,&quot;cc&quot;),(2,&quot;bb&quot;),(1,&quot;dd&quot;)))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24scala&gt; rdd.sortByKey(true).collect()res9: Array[(Int, String)] = Array((1,dd), (2,bb), (3,aa), (6,cc))scala&gt; rdd.sortByKey(false).collect()res10: Array[(Int, String)] = Array((6,cc), (3,aa), (2,bb), (1,dd))</code></pre><p>18) sortBy(func,[ascending], [numTasks])</p><p>与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List(1,2,3,4))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24scala&gt; rdd.sortBy(x =&gt; x).collect()res11: Array[Int] = Array(1, 2, 3, 4)scala&gt; rdd.sortBy(x =&gt; x%3).collect()res12: Array[Int] = Array(3, 4, 1, 2)</code></pre><p>19) join(otherDataset, [numTasks])</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-17.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; rdd.join(rdd1).collect()res13: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6)))</code></pre><p>20) cogroup(otherDataset, [numTasks])</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD。</w></v></p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-18.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24scala&gt; rdd.cogroup(rdd1).collect()res14: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))scala&gt; val rdd2 = sc.parallelize(Array((4,4),(2,5),(3,6)))rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[41] at parallelize at &lt;console&gt;:24scala&gt; rdd.cogroup(rdd2).collect()res15: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))scala&gt; val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24scala&gt; rdd3.cogroup(rdd2).collect()[Stage 36:&gt;                                                         (0 + 0)                                                                             res16: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(d, a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))</code></pre><p>21)  cartesian(otherDataset)</p><p>笛卡尔积</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-19.png">    <pre><code class="scala">scala&gt; val rdd1 = sc.parallelize(1 to 3)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(2 to 5)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at &lt;console&gt;:24scala&gt; rdd1.cartesian(rdd2).collect()res17: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (2,2), (2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5))</code></pre><p>22) pipe(command, [envVars])</p><p>对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。</p><p>Shell脚本</p><pre><code class="shell">#!/bin/shecho &quot;AA&quot;while read LINE; do   echo &quot;&gt;&gt;&gt;&quot;${LINE}done</code></pre><font color="red">shell脚本需要集群中的所有节点都能访问到。</font><pre><code class="scala">scala&gt; val rdd = sc.parallelize(List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;),1)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[50] at parallelize at &lt;console&gt;:24scala&gt; rdd.pipe(&quot;/home/bigdata/pipe.sh&quot;).collect()res18: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)scala&gt; val rdd = sc.parallelize(List(&quot;hi&quot;,&quot;Hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;),2)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:24scala&gt; rdd.pipe(&quot;/home/bigdata/pipe.sh&quot;).collect()res19: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, AA, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)pipe.sh:#!/bin/shecho &quot;AA&quot;while read LINE; do   echo &quot;&gt;&gt;&gt;&quot;${LINE}done</code></pre><p>23) coalesce(numPartitions)    </p><p>缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[54] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres20: Int = 4scala&gt; val coalesceRDD = rdd.coalesce(3)coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[55] at coalesce at &lt;console&gt;:26scala&gt; coalesceRDD.partitions.sizeres21: Int = 3</code></pre><p>24) repartition(numPartitions)</p><p>根据分区数，从新通过网络随机洗牌所有数据。</p><pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres22: Int = 4scala&gt; val rerdd = rdd.repartition(2)rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[60] at repartition at &lt;console&gt;:26scala&gt; rerdd.partitions.sizeres23: Int = 2scala&gt; val rerdd = rdd.repartition(4)rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[64] at repartition at &lt;console&gt;:26scala&gt; rerdd.partitions.sizeres24: Int = 4</code></pre><p>25) repartitionAndSortWithinPartitions(partitioner)</p><p>repartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高。</p><p>26) glom</p><p>将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-20.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[65] at parallelize at &lt;console&gt;:24scala&gt; rdd.glom().collect()res25: Array[Array[Int]] = Array(Array(1, 2, 3, 4), Array(5, 6, 7, 8), Array(9, 10, 11, 12), Array(13, 14, 15, 16))</code></pre><p>27) mapValues</p><p>针对于(K,V)形式的类型只对V进行操作。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-21.png">    <pre><code class="scala">scala&gt; val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[67] at parallelize at &lt;console&gt;:24scala&gt; rdd3.mapValues(_+&quot;|||&quot;).collect()res26: Array[(Int, String)] = Array((1,a|||), (1,d|||), (2,b|||), (3,c|||))</code></pre><p>28) subtract</p><p>计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来。</p><img src="/2018/02/09/spark-4-spark-rdd-program-transformation/spark-22.png">    <pre><code class="scala">scala&gt; val rdd = sc.parallelize(3 to 8)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[70] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[71] at parallelize at &lt;console&gt;:24scala&gt; rdd.subtract(rdd1).collect()res27: Array[Int] = Array(8, 6, 7)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;编程模型&quot;&gt;&lt;a href=&quot;#编程模型&quot; class=&quot;headerlink&quot; title=&quot;编程模型&quot;&gt;&lt;/a&gt;编程模型&lt;/h2&gt;&lt;p&gt;在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。&lt;/p&gt;
&lt;p&gt;要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。&lt;/p&gt;
&lt;img src=&quot;/2018/02/09/spark-4-spark-rdd-program-transformation/spark-1.png&quot;&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：3、Spark RDD</title>
    <link href="http://www.coderfei.com/2018/02/08/spark-3-spark-rdd.html"/>
    <id>http://www.coderfei.com/2018/02/08/spark-3-spark-rdd.html</id>
    <published>2018-02-08T12:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.760Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RDD存在的原因"><a href="#RDD存在的原因" class="headerlink" title="RDD存在的原因"></a>RDD存在的原因</h2><p>RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？<br>Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。<br>MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。</p><p>MR中的迭代</p><img src="/2018/02/08/spark-3-spark-rdd/spark-1.png">    <p>Spark中的迭代</p><img src="/2018/02/08/spark-3-spark-rdd/spark-2.png">    <p>我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。<br>但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。</p><a id="more"></a><h2 id="RDD是什么"><a href="#RDD是什么" class="headerlink" title="RDD是什么"></a>RDD是什么</h2><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</p><p>在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。</p><p>RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><p>RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。 </p><p>Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。</p><p>默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。</p><h2 id="RDD的基本属性"><a href="#RDD的基本属性" class="headerlink" title="RDD的基本属性"></a>RDD的基本属性</h2><p>首先来看一下官方的描述</p><img src="/2018/02/08/spark-3-spark-rdd/spark-3.png">    <p>1)  一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p>2)  一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p>3)  RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p>4)  一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p>5)  一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p><font color="red"><br>小结：RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。<br></font><img src="/2018/02/08/spark-3-spark-rdd/spark-4.png">    <h2 id="RDD的弹性理解"><a href="#RDD的弹性理解" class="headerlink" title="RDD的弹性理解"></a>RDD的弹性理解</h2><p>1)  自动进行内存和磁盘数据存储的切换</p><p>Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换。</p><p>2)  基于血统的高效容错机制</p><p>在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</p><p>3)  Task如果失败会自动进行特定次数的重试</p><p>RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</p><p>4)  Stage如果失败会自动进行特定次数的重试</p><p>如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。</p><p>5)  Checkpoint和Persist可主动或被动触发</p><p>RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RDD依赖都会被移除。</p><p>6)  数据调度弹性</p><p>Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。</p><p>7)  数据分片的高度弹性</p><p>可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。</p><p>小结</p><p>RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系统，简单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。</p><h2 id="RDD的特点"><a href="#RDD的特点" class="headerlink" title="RDD的特点"></a>RDD的特点</h2><font color="red"><br>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。<br></font><p>1)  分区</p><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p><img src="/2018/02/08/spark-3-spark-rdd/spark-5.png">    <p>2)  只读</p><p>如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p><img src="/2018/02/08/spark-3-spark-rdd/spark-6.png">    <p>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。</p><img src="/2018/02/08/spark-3-spark-rdd/spark-7.png">    <p>RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p><img src="/2018/02/08/spark-3-spark-rdd/spark-8.png">    <p>3)  依赖</p><font color="red">RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。<br></font><img src="/2018/02/08/spark-3-spark-rdd/spark-9.png">    <p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。</p><img src="/2018/02/08/spark-3-spark-rdd/spark-10.png">    <p>4)  缓存</p><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><img src="/2018/02/08/spark-3-spark-rdd/spark-11.png">    <p>5)  checkpoint</p><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。<br>给定一个RDD我们至少可以知道如下几点信息：</p><font color="red"><br>1、分区数以及分区方式；<br><br>2、由父RDDs衍生而来的相关依赖信息；<br><br>3、计算每个分区的数据，计算步骤为：<br><br>1）如果被缓存，则从缓存中取的分区的数据；<br><br>2）如果被checkpoint，则从checkpoint处恢复数据；<br><br>3）根据血缘关系计算分区的数据。<br></font>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;RDD存在的原因&quot;&gt;&lt;a href=&quot;#RDD存在的原因&quot; class=&quot;headerlink&quot; title=&quot;RDD存在的原因&quot;&gt;&lt;/a&gt;RDD存在的原因&lt;/h2&gt;&lt;p&gt;RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？&lt;br&gt;Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。&lt;br&gt;MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。&lt;/p&gt;
&lt;p&gt;MR中的迭代&lt;/p&gt;
&lt;img src=&quot;/2018/02/08/spark-3-spark-rdd/spark-1.png&quot;&gt;    
&lt;p&gt;Spark中的迭代&lt;/p&gt;
&lt;img src=&quot;/2018/02/08/spark-3-spark-rdd/spark-2.png&quot;&gt;    
&lt;p&gt;我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。&lt;br&gt;但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：2、Spark 集群部署</title>
    <link href="http://www.coderfei.com/2018/02/06/spark-2-spark-cluster.html"/>
    <id>http://www.coderfei.com/2018/02/06/spark-2-spark-cluster.html</id>
    <published>2018-02-06T13:22:23.000Z</published>
    <updated>2018-11-20T02:07:46.755Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h2><img src="/2018/02/06/spark-2-spark-cluster/spark-1.png"><p>从物理部署层面上来看，Spark主要分为两种类型的节点，Master节点和Worker节点，Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。</p><p>从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。</p><font color="red">Spark的部署模式有Local、Local-Cluster、Standalone、Yarn、Mesos。</font><h2 id="安装部署Standalone模式"><a href="#安装部署Standalone模式" class="headerlink" title="安装部署Standalone模式"></a>安装部署Standalone模式</h2><p>1)  解压spark</p><pre><code>$ tar -zxvf /opt/software/installations/spark-2.2.0-bin-hadoop2.6.0.tgz -C /opt/software/</code></pre><p>2)  进入spark安装目录下的conf目录中，重命名“.template”结尾的文件</p><pre><code>$ mv docker.properties.template docker.properties$ mv log4j.properties.template log4j.properties$ mv metrics.properties.template metrics.properties$ mv slaves.template slaves$ mv spark-defaults.conf.template spark-defaults.conf$ mv spark-env.sh.template spark-env.sh</code></pre><p>3)  修改slaves</p><pre><code>hadoop001hadoop002hadoop003</code></pre><p>4)  修改spark-default.conf，用于配置Job History Server</p><pre><code>spark.eventLog.enabled           truespark.eventLog.dir               hdfs://hadoop001:9000/directoryspark.eventLog.compress          true</code></pre><p>5)  修改spark-env.sh</p><pre><code>SPARK_MASTER_HOST= hadoop001SPARK_MASTER_PORT=7077export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000-Dspark.history.retainedApplications=3-Dspark.history.fs.logDirectory=hdfs://hadoop001:9000/directory&quot;</code></pre><p>6)  分发配置好的spark安装包</p><pre><code>$ scp -r spark-2.2.0-bin-hadoop2.6.0/ hadoop002:/opt/software/$ scp -r spark-2.2.0-bin-hadoop2.6.0/ hadoop003:/opt/software/$ sbin/start-all.sh</code></pre><p>7)  启动spark（启动spark之前，确保HDFS已经启动）</p><pre><code>$ bin/spark-shell \--master spark://hadoop001:7077 \--executor-memory 2g \--total-executor-cores 2</code></pre><a id="more"></a><h2 id="Spark的高可用"><a href="#Spark的高可用" class="headerlink" title="Spark的高可用"></a>Spark的高可用</h2><p>Spark-Master的高可用，需要借助于Zookeeper，所以先确保Zookeeper启动完成。</p><p>高可用架构图：</p><img src="/2018/02/06/spark-2-spark-cluster/spark-2.png"><p>1)  修改spark-env.sh文件(注释掉之前的：SPARK_MASTER_HOST=hadoop001)</p><pre><code class="scala"># SPARK_MASTER_HOST=hadoop001SPARK_MASTER_PORT=7077export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000-Dspark.history.retainedApplications=3-Dspark.history.fs.logDirectory=hdfs://hadoop001:9000/directory&quot;export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop001:2181,hadoop002:2181,hadoop003:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;</code></pre><p>2)  分发配置</p><pre><code class="scala">$ scp -r spark-2.2.0-bin-hadoop2.6.0/conf hadoop002:/opt/software/spark-2.2.0-bin-hadoop2.6.0/$ scp -r spark-2.2.0-bin-hadoop2.6.0/conf hadoop003:/opt/software/spark-2.2.0-bin-hadoop2.6.0</code></pre><p>3)  按照如下规则执行启动脚本</p><p>第一台master机器</p><pre><code class="scala">$ sbin/start-all.sh</code></pre><p>第二台master机器</p><pre><code class="scala">$ sbin/start-master.sh</code></pre><p>4)  client连接高可用的spark集群</p><pre><code>$ bin/spark-shell –master spark://hadoop001:7077,hadoop002:7077</code></pre><h2 id="安装部署Yarn模式"><a href="#安装部署Yarn模式" class="headerlink" title="安装部署Yarn模式"></a>安装部署Yarn模式</h2><p>1)  修改yarn-site.xml</p><pre><code class="xml">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;&lt;property&gt;    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;&lt;property&gt;    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;</code></pre><p>2)  修改spark-evn.sh</p><pre><code>HADOOP_CONF_DIR=/opt/software/hadoop-2.6.0/etc/hadoopYARN_CONF_DIR=/opt/software/hadoop-2.6.0/etc/hadoop</code></pre><h2 id="运行Spark任务"><a href="#运行Spark任务" class="headerlink" title="运行Spark任务"></a>运行Spark任务</h2><h3 id="Standalone上运行"><a href="#Standalone上运行" class="headerlink" title="Standalone上运行"></a>Standalone上运行</h3><pre><code>bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://hadoop001:7077 \--executor-memory 1G \--total-executor-cores 2 \/opt/software/spark-2.2.0-bin-hadoop2.6.0/examples/jars/spark-examples_2.11-2.2.0.jar \100</code></pre><p>参数说明</p><pre><code>--master spark://hadoop001:7077 指定Master的地址--executor-memory 1G 指定每个executor可用内存为1G--total-executor-cores 2 指定每个executor使用的cup核数为2个--jars 添加任务所需的其他依赖</code></pre><h3 id="Yarn上运行"><a href="#Yarn上运行" class="headerlink" title="Yarn上运行"></a>Yarn上运行</h3><pre><code>/opt/software/spark-2.2.0-bin-hadoop2.6.0/bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode client \/opt/software/spark-2.2.0-bin-hadoop2.6.0/examples/jars/spark-examples_2.11-2.2.0.jar \100</code></pre><h3 id="应用提交的方式"><a href="#应用提交的方式" class="headerlink" title="应用提交的方式"></a>应用提交的方式</h3><p>1)  打包完成后，可以使用bin/spark-submit脚本来提交应用，格式如下</p><pre><code>bin/spark-submit \--class &lt;main-class&gt;--master &lt;master-url&gt; \--deploy-mode &lt;deploy-mode&gt; \--conf &lt;key&gt;=&lt;value&gt; \... # other options&lt;application-jar&gt; \[application-arguments]</code></pre><p>2)  解释</p><img src="/2018/02/06/spark-2-spark-cluster/spark-3.png"><p>其中–master可以有如下URL形式    </p><img src="/2018/02/06/spark-2-spark-cluster/spark-4.png"><h2 id="Spark-Shell交互式编程"><a href="#Spark-Shell交互式编程" class="headerlink" title="Spark-Shell交互式编程"></a>Spark-Shell交互式编程</h2><h3 id="启动Spark-Shell"><a href="#启动Spark-Shell" class="headerlink" title="启动Spark-Shell"></a>启动Spark-Shell</h3><pre><code>bin/spark-shell \--master spark://hadoop001:7077 \--executor-memory 2g \--total-executor-cores 2</code></pre><font color="red"><br>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。<br><br><br>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可。<br></font><h3 id="运行WordCount程序"><a href="#运行WordCount程序" class="headerlink" title="运行WordCount程序"></a>运行WordCount程序</h3><p>随意创建一个单词本，例如words.txt，然后上传至HDFS中，使用spark进行操作。</p><p>举例</p><pre><code class="scala">sc.textFile(&quot;hdfs://hadoop001:9000/words.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://hadoop001:9000/output&quot;)</code></pre><h2 id="Spark概念总结"><a href="#Spark概念总结" class="headerlink" title="Spark概念总结"></a>Spark概念总结</h2><p>每个Spark应用都由一个驱动器程序(driver program)来发起集群上的各种 并行操作。驱动器程序包含应用的 main 函数，并且定义了集群上的分布式数据集，还对这 些分布式数据集应用了相关操作。</p><p>驱动器程序通过一个 SparkContext 对象来访问 Spark。这个对象代表对计算集群的一个连 接。shell 启动时已经自动创建了一个 SparkContext 对象，是一个叫作 sc 的变量。</p><p>驱动器程序一般要管理多个执行器(executor)节点。</p><img src="/2018/02/06/spark-2-spark-cluster/spark-5.png">]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;集群角色&quot;&gt;&lt;a href=&quot;#集群角色&quot; class=&quot;headerlink&quot; title=&quot;集群角色&quot;&gt;&lt;/a&gt;集群角色&lt;/h2&gt;&lt;img src=&quot;/2018/02/06/spark-2-spark-cluster/spark-1.png&quot;&gt;
&lt;p&gt;从物理部署层面上来看，Spark主要分为两种类型的节点，Master节点和Worker节点，Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。&lt;/p&gt;
&lt;p&gt;从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。&lt;/p&gt;
&lt;font color=&quot;red&quot;&gt;Spark的部署模式有Local、Local-Cluster、Standalone、Yarn、Mesos。&lt;/font&gt;

&lt;h2 id=&quot;安装部署Standalone模式&quot;&gt;&lt;a href=&quot;#安装部署Standalone模式&quot; class=&quot;headerlink&quot; title=&quot;安装部署Standalone模式&quot;&gt;&lt;/a&gt;安装部署Standalone模式&lt;/h2&gt;&lt;p&gt;1)  解压spark&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tar -zxvf /opt/software/installations/spark-2.2.0-bin-hadoop2.6.0.tgz -C /opt/software/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2)  进入spark安装目录下的conf目录中，重命名“.template”结尾的文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mv docker.properties.template docker.properties
$ mv log4j.properties.template log4j.properties
$ mv metrics.properties.template metrics.properties
$ mv slaves.template slaves
$ mv spark-defaults.conf.template spark-defaults.conf
$ mv spark-env.sh.template spark-env.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3)  修改slaves&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hadoop001
hadoop002
hadoop003
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4)  修改spark-default.conf，用于配置Job History Server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://hadoop001:9000/directory
spark.eventLog.compress          true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5)  修改spark-env.sh&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SPARK_MASTER_HOST= hadoop001
SPARK_MASTER_PORT=7077

export SPARK_HISTORY_OPTS=&amp;quot;-Dspark.history.ui.port=4000
-Dspark.history.retainedApplications=3
-Dspark.history.fs.logDirectory=hdfs://hadoop001:9000/directory&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6)  分发配置好的spark安装包&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ scp -r spark-2.2.0-bin-hadoop2.6.0/ hadoop002:/opt/software/
$ scp -r spark-2.2.0-bin-hadoop2.6.0/ hadoop003:/opt/software/
$ sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;7)  启动spark（启动spark之前，确保HDFS已经启动）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/spark-shell \
--master spark://hadoop001:7077 \
--executor-memory 2g \
--total-executor-cores 2
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark：1、Spark 概述</title>
    <link href="http://www.coderfei.com/2018/02/05/spark-1-spark-summary.html"/>
    <id>http://www.coderfei.com/2018/02/05/spark-1-spark-summary.html</id>
    <published>2018-02-05T12:12:23.000Z</published>
    <updated>2018-11-20T02:07:46.751Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是spark"><a href="#什么是spark" class="headerlink" title="什么是spark"></a>什么是spark</h2><p><a href="http://spark.apache.org" target="_blank" rel="noopener">官网地址</a></p><p>Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。项目是用Scala进行编写。</p><p>目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是基于内存计算的大数据并行计算框架。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark 使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分 别管理的负担。 </p><p>Spark的内置项目如下</p><img src="/2018/02/05/spark-1-spark-summary/spark-1.png"><a id="more"></a><h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><p>Spark Core实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义。 </p><h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>Spark SQL是Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。 </p><h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>Spark Streaming是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。 </p><h3 id="Spark-MLlib"><a href="#Spark-MLlib" class="headerlink" title="Spark MLlib"></a>Spark MLlib</h3><p>提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 </p><h3 id="集群管理器"><a href="#集群管理器" class="headerlink" title="集群管理器"></a>集群管理器</h3><p>Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(cluster manager)上运行，包括 Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度 器，叫作独立调度器。 </p><p>Spark得到了众多大数据公司的支持，这些公司包括Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、阿里、腾讯、京东、携程、优酷土豆。当前百度的Spark已应用于凤巢、大搜索、直达号、百度大数据等业务；阿里利用GraphX构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯Spark集群达到8000台的规模，是当前已知的世界上最大的Spark集群。</p><h2 id="Spark特点"><a href="#Spark特点" class="headerlink" title="Spark特点"></a>Spark特点</h2><h3 id="快"><a href="#快" class="headerlink" title="快"></a>快</h3><p>与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。</p><img src="/2018/02/05/spark-1-spark-summary/spark-2.png"><h3 id="易用"><a href="#易用" class="headerlink" title="易用"></a>易用</h3><p>Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。</p><img src="/2018/02/05/spark-1-spark-summary/spark-3.png"><h3 id="通用"><a href="#通用" class="headerlink" title="通用"></a>通用</h3><p>Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</p><h3 id="兼容性"><a href="#兼容性" class="headerlink" title="兼容性"></a>兼容性</h3><p>Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。</p><img src="/2018/02/05/spark-1-spark-summary/spark-4.png"><h2 id="Spark的用户和用途"><a href="#Spark的用户和用途" class="headerlink" title="Spark的用户和用途"></a>Spark的用户和用途</h2><p>大致把Spark的用例分为两类：数据科学应用和数据处理应用。也就对应的有两种人群：数据科学家和工程师。</p><h3 id="数据科学任务"><a href="#数据科学任务" class="headerlink" title="数据科学任务"></a>数据科学任务</h3><p>主要是数据分析领域，数据科学家要负责分析数据并建模，具备 SQL、统计、预测建模(机器学习)等方面的经验，以及一定的使用 Python、 Matlab 或 R 语言进行编程的能力。</p><h3 id="数据处理应用"><a href="#数据处理应用" class="headerlink" title="数据处理应用"></a>数据处理应用</h3><p>工程师定义为使用 Spark 开发 生产环境中的数据处理应用的软件开发者，通过对接Spark的API实现对处理的处理和转换等任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是spark&quot;&gt;&lt;a href=&quot;#什么是spark&quot; class=&quot;headerlink&quot; title=&quot;什么是spark&quot;&gt;&lt;/a&gt;什么是spark&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。项目是用Scala进行编写。&lt;/p&gt;
&lt;p&gt;目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是基于内存计算的大数据并行计算框架。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark 使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分 别管理的负担。 &lt;/p&gt;
&lt;p&gt;Spark的内置项目如下&lt;/p&gt;
&lt;img src=&quot;/2018/02/05/spark-1-spark-summary/spark-1.png&quot;&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.coderfei.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.coderfei.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>HBase：6、HBase 优化</title>
    <link href="http://www.coderfei.com/2018/02/03/hbase-6-hbase-optimization.html"/>
    <id>http://www.coderfei.com/2018/02/03/hbase-6-hbase-optimization.html</id>
    <published>2018-02-03T14:12:23.000Z</published>
    <updated>2018-11-20T02:07:46.696Z</updated>
    
    <content type="html"><![CDATA[<h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><p>在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。</p><p>1) 关闭HBase集群（如果没有开启则跳过此步）</p><pre><code>$ bin/stop-hbase.sh</code></pre><p>2) 在conf目录下创建backup-masters文件</p><pre><code>$ touch conf/backup-masters</code></pre><p>3) 在backup-masters文件中配置高可用HMaster节点</p><pre><code>$ echo hadoop002 &gt; conf/backup-masters</code></pre><p>4) 将整个conf目录scp到其他节点</p><pre><code>$ scp -r conf/ hadoop002:/opt/software/hbase-1.2.0$ scp -r conf/ hadoop003:/opt/software/hbase-1.2.0</code></pre><p>5) 重新启动HBase后打开页面测试查看</p><pre><code>0.98版本之前：http://hadoop001:600100.98版本之后：http://hadoop001:16010</code></pre><a id="more"></a><h2 id="Hadoop的通用性优化"><a href="#Hadoop的通用性优化" class="headerlink" title="Hadoop的通用性优化"></a>Hadoop的通用性优化</h2><p>1) NameNode元数据备份使用SSD</p><p>2) 定时备份NameNode上的元数据</p><p>每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。</p><p>3) 为NameNode指定多个元数据目录</p><p>使用dfs.name.dir或者dfs.namenode.name.dir指定。这样可以提供元数据的冗余和健壮性，以免发生故障。</p><p>4) NameNode的dir自恢复</p><p>设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。</p><p>5) HDFS保证RPC调用会有较多的线程数</p><p>hdfs-site.xml</p><pre><code>属性：dfs.namenode.handler.count解释：该属性是NameNode服务默认线程数，默认值是10，根据机器的可用内存可以调整为50~100。属性：dfs.datanode.handler.count解释：该属性默认值为10，是DataNode的处理线程数，如果HDFS客户端程序读写请求比较多，可以调高到15~20，设置的值越大，内存消耗越多，不要调整的过高，一般业务中，5~10即可。</code></pre><p>6) HDFS副本数的调整</p><p>hdfs-site.xml</p><pre><code>属性：dfs.replication解释：如果数据量巨大，且不是非常之重要，可以调整为2~3，如果数据非常之重要，可以调整为3~5。</code></pre><p>7) HDFS文件块大小的调整</p><p>hdfs-site.xml</p><pre><code>属性：dfs.blocksize解释：块大小定义，该属性应该根据存储的大量的单个文件大小来设置，如果大量的单个文件都小于100M，建议设置成64M块大小，对于大于100M或者达到GB的这种情况，建议设置成256M，一般设置范围波动在64M~256M之间。</code></pre><p>8) MapReduce Job任务服务线程数调整</p><p>mapred-site.xml</p><pre><code>属性：mapreduce.jobtracker.handler.count解释：该属性是Job任务线程数，默认值是10，根据机器的可用内存可以调整为50~100</code></pre><p>9) Http服务器工作线程数</p><p>mapred-site.xml</p><pre><code>属性：mapreduce.tasktracker.http.threads解释：定义HTTP服务器工作线程数，默认值为40，对于大集群可以调整到80~100</code></pre><p>10) 文件排序合并优化</p><p>mapred-site.xml</p><pre><code>属性：mapreduce.task.io.sort.factor解释：文件排序时同时合并的数据流的数量，这也定义了同时打开文件的个数，默认值为10，如果调高该参数，可以明显减少磁盘IO，即减少文件读取的次数。</code></pre><p>11) 设置任务并发</p><p>mapred-site.xml</p><pre><code>属性：mapreduce.map.speculative解释：该属性可以设置任务是否可以并发执行，如果任务多而小，该属性设置为true可以明显加快任务执行效率，但是对于延迟非常高的任务，建议改为false，这就类似于迅雷下载。</code></pre><p>12) MR输出数据的压缩</p><p>mapred-site.xml</p><pre><code>属性：mapreduce.map.output.compress、mapreduce.output.fileoutputformat.compress解释：对于大集群而言，建议设置Map-Reduce的输出为压缩的数据，而对于小集群，则不需要。</code></pre><p>13) 优化Mapper和Reducer的个数</p><p>mapred-site.xml</p><pre><code>属性：mapreduce.tasktracker.map.tasks.maximummapreduce.tasktracker.reduce.tasks.maximum解释：以上两个属性分别为一个单独的Job任务可以同时运行的Map和Reduce的数量。设置上面两个参数时，需要考虑CPU核数、磁盘和内存容量。假设一个8核的CPU，业务内容非常消耗CPU，那么可以设置map数量为4，如果该业务不是特别消耗CPU类型的，那么可以设置map数量为40，reduce数量为20。这些参数的值修改完成之后，一定要观察是否有较长等待的任务，如果有的话，可以减少数量以加快任务执行，如果设置一个很大的值，会引起大量的上下文切换，以及内存与磁盘之间的数据交换，这里没有标准的配置数值，需要根据业务和硬件配置以及经验来做出选择。在同一时刻，不要同时运行太多的MapReduce，这样会消耗过多的内存，任务会执行的非常缓慢，我们需要根据CPU核数，内存容量设置一个MR任务并发的最大值，使固定数据量的任务完全加载到内存中，避免频繁的内存和磁盘数据交换，从而降低磁盘IO，提高性能。</code></pre><p>大概估算公式</p><pre><code>map = 2 + ⅔cpu_corereduce = 2 + ⅓cpu_core</code></pre><h2 id="Linux优化"><a href="#Linux优化" class="headerlink" title="Linux优化"></a>Linux优化</h2><p>1) 开启文件系统的预读缓存可以提高读取速度</p><pre><code>$ sudo blockdev --setra 32768 /dev/sda</code></pre><font color="red">ra是readahead的缩写</font><p>2) 关闭进程睡眠池</p><p>即不允许后台进程进入睡眠状态，如果进程空闲，则直接kill掉释放资源</p><pre><code>$ sudo sysctl -w vm.swappiness=0</code></pre><p>3) 调整ulimit上限，默认值为比较小的数字</p><pre><code>$ ulimit -n 查看允许最大进程数$ ulimit -u 查看允许打开最大文件数</code></pre><p>优化修改</p><pre><code>$ sudo vi /etc/security/limits.conf 修改打开文件数限制末尾添加：*                soft    nofile          1024000*                hard    nofile          1024000Hive             -       nofile          1024000hive             -       nproc           1024000 </code></pre><p>$ sudo vi /etc/security/limits.d/90-nproc.conf 修改用户打开进程数限制<br>修改为</p><pre><code>#*          soft    nproc     4096#root       soft    nproc     unlimited*          soft    nproc     40960root       soft    nproc     unlimited</code></pre><p>4) 开启集群的时间同步NTP</p><p>集群中某台机器同步网络时间服务器的时间，集群中其他机器则同步这台机器的时间。</p><p>5) 更新系统补丁</p><p>更新补丁前，请先测试新版本补丁对集群节点的兼容性。</p><h2 id="Zookeeper优化"><a href="#Zookeeper优化" class="headerlink" title="Zookeeper优化"></a>Zookeeper优化</h2><p>1) 优化Zookeeper会话超时时间</p><p>hbase-site.xml</p><pre><code>参数：zookeeper.session.timeout解释：In hbase-site.xml, set zookeeper.session.timeout to 30 seconds or less to bound failure detection (20-30 seconds is a good start).该值会直接关系到master发现服务器宕机的最大周期，默认值为30秒（不同的HBase版本，该默认值不一样），如果该值过小，会在HBase在写入大量数据发生而GC时，导致RegionServer短暂的不可用，从而没有向ZK发送心跳包，最终导致认为从节点shutdown。一般20台左右的集群需要配置5台zookeeper。</code></pre><h2 id="HBase优化"><a href="#HBase优化" class="headerlink" title="HBase优化"></a>HBase优化</h2><h3 id="预分区"><a href="#预分区" class="headerlink" title="预分区"></a>预分区</h3><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据索要投放的分区提前大致的规划好，以提高HBase性能。</p><p>1) 手动设定预分区</p><pre><code>hbase&gt; create &#39;staff&#39;,&#39;info&#39;,&#39;partition1&#39;,SPLITS =&gt; [&#39;1000&#39;,&#39;2000&#39;,&#39;3000&#39;,&#39;4000&#39;]</code></pre><p>2) 生成16进制序列预分区</p><pre><code>create &#39;staff2&#39;,&#39;info&#39;,&#39;partition2&#39;,{NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#39;HexStringSplit&#39;}</code></pre><p>3) 按照文件中设置的规则预分区</p><p>创建splits.txt文件内容如下</p><pre><code>bbbbaaaaccccdddd</code></pre><p>然后执行</p><pre><code>create &#39;staff3&#39;,&#39;partition3&#39;,SPLITS_FILE =&gt; &#39;splits.txt&#39;</code></pre><p>4) 使用JavaAPI创建预分区</p><pre><code>//自定义算法，产生一系列Hash散列值存储在二维数组中byte[][] splitKeys = 某个散列值函数//创建HBaseAdmin实例HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create());//创建HTableDescriptor实例HTableDescriptor tableDesc = new HTableDescriptor(tableName);//通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表hAdmin.createTable(tableDesc, splitKeys);</code></pre><p>3.5.2、RowKey设计</p><p>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。</p><p>1) 生成随机数、hash、散列值</p><pre><code>比如原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。</code></pre><p>2) 字符串反转</p><pre><code>20170524000001转成1000004250710220170524000002转成20000042507102这样也可以在一定程度上散列逐步put进来的数据。</code></pre><p>3) 字符串拼接</p><pre><code>20170524000001_a12e20170524000001_93i7</code></pre><h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p><h3 id="基础优化"><a href="#基础优化" class="headerlink" title="基础优化"></a>基础优化</h3><p>1) 允许在HDFS的文件中追加内容</p><p>不是不允许追加内容么？没错，请看背景故事</p><p><a href="http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/" target="_blank" rel="noopener">http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/</a> </p><p>hdfs-site.xml、hbase-site.xml</p><pre><code>属性：dfs.support.append解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。</code></pre><p>2) 优化DataNode允许的最大文件打开数</p><p>hdfs-site.xml</p><pre><code>属性：dfs.datanode.max.transfer.threads解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096</code></pre><p>3) 优化延迟高的数据操作的等待时间</p><p>hdfs-site.xml</p><pre><code>属性：dfs.image.transfer.timeout解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。</code></pre><p>4) 优化数据的写入效率</p><p>mapred-site.xml</p><pre><code>属性：mapreduce.map.output.compressmapreduce.map.output.compress.codec解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。</code></pre><p>5) 优化DataNode存储</p><p>hdfs-site.xml</p><pre><code>属性：dfs.datanode.failed.volumes.tolerated解释： 默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。</code></pre><p>6) 设置RPC监听数量</p><p>hbase-site.xml</p><pre><code>属性：hbase.regionserver.handler.count解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。</code></pre><p>7) 优化HStore文件大小</p><p>hbase-site.xml</p><pre><code>属性：hbase.hregion.max.filesize解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。</code></pre><p>8) 优化hbase客户端缓存</p><p>hbase-site.xml</p><pre><code>属性：hbase.client.write.buffer解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</code></pre><p>9) 指定scan.next扫描HBase所获取的行数</p><p>hbase-site.xml</p><pre><code>属性：hbase.client.scanner.caching解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。</code></pre><p>10) flush、compact、split机制</p><p>当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。</p><p>涉及属性：</p><p>即：128M就是Memstore的默认阈值</p><pre><code>hbase.hregion.memstore.flush.size：134217728</code></pre><p>即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。</p><pre><code>hbase.regionserver.global.memstore.upperLimit：0.4hbase.regionserver.global.memstore.lowerLimit：0.38新版本为none</code></pre><p>即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;高可用&quot;&gt;&lt;a href=&quot;#高可用&quot; class=&quot;headerlink&quot; title=&quot;高可用&quot;&gt;&lt;/a&gt;高可用&lt;/h2&gt;&lt;p&gt;在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。&lt;/p&gt;
&lt;p&gt;1) 关闭HBase集群（如果没有开启则跳过此步）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/stop-hbase.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2) 在conf目录下创建backup-masters文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ touch conf/backup-masters
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3) 在backup-masters文件中配置高可用HMaster节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo hadoop002 &amp;gt; conf/backup-masters
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4) 将整个conf目录scp到其他节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ scp -r conf/ hadoop002:/opt/software/hbase-1.2.0
$ scp -r conf/ hadoop003:/opt/software/hbase-1.2.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5) 重新启动HBase后打开页面测试查看&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0.98版本之前：http://hadoop001:60010
0.98版本之后：http://hadoop001:16010
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="HBase" scheme="http://www.coderfei.com/categories/HBase/"/>
    
    
      <category term="HBase" scheme="http://www.coderfei.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase：5、HBase与Hive集成</title>
    <link href="http://www.coderfei.com/2018/02/02/hbase-5-hbase-hive.html"/>
    <id>http://www.coderfei.com/2018/02/02/hbase-5-hbase-hive.html</id>
    <published>2018-02-02T15:32:23.000Z</published>
    <updated>2018-11-20T02:07:46.696Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HBase与Hive的对比"><a href="#HBase与Hive的对比" class="headerlink" title="HBase与Hive的对比"></a>HBase与Hive的对比</h2><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>(1) 数据仓库</p><p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p><p>(2) 用于数据分析、清洗</p><p>Hive适用于离线的数据分析和清洗，延迟较高。</p><p>(3) 基于HDFS、MapReduce</p><p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>(1) 数据库</p><p>是一种面向列存储的非关系型数据库。</p><p>(2) 用于存储结构化和非结构话的数据</p><p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p><p>(3) 基于HDFS</p><p>数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。</p><p>(4) 延迟较低，接入在线业务使用</p><p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p><h2 id="HBase与Hive集成使用"><a href="#HBase与Hive集成使用" class="headerlink" title="HBase与Hive集成使用"></a>HBase与Hive集成使用</h2><font color="red"><br>HBase与Hive的集成在最新的两个版本中无法兼容。所以，我们只能重新编译：hive-hbase-handler-1.2.0.jar<br></font><p>环境准备</p><p>因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。记得还有把zookeeper的jar包考入到hive的lib目录下。</p><pre><code>$ export HBASE_HOME=/opt/software/hbase-1.2.0$ export HIVE_HOME=/opt/software/apache-hive-1.1.0-bin$ ln -s $HBASE_HOME/lib/hbase-common-1.2.0.jar  $HIVE_HOME/lib/hbase-common-1.2.0.jar$ ln -s $HBASE_HOME/lib/hbase-server-1.2.0.jar $HIVE_HOME/lib/hbase-server-1.2.0.jar$ ln -s $HBASE_HOME/lib/hbase-client-1.2.0.jar $HIVE_HOME/lib/hbase-client-1.2.0.jar$ ln -s $HBASE_HOME/lib/hbase-protocol-1.2.0.jar $HIVE_HOME/lib/hbase-protocol-1.2.0.jar$ ln -s $HBASE_HOME/lib/hbase-it-1.2.0.jar $HIVE_HOME/lib/hbase-it-1.2.0.jar$ ln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar$ ln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.2.0.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.2.0.jar$ ln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.2.0.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.2.0.jar</code></pre><a id="more"></a><p>同时在hive-site.xml中修改zookeeper的属性</p><pre><code>&lt;property&gt;  &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;  &lt;value&gt;hadoop001, hadoop002, hadoop003 &lt;/value&gt;  &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt;  &lt;value&gt;2181&lt;/value&gt;  &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;</code></pre><p>1) 案例一</p><p>目标：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p><p>(1) 在Hive中创建表同时关联HBase</p><pre><code>CREATE TABLE hive_hbase_emp_table(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);</code></pre><font color="red"><br>完成之后，可以分别进入Hive和HBase查看，都生成了对应的表<br></font><p>(2) 在Hive中创建临时中间表，用于load文件中的数据</p><font color="red"><br>不能将数据直接load进Hive所关联HBase的那张表中<br></font><pre><code>CREATE TABLE emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)row format delimited fields terminated by &#39;\t&#39;;</code></pre><p>(3) 向Hive中间表中load数据</p><pre><code>hive&gt; load data local inpath &#39;/opt/softwares/data/emp.txt&#39; into table emp;</code></pre><p>(4) 通过insert命令将中间表中的数据导入到Hive关联HBase的那张表中</p><pre><code>hive&gt; insert into table hive_hbase_emp_table select * from emp;</code></pre><p>(5) 查看Hive以及关联的HBase表中是否已经成功的同步插入了数据</p><pre><code>Hive：hive&gt; select * from hive_hbase_emp_table;HBase：hbase&gt; scan ‘hbase_emp_table’</code></pre><p>2) 案例二</p><p>目标：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。</p><p>注：该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。</p><p>(1) 在Hive中创建外部表</p><pre><code>CREATE EXTERNAL TABLE relevance_hbase_emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);</code></pre><p>(2) 关联后就可以使用Hive函数进行一些分析操作了</p><pre><code>hive (default)&gt; select * from relevance_hbase_emp;</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;HBase与Hive的对比&quot;&gt;&lt;a href=&quot;#HBase与Hive的对比&quot; class=&quot;headerlink&quot; title=&quot;HBase与Hive的对比&quot;&gt;&lt;/a&gt;HBase与Hive的对比&lt;/h2&gt;&lt;h3 id=&quot;Hive&quot;&gt;&lt;a href=&quot;#Hive&quot; class=&quot;headerlink&quot; title=&quot;Hive&quot;&gt;&lt;/a&gt;Hive&lt;/h3&gt;&lt;p&gt;(1) 数据仓库&lt;/p&gt;
&lt;p&gt;Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。&lt;/p&gt;
&lt;p&gt;(2) 用于数据分析、清洗&lt;/p&gt;
&lt;p&gt;Hive适用于离线的数据分析和清洗，延迟较高。&lt;/p&gt;
&lt;p&gt;(3) 基于HDFS、MapReduce&lt;/p&gt;
&lt;p&gt;Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。&lt;/p&gt;
&lt;h3 id=&quot;HBase&quot;&gt;&lt;a href=&quot;#HBase&quot; class=&quot;headerlink&quot; title=&quot;HBase&quot;&gt;&lt;/a&gt;HBase&lt;/h3&gt;&lt;p&gt;(1) 数据库&lt;/p&gt;
&lt;p&gt;是一种面向列存储的非关系型数据库。&lt;/p&gt;
&lt;p&gt;(2) 用于存储结构化和非结构话的数据&lt;/p&gt;
&lt;p&gt;适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。&lt;/p&gt;
&lt;p&gt;(3) 基于HDFS&lt;/p&gt;
&lt;p&gt;数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。&lt;/p&gt;
&lt;p&gt;(4) 延迟较低，接入在线业务使用&lt;/p&gt;
&lt;p&gt;面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。&lt;/p&gt;
&lt;h2 id=&quot;HBase与Hive集成使用&quot;&gt;&lt;a href=&quot;#HBase与Hive集成使用&quot; class=&quot;headerlink&quot; title=&quot;HBase与Hive集成使用&quot;&gt;&lt;/a&gt;HBase与Hive集成使用&lt;/h2&gt;&lt;font color=&quot;red&quot;&gt;&lt;br&gt;HBase与Hive的集成在最新的两个版本中无法兼容。所以，我们只能重新编译：hive-hbase-handler-1.2.0.jar&lt;br&gt;&lt;/font&gt;

&lt;p&gt;环境准备&lt;/p&gt;
&lt;p&gt;因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。记得还有把zookeeper的jar包考入到hive的lib目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export HBASE_HOME=/opt/software/hbase-1.2.0
$ export HIVE_HOME=/opt/software/apache-hive-1.1.0-bin

$ ln -s $HBASE_HOME/lib/hbase-common-1.2.0.jar  $HIVE_HOME/lib/hbase-common-1.2.0.jar
$ ln -s $HBASE_HOME/lib/hbase-server-1.2.0.jar $HIVE_HOME/lib/hbase-server-1.2.0.jar
$ ln -s $HBASE_HOME/lib/hbase-client-1.2.0.jar $HIVE_HOME/lib/hbase-client-1.2.0.jar
$ ln -s $HBASE_HOME/lib/hbase-protocol-1.2.0.jar $HIVE_HOME/lib/hbase-protocol-1.2.0.jar
$ ln -s $HBASE_HOME/lib/hbase-it-1.2.0.jar $HIVE_HOME/lib/hbase-it-1.2.0.jar
$ ln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar
$ ln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.2.0.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.2.0.jar
$ ln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.2.0.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.2.0.jar
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="HBase" scheme="http://www.coderfei.com/categories/HBase/"/>
    
    
      <category term="HBase" scheme="http://www.coderfei.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase：4、HBase 使用（二）</title>
    <link href="http://www.coderfei.com/2018/02/01/hbase-4-hbase-use.html"/>
    <id>http://www.coderfei.com/2018/02/01/hbase-4-hbase-use.html</id>
    <published>2018-02-01T15:32:23.000Z</published>
    <updated>2018-11-20T02:07:46.696Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HBase读写流程"><a href="#HBase读写流程" class="headerlink" title="HBase读写流程"></a>HBase读写流程</h2><h3 id="HBase读数据流程"><a href="#HBase读数据流程" class="headerlink" title="HBase读数据流程"></a>HBase读数据流程</h3><p>1) HRegionServer保存着.META.的这样一张表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取-ROOT-表所在位置（新版本没有，暂不讨论新版本），进而找到.META.表所在的位置信息，即找到这个.META.表在哪个HRegionServer上保存着。</p><p>2) 接着Client通过刚才获取到的HRegionServer的IP来访问.META.表所在的HRegionServer，从而读取到.META.，进而获取到.META.表中存放的元数据。</p><p>3) Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。</p><p>4) 最后HRegionServer把查询到的数据响应给Client。</p><h3 id="HBase写数据流程"><a href="#HBase写数据流程" class="headerlink" title="HBase写数据流程"></a>HBase写数据流程</h3><p>1) Client也是先访问zookeeper，找到-ROOT-表（新版本没有该表，暂不讨论），进而找到.META.表，并获取.META.表信息。</p><p>2) 确定当前将要写入的数据所对应的RegionServer服务器和Region。</p><p>3) Client向该RegionServer服务器发起写入数据请求，然后RegionServer收到请求并响应。</p><p>4) Client先把数据写入到HLog，以防止数据丢失。</p><p>5) 然后将数据写入到Memstore。</p><p>6) 如果Hlog和Memstore均写入成功，则这条数据写入成功。在此过程中，如果Memstore达到阈值，会把Memstore中的数据flush到StoreFile中。</p><p>7) 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile。当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。</p><font color="red"><br>提示：因为内存空间是有限的，所以说溢写过程必定伴随着大量的小文件产生。</font><a id="more"></a><h2 id="JavaAPI"><a href="#JavaAPI" class="headerlink" title="JavaAPI"></a>JavaAPI</h2><h3 id="新建Maven-Project"><a href="#新建Maven-Project" class="headerlink" title="新建Maven Project"></a>新建Maven Project</h3><p>新建项目后在pom.xml中添加依赖</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;    &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;    &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;    &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;    &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 下面这个依赖不是必须添加 --&gt;&lt;dependency&gt;    &lt;groupId&gt;jdk.tools&lt;/groupId&gt;    &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;    &lt;version&gt;1.6&lt;/version&gt;    &lt;scope&gt;system&lt;/scope&gt;    &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt;</code></pre><h3 id="编写HBaseAPI"><a href="#编写HBaseAPI" class="headerlink" title="编写HBaseAPI"></a>编写HBaseAPI</h3><p>这部分的内容，我们先学习使用老版本的API，接着再写出新版本的API调用方式。因为有些时候我们需要一些过时的API来提供更好的兼容性。</p><p>首先需要获取Configuration对象</p><pre><code class="java">public static Configuration conf;static{//使用HBaseConfiguration的单例方法实例化conf = HBaseConfiguration.create();conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.216.20&quot;);conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);}</code></pre><p>判断表是否存在</p><pre><code class="java">public static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{    //在HBase中管理、访问表需要先创建HBaseAdmin对象//Connection connection = ConnectionFactory.createConnection(conf);//HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();    HBaseAdmin admin = new HBaseAdmin(conf);    return admin.tableExists(tableName);}</code></pre><p>创建表</p><pre><code class="java">public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{    HBaseAdmin admin = new HBaseAdmin(conf);    //判断表是否存在    if(isTableExist(tableName)){        System.out.println(&quot;表&quot; + tableName + &quot;已存在&quot;);        //System.exit(0);    }else{        //创建表属性对象,表名需要转字节        HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName));        //创建多个列族        for(String cf : columnFamily){            descriptor.addFamily(new HColumnDescriptor(cf));        }        //根据对表的配置，创建表        admin.createTable(descriptor);        System.out.println(&quot;表&quot; + tableName + &quot;创建成功！&quot;);    }}</code></pre><p>删除表</p><pre><code class="java">public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{    HBaseAdmin admin = new HBaseAdmin(conf);    if(isTableExist(tableName)){        admin.disableTable(tableName);        admin.deleteTable(tableName);        System.out.println(&quot;表&quot; + tableName + &quot;删除成功！&quot;);    }else{        System.out.println(&quot;表&quot; + tableName + &quot;不存在！&quot;);    }}</code></pre><p>向表中插入数据</p><pre><code class="java">public static void addRowData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException{    //创建HTable对象    HTable hTable = new HTable(conf, tableName);    //向表中插入数据    Put put = new Put(Bytes.toBytes(rowKey));    //向Put对象中组装数据    put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value));    hTable.put(put);    hTable.close();    System.out.println(&quot;插入数据成功&quot;);}</code></pre><p>删除多行数据</p><pre><code class="java">public static void deleteMultiRow(String tableName, String... rows) throws IOException{    HTable hTable = new HTable(conf, tableName);    List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;();    for(String row : rows){        Delete delete = new Delete(Bytes.toBytes(row));        deleteList.add(delete);    }    hTable.delete(deleteList);    hTable.close();}</code></pre><p>得到所有数据</p><pre><code class="java">public static void getAllRows(String tableName) throws IOException{    HTable hTable = new HTable(conf, tableName);    //得到用于扫描region的对象    Scan scan = new Scan();    //使用HTable得到resultcanner实现类的对象    ResultScanner resultScanner = hTable.getScanner(scan);    for(Result result : resultScanner){        Cell[] cells = result.rawCells();        for(Cell cell : cells){            //得到rowkey            System.out.println(&quot;行键:&quot; + Bytes.toString(CellUtil.cloneRow(cell)));            //得到列族            System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell)));            System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell)));            System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell)));        }    }}</code></pre><p>得到某一行所有数据</p><pre><code class="java">public static void getRow(String tableName, String rowKey) throws IOException{    HTable table = new HTable(conf, tableName);    Get get = new Get(Bytes.toBytes(rowKey));    //get.setMaxVersions();显示所有版本    //get.setTimeStamp();显示指定时间戳的版本    Result result = table.get(get);    for(Cell cell : result.rawCells()){        System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow()));        System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell)));        System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell)));        System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell)));        System.out.println(&quot;时间戳:&quot; + cell.getTimestamp());    }}</code></pre><p>获取某一行指定“列族:列”的数据</p><pre><code class="java">public static void getRowQualifier(String tableName, String rowKey, String family, String qualifier) throws IOException{    HTable table = new HTable(conf, tableName);    Get get = new Get(Bytes.toBytes(rowKey));    get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier));    Result result = table.get(get);    for(Cell cell : result.rawCells()){        System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow()));        System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell)));        System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell)));        System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell)));    }}</code></pre><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。</p><h3 id="官方HBase-MapReduce"><a href="#官方HBase-MapReduce" class="headerlink" title="官方HBase-MapReduce"></a>官方HBase-MapReduce</h3><p>查看HBase的MapReduce任务的所需的依赖</p><pre><code>$ bin/hbase mapredcp</code></pre><p>执行环境变量的导入</p><pre><code>$ export HBASE_HOME=/opt/software/hbase-1.2.0$ export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp`</code></pre><p>运行官方的MapReduce任务</p><p>案例一：统计Student表中有多少行数据</p><pre><code>$ /opt/software/hadoop-2.6.0//bin/yarn jar /opt/software/hbase-1.2.0/lib/hbase-server-1.2.0.jar rowcounter student</code></pre><p>案例二：使用MapReduce将本地数据导入到HBase</p><p>(1) 在本地创建一个tsv格式的文件：fruit.tsv</p><pre><code>1001    Apple    Red1002    Pear        Yellow1003    Pineapple    Yellow</code></pre><p>(2) 创建HBase表</p><pre><code>hbase(main):001:0&gt; create &#39;fruit&#39;,&#39;info&#39;</code></pre><p>(3) 在HDFS中创建input_fruit文件夹并上传fruit.tsv文件</p><pre><code>$ /opt/software/hadoop-2.6.0/bin/hdfs dfs -mkdir /input_fruit/$ /opt/software/hadoop-2.6.0/bin/hdfs dfs -put fruit.tsv /input_fruit/</code></pre><p>(4) 执行MapReduce到HBase的fruit表中</p><pre><code>$ /opt/software/hadoop-2.6.0/bin/yarn jar lib/hbase-server-1.2.0.jar importtsv \-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \hdfs://hadoop001:9000/input_fruit</code></pre><p>(5) 使用scan命令查看导入后的结果</p><pre><code>hbase(main):001:0&gt; scan ‘fruit’</code></pre><h3 id="自定义HBase-MapReduce1"><a href="#自定义HBase-MapReduce1" class="headerlink" title="自定义HBase-MapReduce1"></a>自定义HBase-MapReduce1</h3><p>目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中。</p><p>1) 构建ReadFruitMapper类，用于读取fruit表中的数据</p><pre><code class="java">package com.z.hbase_mr;import java.io.IOException;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;public class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; {    @Override    protected void map(ImmutableBytesWritable key, Result value, Context context)     throws IOException, InterruptedException {    //将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。        Put put = new Put(key.get());        //遍历添加column行        for(Cell cell: value.rawCells()){            //添加/克隆列族:info            if(&quot;info&quot;.equals(Bytes.toString(CellUtil.cloneFamily(cell)))){                //添加/克隆列：name                if(&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){                    //将该列cell加入到put对象中                    put.add(cell);                    //添加/克隆列:color                }else if(&quot;color&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){                    //向该列cell加入到put对象中                    put.add(cell);                }            }        }        //将从fruit读取到的每行数据写入到context中作为map的输出        context.write(key, put);    }}</code></pre><p>2) 构建WriteFruitMRReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中</p><pre><code class="java">package com.z.hbase_mr;import java.io.IOException;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; {    @Override    protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)     throws IOException, InterruptedException {        //读出来的每一行数据写入到fruit_mr表中        for(Put put: values){            context.write(NullWritable.get(), put);        }    }}</code></pre><p>3) 构建Fruit2FruitMRRunner extends Configured implements Tool用于组装运行Job任务</p><pre><code class="java">//组装Jobpublic int run(String[] args) throws Exception {    //得到Configuration    Configuration conf = this.getConf();    //创建Job任务    Job job = Job.getInstance(conf, this.getClass().getSimpleName());    job.setJarByClass(Fruit2FruitMRRunner.class);    //配置Job    Scan scan = new Scan();    scan.setCacheBlocks(false);    scan.setCaching(500);    //设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本    TableMapReduceUtil.initTableMapperJob(    &quot;fruit&quot;, //数据源的表名    scan, //scan扫描控制器    ReadFruitMapper.class,//设置Mapper类    ImmutableBytesWritable.class,//设置Mapper输出key类型    Put.class,//设置Mapper输出value值类型    job//设置给哪个JOB    );    //设置Reducer    TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRReducer.class, job);    //设置Reduce数量，最少1个    job.setNumReduceTasks(1);    boolean isSuccess = job.waitForCompletion(true);    if(!isSuccess){        throw new IOException(&quot;Job running with error&quot;);    }    return isSuccess ? 0 : 1;}</code></pre><p>4) 主函数中调用运行该Job任务</p><pre><code class="java">public static void main( String[] args ) throws Exception{    Configuration conf = HBaseConfiguration.create();    int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args);    System.exit(status);}</code></pre><p>5) 打包运行任务</p><pre><code>$ /opt/software/hadoop-2.6.0/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr1.Fruit2FruitMRRunner</code></pre><font color="red"><br><br>运行任务前，如果待数据导入的表不存在，则需要提前创建之。<br><br>maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</font><h3 id="自定义HBase-MapReduce2"><a href="#自定义HBase-MapReduce2" class="headerlink" title="自定义HBase-MapReduce2"></a>自定义HBase-MapReduce2</h3><p>目标：实现将HDFS中的数据写入到HBase表中。</p><p>1) 构建ReadFruitFromHDFSMapper于读取HDFS中的文件数据</p><pre><code class="java">package com.z.hbase.mr2;import java.io.IOException;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //从HDFS中读取的数据        String lineValue = value.toString();        //读取出来的每行数据使用\t进行分割，存于String数组        String[] values = lineValue.split(&quot;\t&quot;);        //根据数据中值的含义取值        String rowKey = values[0];        String name = values[1];        String color = values[2];        //初始化rowKey        ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey));        //初始化put对象        Put put = new Put(Bytes.toBytes(rowKey));        //参数分别:列族、列、值          put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;),  Bytes.toBytes(name));         put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;color&quot;),  Bytes.toBytes(color));         context.write(rowKeyWritable, put);    }}</code></pre><p>2) 构建WriteFruitMRFromTxtReducer类</p><pre><code class="java">package com.z.hbase.mr2;import java.io.IOException;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; {    @Override    protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException {        //读出来的每一行数据写入到fruit_hdfs表中        for(Put put: values){            context.write(NullWritable.get(), put);        }    }}</code></pre><p>3) 创建Txt2FruitRunner组装Job</p><pre><code class="java">public int run(String[] args) throws Exception {//得到ConfigurationConfiguration conf = this.getConf();//创建Job任务Job job = Job.getInstance(conf, this.getClass().getSimpleName());job.setJarByClass(Txt2FruitRunner.class);Path inPath = new Path(&quot;hdfs://linux01:8020/input_fruit/fruit.tsv&quot;);FileInputFormat.addInputPath(job, inPath);//设置Mapperjob.setMapperClass(ReadFruitFromHDFSMapper.class);job.setMapOutputKeyClass(ImmutableBytesWritable.class);job.setMapOutputValueClass(Put.class);//设置ReducerTableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRFromTxtReducer.class, job);//设置Reduce数量，最少1个job.setNumReduceTasks(1);boolean isSuccess = job.waitForCompletion(true);if(!isSuccess){throw new IOException(&quot;Job running with error&quot;);}return isSuccess ? 0 : 1;}</code></pre><p>4) 调用执行Job</p><pre><code class="java">public static void main(String[] args) throws Exception {        Configuration conf = HBaseConfiguration.create();        int status = ToolRunner.run(conf, new Txt2FruitRunner(), args);        System.exit(status);}</code></pre><p>5) 打包运行</p><pre><code>$ /opt/software/hadoop-2.6.0/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr2.Txt2FruitRunner</code></pre><font color="red"><br>运行任务前，如果待数据导入的表不存在，则需要提前创建之。<br><br>maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）<br><br></font>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;HBase读写流程&quot;&gt;&lt;a href=&quot;#HBase读写流程&quot; class=&quot;headerlink&quot; title=&quot;HBase读写流程&quot;&gt;&lt;/a&gt;HBase读写流程&lt;/h2&gt;&lt;h3 id=&quot;HBase读数据流程&quot;&gt;&lt;a href=&quot;#HBase读数据流程&quot; class=&quot;headerlink&quot; title=&quot;HBase读数据流程&quot;&gt;&lt;/a&gt;HBase读数据流程&lt;/h3&gt;&lt;p&gt;1) HRegionServer保存着.META.的这样一张表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取-ROOT-表所在位置（新版本没有，暂不讨论新版本），进而找到.META.表所在的位置信息，即找到这个.META.表在哪个HRegionServer上保存着。&lt;/p&gt;
&lt;p&gt;2) 接着Client通过刚才获取到的HRegionServer的IP来访问.META.表所在的HRegionServer，从而读取到.META.，进而获取到.META.表中存放的元数据。&lt;/p&gt;
&lt;p&gt;3) Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。&lt;/p&gt;
&lt;p&gt;4) 最后HRegionServer把查询到的数据响应给Client。&lt;/p&gt;
&lt;h3 id=&quot;HBase写数据流程&quot;&gt;&lt;a href=&quot;#HBase写数据流程&quot; class=&quot;headerlink&quot; title=&quot;HBase写数据流程&quot;&gt;&lt;/a&gt;HBase写数据流程&lt;/h3&gt;&lt;p&gt;1) Client也是先访问zookeeper，找到-ROOT-表（新版本没有该表，暂不讨论），进而找到.META.表，并获取.META.表信息。&lt;/p&gt;
&lt;p&gt;2) 确定当前将要写入的数据所对应的RegionServer服务器和Region。&lt;/p&gt;
&lt;p&gt;3) Client向该RegionServer服务器发起写入数据请求，然后RegionServer收到请求并响应。&lt;/p&gt;
&lt;p&gt;4) Client先把数据写入到HLog，以防止数据丢失。&lt;/p&gt;
&lt;p&gt;5) 然后将数据写入到Memstore。&lt;/p&gt;
&lt;p&gt;6) 如果Hlog和Memstore均写入成功，则这条数据写入成功。在此过程中，如果Memstore达到阈值，会把Memstore中的数据flush到StoreFile中。&lt;/p&gt;
&lt;p&gt;7) 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile。当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。&lt;/p&gt;
&lt;font color=&quot;red&quot;&gt;&lt;br&gt;提示：因为内存空间是有限的，所以说溢写过程必定伴随着大量的小文件产生。&lt;/font&gt;
    
    </summary>
    
      <category term="HBase" scheme="http://www.coderfei.com/categories/HBase/"/>
    
    
      <category term="HBase" scheme="http://www.coderfei.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase：3、HBase 使用（一）</title>
    <link href="http://www.coderfei.com/2018/01/29/hbase-3-hbase-use.html"/>
    <id>http://www.coderfei.com/2018/01/29/hbase-3-hbase-use.html</id>
    <published>2018-01-29T14:25:23.000Z</published>
    <updated>2018-11-20T02:07:46.695Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>进入HBase客户端命令行</p><pre><code>$ bin/hbase shell</code></pre><p>查看帮助命令</p><pre><code>hbase(main)&gt; help</code></pre><p>查看当前数据库中有哪些表</p><pre><code>hbase(main)&gt; list</code></pre><h3 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h3><p>创建表</p><pre><code>hbase(main)&gt; create &#39;student&#39;,&#39;info&#39;</code></pre><p>插入数据到表</p><pre><code>hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Thomas&#39;hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:sex&#39;,&#39;male&#39;hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;18&#39;hbase(main) &gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:name&#39;,&#39;Janna&#39;hbase(main) &gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:sex&#39;,&#39;female&#39;hbase(main) &gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:age&#39;,&#39;20&#39;</code></pre><p>扫描查看表数据</p><pre><code>hbase(main) &gt; scan &#39;student&#39;hbase(main) &gt; scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;, STOPROW  =&gt; &#39;1001&#39;}hbase(main) &gt; scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;}</code></pre><a id="more"></a><p>查看表结构</p><pre><code>hbase(main):012:0&gt; describe ‘student’</code></pre><p>更新指定字段的数据</p><pre><code>hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Nick&#39;hbase(main) &gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;100&#39;</code></pre><p>查看“指定行”或“指定列族:列”的数据</p><pre><code>hbase(main) &gt; get &#39;student&#39;,&#39;1001&#39;hbase(main) &gt; get &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;</code></pre><p>删除某rowkey的全部数据</p><pre><code>hbase(main) &gt; deleteall &#39;student&#39;,&#39;1001&#39;</code></pre><p>删除某rowkey的某一列数据</p><pre><code>hbase(main) &gt; delete &#39;student&#39;,&#39;1002&#39;,&#39;info:sex&#39;</code></pre><p>清空表数据</p><pre><code>hbase(main) &gt; truncate &#39;student&#39;</code></pre><font color="red">提示：清空表的操作顺序为先disable，然后再truncating。</font><p>删除表</p><p>首先需要先让该表为disable状态</p><pre><code>hbase(main) &gt; disable &#39;student&#39;</code></pre><p>然后才能drop这个表</p><pre><code>hbase(main) &gt; drop &#39;student&#39;</code></pre><font color="red">提示：如果直接drop表，会报错：Drop the named table. Table must first be disabled<br>ERROR: Table student is enabled. Disable it first.</font><p>统计表数据行数</p><pre><code>hbase(main) &gt; count &#39;student&#39;</code></pre><p>变更表信息</p><p>将info列族中的数据存放3个版本</p><pre><code>hbase(main) &gt; alter &#39;student&#39;,{NAME=&gt;&#39;info&#39;,VERSIONS=&gt;3}</code></pre><h2 id="常用Shell操作"><a href="#常用Shell操作" class="headerlink" title="常用Shell操作"></a>常用Shell操作</h2><h3 id="satus"><a href="#satus" class="headerlink" title="satus"></a>satus</h3><p>显示服务器状态</p><pre><code>hbase&gt; status ‘linux01’</code></pre><h3 id="whoami"><a href="#whoami" class="headerlink" title="whoami"></a>whoami</h3><p>显示HBase当前用户</p><pre><code>hbase&gt; whoami</code></pre><h3 id="list"><a href="#list" class="headerlink" title="list"></a>list</h3><p>显示当前所有的表</p><pre><code>hbase&gt; list</code></pre><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>统计指定表的记录数，例如：</p><pre><code>hbase&gt; count &#39;hbase_book&#39;</code></pre><h3 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h3><p>展示表结构信息</p><pre><code>hbase&gt; describe &#39;hbase_book&#39;</code></pre><h3 id="exist"><a href="#exist" class="headerlink" title="exist"></a>exist</h3><p>检查表是否存在，适用于表量特别多的情况</p><pre><code>hbase&gt; exist &#39;hbase_book&#39;</code></pre><h3 id="is-enabled-is-disabled"><a href="#is-enabled-is-disabled" class="headerlink" title="is_enabled/is_disabled"></a>is_enabled/is_disabled</h3><p>检查表是否启用或禁用</p><pre><code>hbase&gt; is_enabled &#39;hbase_book&#39;hbase&gt; is_disabled &#39;hbase_book&#39;</code></pre><h3 id="alter"><a href="#alter" class="headerlink" title="alter"></a>alter</h3><p>该命令可以改变表和列族的模式</p><p>为当前表增加列族</p><pre><code>hbase&gt; alter &#39;hbase_book&#39;, NAME =&gt; &#39;CF2&#39;, VERSIONS =&gt; 2</code></pre><p>为当前表删除列族</p><pre><code>hbase&gt; alter &#39;hbase_book&#39;, &#39;delete&#39; =&gt; &#39;CF2&#39;</code></pre><h3 id="disable"><a href="#disable" class="headerlink" title="disable"></a>disable</h3><p>禁用一张表</p><pre><code>hbase&gt; disable &#39;hbase_book&#39;</code></pre><h3 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h3><p>删除一张表，记得在删除表之前必须先禁用</p><pre><code>hbase&gt; drop &#39;hbase_book&#39;</code></pre><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><p>删除一行中一个单元格的值</p><pre><code>hbase&gt; delete ‘hbase_book’, ‘rowKey’, ‘CF:C’</code></pre><h3 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h3><p>清空表数据，即禁用表-删除表-创建表</p><pre><code>hbase&gt; truncate &#39;hbase_book&#39;</code></pre><h3 id="create"><a href="#create" class="headerlink" title="create"></a>create</h3><p>创建表</p><pre><code>hbase&gt; create ‘table’, ‘cf’</code></pre><p>创建多个列族</p><pre><code>hbase&gt; create &#39;t1&#39;, {NAME =&gt; &#39;f1&#39;}, {NAME =&gt; &#39;f2&#39;}, {NAME =&gt; &#39;f3&#39;}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简单使用&quot;&gt;&lt;a href=&quot;#简单使用&quot; class=&quot;headerlink&quot; title=&quot;简单使用&quot;&gt;&lt;/a&gt;简单使用&lt;/h2&gt;&lt;h3 id=&quot;基本操作&quot;&gt;&lt;a href=&quot;#基本操作&quot; class=&quot;headerlink&quot; title=&quot;基本操作&quot;&gt;&lt;/a&gt;基本操作&lt;/h3&gt;&lt;p&gt;进入HBase客户端命令行&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/hbase shell
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看帮助命令&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hbase(main)&amp;gt; help
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看当前数据库中有哪些表&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hbase(main)&amp;gt; list
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;表的操作&quot;&gt;&lt;a href=&quot;#表的操作&quot; class=&quot;headerlink&quot; title=&quot;表的操作&quot;&gt;&lt;/a&gt;表的操作&lt;/h3&gt;&lt;p&gt;创建表&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hbase(main)&amp;gt; create &amp;#39;student&amp;#39;,&amp;#39;info&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;插入数据到表&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hbase(main) &amp;gt; put &amp;#39;student&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;info:name&amp;#39;,&amp;#39;Thomas&amp;#39;
hbase(main) &amp;gt; put &amp;#39;student&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;info:sex&amp;#39;,&amp;#39;male&amp;#39;
hbase(main) &amp;gt; put &amp;#39;student&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;info:age&amp;#39;,&amp;#39;18&amp;#39;
hbase(main) &amp;gt; put &amp;#39;student&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;info:name&amp;#39;,&amp;#39;Janna&amp;#39;
hbase(main) &amp;gt; put &amp;#39;student&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;info:sex&amp;#39;,&amp;#39;female&amp;#39;
hbase(main) &amp;gt; put &amp;#39;student&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;info:age&amp;#39;,&amp;#39;20&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;扫描查看表数据&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hbase(main) &amp;gt; scan &amp;#39;student&amp;#39;
hbase(main) &amp;gt; scan &amp;#39;student&amp;#39;,{STARTROW =&amp;gt; &amp;#39;1001&amp;#39;, STOPROW  =&amp;gt; &amp;#39;1001&amp;#39;}
hbase(main) &amp;gt; scan &amp;#39;student&amp;#39;,{STARTROW =&amp;gt; &amp;#39;1001&amp;#39;}
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="HBase" scheme="http://www.coderfei.com/categories/HBase/"/>
    
    
      <category term="HBase" scheme="http://www.coderfei.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase：2、HBase 部署</title>
    <link href="http://www.coderfei.com/2018/01/28/hbase-2-hbase-cluster.html"/>
    <id>http://www.coderfei.com/2018/01/28/hbase-2-hbase-cluster.html</id>
    <published>2018-01-28T14:35:23.000Z</published>
    <updated>2018-11-20T02:07:46.695Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Zookeeper正常部署"><a href="#Zookeeper正常部署" class="headerlink" title="Zookeeper正常部署"></a>Zookeeper正常部署</h2><p>首先保证Zookeeper集群的正常部署，并启动</p><pre><code>$ bin/zkServer.sh start</code></pre><h2 id="Hadoop正常部署"><a href="#Hadoop正常部署" class="headerlink" title="Hadoop正常部署"></a>Hadoop正常部署</h2><p>Hadoop集群的正常部署并启动</p><pre><code>$ sbin/start-dfs.sh$ sbin/start-yarn.sh</code></pre><h2 id="HBase解压"><a href="#HBase解压" class="headerlink" title="HBase解压"></a>HBase解压</h2><p>解压HBase到指定目录</p><pre><code>$ tar -zxf ~/softwares/installations/hbase-1.2.0-bin.tar.gz -C /opt/software/</code></pre><h2 id="HBase配置文件"><a href="#HBase配置文件" class="headerlink" title="HBase配置文件"></a>HBase配置文件</h2><p>hbase-env.sh</p><pre><code>export JAVA_HOME=/usr/java/jdk1.8.0_121export HBASE_MANAGES_ZK=false</code></pre><p>hbase-site.xml</p><pre><code class="xml">&lt;configuration&gt;    &lt;property&gt;             &lt;name&gt;hbase.rootdir&lt;/name&gt;             &lt;value&gt;hdfs://hadoop001:9000/hbase&lt;/value&gt;       &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;   &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.master.port&lt;/name&gt;        &lt;value&gt;16000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;        &lt;value&gt; hadoop001:2181, hadoop002:2181, hadoop003:2181&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;           &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;     &lt;value&gt;/opt/software/zookeeper-3.4.5/zkData&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>regionservers</p><pre><code>hadoop001hadoop002hadoop003</code></pre><a id="more"></a><h2 id="HBase需要依赖的Jar包"><a href="#HBase需要依赖的Jar包" class="headerlink" title="HBase需要依赖的Jar包"></a>HBase需要依赖的Jar包</h2><p>由于HBase需要依赖Hadoop，所以替换HBase的lib目录下的jar包，以解决兼容问题</p><p>1) 删除原有的jar</p><pre><code>$ rm -rf /opt/software/hbase-1.2.0/lib/hadoop-*$ rm -rf /opt/software/hbase-1.2.0/lib/zookeeper-3.4.6.jar</code></pre><p>2) 拷贝新jar，涉及的jar</p><pre><code>hadoop-annotations-2.6.0.jarhadoop-auth-2.6.0.jarhadoop-client-2.6.0.jarhadoop-common-2.6.0.jarhadoop-hdfs-2.6.0.jarhadoop-mapreduce-client-app-2.6.0.jarhadoop-mapreduce-client-common-2.6.0.jarhadoop-mapreduce-client-core-2.6.0.jarhadoop-mapreduce-client-hs-2.6.0.jarhadoop-mapreduce-client-hs-plugins-2.6.0.jarhadoop-mapreduce-client-jobclient-2.6.0.jarhadoop-mapreduce-client-jobclient-2.6.0-tests.jarhadoop-mapreduce-client-shuffle-2.6.0.jarhadoop-yarn-api-2.6.0.jarhadoop-yarn-applications-distributedshell-2.6.0.jarhadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jarhadoop-yarn-client-2.6.0.jarhadoop-yarn-common-2.6.0.jarhadoop-yarn-server-applicationhistoryservice-2.6.0.jarhadoop-yarn-server-common-2.6.0.jarhadoop-yarn-server-nodemanager-2.6.0.jarhadoop-yarn-server-resourcemanager-2.6.0.jarhadoop-yarn-server-tests-2.6.0.jarhadoop-yarn-server-web-proxy-2.6.0.jarzookeeper-3.4.5.jar</code></pre><font color="red">提示：这些jar包的对应版本应替换成你目前使用的hadoop版本，具体情况具体分析。</font><p>查找jar包举例</p><p>$ find /opt/software/hadoop-2.6.0/ -name hadoop-annotations*<br>然后将找到的jar包复制到HBase的lib目录下即可。</p><h2 id="HBase软连接Hadoop配置"><a href="#HBase软连接Hadoop配置" class="headerlink" title="HBase软连接Hadoop配置"></a>HBase软连接Hadoop配置</h2><pre><code>$ ln -s /opt/software/hadoop-2.6.0/etc/hadoop/core-site.xml /opt/software/hbase-1.2.0/conf/core-site.xml$ ln -s /opt/software/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /opt/software/hbase-1.2.0/conf/hdfs-site.xml</code></pre><h2 id="HBase远程scp到其他机器"><a href="#HBase远程scp到其他机器" class="headerlink" title="HBase远程scp到其他机器"></a>HBase远程scp到其他机器</h2><pre><code>$ scp -r /opt/software/hbase-1.2.0/ hadoop002:/opt/software/$ scp -r /opt/software/hbase-1.2.0/ hadoop003:/opt/software/</code></pre><h2 id="HBase服务的启动"><a href="#HBase服务的启动" class="headerlink" title="HBase服务的启动"></a>HBase服务的启动</h2><p>启动方式1</p><pre><code>$ bin/hbase-daemon.sh start master$ bin/hbase-daemon.sh start regionserver</code></pre><font color="red">提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。</font><p>修复提示</p><p>a、同步时间服务</p><p>b、属性：hbase.master.maxclockskew设置更大的值</p><pre><code class="xml">&lt;property&gt;        &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt;        &lt;value&gt;180000&lt;/value&gt;        &lt;description&gt;Time difference of regionserver from master&lt;/description&gt;&lt;/property&gt;</code></pre><p>启动方式2</p><pre><code>$ bin/start-hbase.sh</code></pre><p>对应的停止服务</p><pre><code>$ bin/stop-hbase.sh</code></pre><font color="red">提示：如果使用的是JDK8以上版本，则应在hbase-evn.sh中移除“HBASE_MASTER_OPTS”和“HBASE_REGIONSERVER_OPTS”配置。</font><h2 id="查看HBase页面"><a href="#查看HBase页面" class="headerlink" title="查看HBase页面"></a>查看HBase页面</h2><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：</p><p><a href="http://hadoop001:16010" target="_blank" rel="noopener">http://hadoop001:16010</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Zookeeper正常部署&quot;&gt;&lt;a href=&quot;#Zookeeper正常部署&quot; class=&quot;headerlink&quot; title=&quot;Zookeeper正常部署&quot;&gt;&lt;/a&gt;Zookeeper正常部署&lt;/h2&gt;&lt;p&gt;首先保证Zookeeper集群的正常部署，并启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Hadoop正常部署&quot;&gt;&lt;a href=&quot;#Hadoop正常部署&quot; class=&quot;headerlink&quot; title=&quot;Hadoop正常部署&quot;&gt;&lt;/a&gt;Hadoop正常部署&lt;/h2&gt;&lt;p&gt;Hadoop集群的正常部署并启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sbin/start-dfs.sh
$ sbin/start-yarn.sh
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;HBase解压&quot;&gt;&lt;a href=&quot;#HBase解压&quot; class=&quot;headerlink&quot; title=&quot;HBase解压&quot;&gt;&lt;/a&gt;HBase解压&lt;/h2&gt;&lt;p&gt;解压HBase到指定目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tar -zxf ~/softwares/installations/hbase-1.2.0-bin.tar.gz -C /opt/software/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;HBase配置文件&quot;&gt;&lt;a href=&quot;#HBase配置文件&quot; class=&quot;headerlink&quot; title=&quot;HBase配置文件&quot;&gt;&lt;/a&gt;HBase配置文件&lt;/h2&gt;&lt;p&gt;hbase-env.sh&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export JAVA_HOME=/usr/java/jdk1.8.0_121
export HBASE_MANAGES_ZK=false
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;hbase-site.xml&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;xml&quot;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;     
        &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;     
        &amp;lt;value&amp;gt;hdfs://hadoop001:9000/hbase&amp;lt;/value&amp;gt;   
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;   
        &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

   &amp;lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.master.port&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;16000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;   
        &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt; hadoop001:2181, hadoop002:2181, hadoop003:2181&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;   
        &amp;lt;name&amp;gt;hbase.zookeeper.property.dataDir&amp;lt;/name&amp;gt;
     &amp;lt;value&amp;gt;/opt/software/zookeeper-3.4.5/zkData&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;regionservers&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hadoop001
hadoop002
hadoop003
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="HBase" scheme="http://www.coderfei.com/categories/HBase/"/>
    
    
      <category term="HBase" scheme="http://www.coderfei.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>HBase：1、HBase 概述</title>
    <link href="http://www.coderfei.com/2018/01/27/hbase-1-hbase-summary.html"/>
    <id>http://www.coderfei.com/2018/01/27/hbase-1-hbase-summary.html</id>
    <published>2018-01-27T13:35:23.000Z</published>
    <updated>2018-11-20T02:07:46.693Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HBaes介绍"><a href="#HBaes介绍" class="headerlink" title="HBaes介绍"></a>HBaes介绍</h2><h3 id="HBase的起源"><a href="#HBase的起源" class="headerlink" title="HBase的起源"></a>HBase的起源</h3><p>HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。</p><p><a href="http://hbase.apache.org" target="_blank" rel="noopener">官方网站</a></p><ul><li><p>2006年Google发表BigTable白皮书。</p></li><li><p>2006年开始开发HBase。</p></li><li><p>2008年北京成功开奥运会，程序员默默地将HBase弄成了Hadoop的子项目。</p></li><li><p>2010年HBase成为Apache顶级项目。</p></li><li><p>现在很多公司二次开发出了很多发行版本，也开始使用了。</p></li></ul><h2 id="HBase的角色"><a href="#HBase的角色" class="headerlink" title="HBase的角色"></a>HBase的角色</h2><h3 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h3><p>功能：</p><p>1) 监控RegionServer</p><p>2) 处理RegionServer故障转移</p><p>3) 处理元数据的变更</p><p>4) 处理region的分配或移除</p><p>5) 在空闲时间进行数据的负载均衡</p><p>6) 通过Zookeeper发布自己的位置给客户端</p><h3 id="RegionServer"><a href="#RegionServer" class="headerlink" title="RegionServer"></a>RegionServer</h3><p>功能：</p><p>1) 负责存储HBase的实际数据</p><p>2) 处理分配给它的Region</p><p>3) 刷新缓存到HDFS</p><p>4) 维护HLog</p><p>5) 执行压缩</p><p>6) 负责处理Region分片</p><p>组件：</p><p>1) Write-Ahead logs</p><p>HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p><p>2) HFile</p><p>这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。</p><p>3) Store</p><p>HFile存储在Store中，一个Store对应HBase表中的一个列族。</p><p>4) MemStore</p><p>顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。</p><p>5) Region</p><p>Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。</p><h2 id="HBase的架构"><a href="#HBase的架构" class="headerlink" title="HBase的架构"></a>HBase的架构</h2><p>HBase一种是作为存储的分布式文件系统，另一种是作为数据处理模型的MR框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在HDFS直接存储的文件往往不具有结构化，所以催生出了HBase在HDFS上的操作。如果需要查询数据，只需要通过键值便可以成功访问。</p><p>架构图如下图所示：</p><img src="/2018/01/27/hbase-1-hbase-summary/hbase-1.png"><a id="more"></a><p>HBase内置有Zookeeper，但一般我们会有其他的Zookeeper集群来监管master和regionserver，Zookeeper通过选举，保证任何时候，集群中只有一个活跃的HMaster，HMaster与HRegionServer 启动时会向ZooKeeper注册，存储所有HRegion的寻址入口，实时监控HRegionserver的上线和下线信息。并实时通知给HMaster，存储HBase的schema和table元数据，默认情况下，HBase 管理ZooKeeper 实例，Zookeeper的引入使得HMaster不再是单点故障。一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。</p><p>一个RegionServer可以包含多个HRegion，每个RegionServer维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致，请参考如下架构图：</p><img src="/2018/01/27/hbase-1-hbase-summary/hbase-2.png">]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;HBaes介绍&quot;&gt;&lt;a href=&quot;#HBaes介绍&quot; class=&quot;headerlink&quot; title=&quot;HBaes介绍&quot;&gt;&lt;/a&gt;HBaes介绍&lt;/h2&gt;&lt;h3 id=&quot;HBase的起源&quot;&gt;&lt;a href=&quot;#HBase的起源&quot; class=&quot;headerlink&quot; title=&quot;HBase的起源&quot;&gt;&lt;/a&gt;HBase的起源&lt;/h3&gt;&lt;p&gt;HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://hbase.apache.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方网站&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;2006年Google发表BigTable白皮书。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2006年开始开发HBase。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2008年北京成功开奥运会，程序员默默地将HBase弄成了Hadoop的子项目。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2010年HBase成为Apache顶级项目。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;现在很多公司二次开发出了很多发行版本，也开始使用了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;HBase的角色&quot;&gt;&lt;a href=&quot;#HBase的角色&quot; class=&quot;headerlink&quot; title=&quot;HBase的角色&quot;&gt;&lt;/a&gt;HBase的角色&lt;/h2&gt;&lt;h3 id=&quot;HMaster&quot;&gt;&lt;a href=&quot;#HMaster&quot; class=&quot;headerlink&quot; title=&quot;HMaster&quot;&gt;&lt;/a&gt;HMaster&lt;/h3&gt;&lt;p&gt;功能：&lt;/p&gt;
&lt;p&gt;1) 监控RegionServer&lt;/p&gt;
&lt;p&gt;2) 处理RegionServer故障转移&lt;/p&gt;
&lt;p&gt;3) 处理元数据的变更&lt;/p&gt;
&lt;p&gt;4) 处理region的分配或移除&lt;/p&gt;
&lt;p&gt;5) 在空闲时间进行数据的负载均衡&lt;/p&gt;
&lt;p&gt;6) 通过Zookeeper发布自己的位置给客户端&lt;/p&gt;
&lt;h3 id=&quot;RegionServer&quot;&gt;&lt;a href=&quot;#RegionServer&quot; class=&quot;headerlink&quot; title=&quot;RegionServer&quot;&gt;&lt;/a&gt;RegionServer&lt;/h3&gt;&lt;p&gt;功能：&lt;/p&gt;
&lt;p&gt;1) 负责存储HBase的实际数据&lt;/p&gt;
&lt;p&gt;2) 处理分配给它的Region&lt;/p&gt;
&lt;p&gt;3) 刷新缓存到HDFS&lt;/p&gt;
&lt;p&gt;4) 维护HLog&lt;/p&gt;
&lt;p&gt;5) 执行压缩&lt;/p&gt;
&lt;p&gt;6) 负责处理Region分片&lt;/p&gt;
&lt;p&gt;组件：&lt;/p&gt;
&lt;p&gt;1) Write-Ahead logs&lt;/p&gt;
&lt;p&gt;HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。&lt;/p&gt;
&lt;p&gt;2) HFile&lt;/p&gt;
&lt;p&gt;这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。&lt;/p&gt;
&lt;p&gt;3) Store&lt;/p&gt;
&lt;p&gt;HFile存储在Store中，一个Store对应HBase表中的一个列族。&lt;/p&gt;
&lt;p&gt;4) MemStore&lt;/p&gt;
&lt;p&gt;顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。&lt;/p&gt;
&lt;p&gt;5) Region&lt;/p&gt;
&lt;p&gt;Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。&lt;/p&gt;
&lt;h2 id=&quot;HBase的架构&quot;&gt;&lt;a href=&quot;#HBase的架构&quot; class=&quot;headerlink&quot; title=&quot;HBase的架构&quot;&gt;&lt;/a&gt;HBase的架构&lt;/h2&gt;&lt;p&gt;HBase一种是作为存储的分布式文件系统，另一种是作为数据处理模型的MR框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在HDFS直接存储的文件往往不具有结构化，所以催生出了HBase在HDFS上的操作。如果需要查询数据，只需要通过键值便可以成功访问。&lt;/p&gt;
&lt;p&gt;架构图如下图所示：&lt;/p&gt;
&lt;img src=&quot;/2018/01/27/hbase-1-hbase-summary/hbase-1.png&quot;&gt;
    
    </summary>
    
      <category term="HBase" scheme="http://www.coderfei.com/categories/HBase/"/>
    
    
      <category term="HBase" scheme="http://www.coderfei.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>Kafka：6、Kafka Streams</title>
    <link href="http://www.coderfei.com/2018/01/26/kafka-6-kafka-streams.html"/>
    <id>http://www.coderfei.com/2018/01/26/kafka-6-kafka-streams.html</id>
    <published>2018-01-26T12:38:23.000Z</published>
    <updated>2018-11-20T02:07:46.744Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="Kafka-Streams"><a href="#Kafka-Streams" class="headerlink" title="Kafka Streams"></a>Kafka Streams</h3><p>Kafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。</p><h3 id="Kafka-Streams特点"><a href="#Kafka-Streams特点" class="headerlink" title="Kafka Streams特点"></a>Kafka Streams特点</h3><p>1）功能强大 </p><p>高扩展性，弹性，容错 </p><p>2）轻量级 </p><p>无需专门的集群 </p><p>一个库，而不是框架</p><p>3）完全集成 </p><p>100%的Kafka 0.10.0版本兼容</p><p>易于集成到现有的应用程序 </p><p>4）实时性</p><p>毫秒级延迟 </p><p>并非微批处理 </p><p>窗口允许乱序数据 </p><p>允许迟到数据</p><h3 id="为什么要有Kafka-Stream"><a href="#为什么要有Kafka-Stream" class="headerlink" title="为什么要有Kafka Stream"></a>为什么要有Kafka Stream</h3><p>当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。</p><p>既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因。</p><p>第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。</p><img src="/2018/01/26/kafka-6-kafka-streams/kafka-1.png"><a id="more"></a><p>第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。</p><p>第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。</p><p>第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。但是Kafka作为类库不占用系统资源。</p><p>第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。</p><p>第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。</p><h2 id="Kafka-Stream数据清洗案例"><a href="#Kafka-Stream数据清洗案例" class="headerlink" title="Kafka Stream数据清洗案例"></a>Kafka Stream数据清洗案例</h2><p>0）需求</p><p>实时处理单词带有”&gt;&gt;&gt;”前缀的内容。例如输入”coderf&gt;&gt;&gt;xiaoming”，最终处理成“xiaoming”</p><p>1）需求分析</p><img src="/2018/01/26/kafka-6-kafka-streams/kafka-2.png"><p>2）案例实操</p><p>（1）创建一个工程，并添加jar包</p><p>（2）创建主类</p><pre><code class="scala">package com.coderf.kafka.stream;import java.util.Properties;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorSupplier;import org.apache.kafka.streams.processor.TopologyBuilder;public class Application {    public static void main(String[] args) {        // 定义输入的topic        String from = &quot;first&quot;;        // 定义输出的topic        String to = &quot;second&quot;;        // 设置参数        Properties settings = new Properties();        settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;logFilter&quot;);        settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;hadoop102:9092&quot;);        StreamsConfig config = new StreamsConfig(settings);        // 构建拓扑        TopologyBuilder builder = new TopologyBuilder();        builder.addSource(&quot;SOURCE&quot;, from)               .addProcessor(&quot;PROCESS&quot;, new ProcessorSupplier&lt;byte[], byte[]&gt;() {                    @Override                    public Processor&lt;byte[], byte[]&gt; get() {                        // 具体分析处理                        return new LogProcessor();                    }                }, &quot;SOURCE&quot;)                .addSink(&quot;SINK&quot;, to, &quot;PROCESS&quot;);        // 创建kafka stream        KafkaStreams streams = new KafkaStreams(builder, config);        streams.start();    }}</code></pre><p>（3）具体业务处理</p><pre><code class="scala">package com.coderf.kafka.stream;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorContext;public class LogProcessor implements Processor&lt;byte[], byte[]&gt; {    private ProcessorContext context;    @Override    public void init(ProcessorContext context) {        this.context = context;    }    @Override    public void process(byte[] key, byte[] value) {        String input = new String(value);        // 如果包含“&gt;&gt;&gt;”则只保留该标记后面的内容        if (input.contains(&quot;&gt;&gt;&gt;&quot;)) {            input = input.split(&quot;&gt;&gt;&gt;&quot;)[1].trim();            // 输出到下一个topic            context.forward(&quot;logProcessor&quot;.getBytes(), input.getBytes());        }else{            context.forward(&quot;logProcessor&quot;.getBytes(), input.getBytes());        }    }    @Override    public void punctuate(long timestamp) {    }    @Override    public void close() {    }}</code></pre><p>（4）运行程序</p><p>（5）在hadoop003上启动生产者</p><pre><code class="scala">[root@hadoop003 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic first&gt;hello&gt;&gt;&gt;world&gt;h&gt;&gt;&gt;coderf&gt;hahaha</code></pre><p>（6）在hadoop002上启动消费者</p><pre><code class="scala">[root@hadoop002 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop001:2181 --from-beginning --topic secondworldcoderfhahaha</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;h3 id=&quot;Kafka-Streams&quot;&gt;&lt;a href=&quot;#Kafka-Streams&quot; class=&quot;headerlink&quot; title=&quot;Kafka Streams&quot;&gt;&lt;/a&gt;Kafka Streams&lt;/h3&gt;&lt;p&gt;Kafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。&lt;/p&gt;
&lt;h3 id=&quot;Kafka-Streams特点&quot;&gt;&lt;a href=&quot;#Kafka-Streams特点&quot; class=&quot;headerlink&quot; title=&quot;Kafka Streams特点&quot;&gt;&lt;/a&gt;Kafka Streams特点&lt;/h3&gt;&lt;p&gt;1）功能强大 &lt;/p&gt;
&lt;p&gt;高扩展性，弹性，容错 &lt;/p&gt;
&lt;p&gt;2）轻量级 &lt;/p&gt;
&lt;p&gt;无需专门的集群 &lt;/p&gt;
&lt;p&gt;一个库，而不是框架&lt;/p&gt;
&lt;p&gt;3）完全集成 &lt;/p&gt;
&lt;p&gt;100%的Kafka 0.10.0版本兼容&lt;/p&gt;
&lt;p&gt;易于集成到现有的应用程序 &lt;/p&gt;
&lt;p&gt;4）实时性&lt;/p&gt;
&lt;p&gt;毫秒级延迟 &lt;/p&gt;
&lt;p&gt;并非微批处理 &lt;/p&gt;
&lt;p&gt;窗口允许乱序数据 &lt;/p&gt;
&lt;p&gt;允许迟到数据&lt;/p&gt;
&lt;h3 id=&quot;为什么要有Kafka-Stream&quot;&gt;&lt;a href=&quot;#为什么要有Kafka-Stream&quot; class=&quot;headerlink&quot; title=&quot;为什么要有Kafka Stream&quot;&gt;&lt;/a&gt;为什么要有Kafka Stream&lt;/h3&gt;&lt;p&gt;当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。&lt;/p&gt;
&lt;p&gt;既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因。&lt;/p&gt;
&lt;p&gt;第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。&lt;/p&gt;
&lt;img src=&quot;/2018/01/26/kafka-6-kafka-streams/kafka-1.png&quot;&gt;
    
    </summary>
    
      <category term="Kafka" scheme="http://www.coderfei.com/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="http://www.coderfei.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka：5、Kafka Producer Interceptor</title>
    <link href="http://www.coderfei.com/2018/01/25/kafka-5-kafka-producer-interceptor.html"/>
    <id>http://www.coderfei.com/2018/01/25/kafka-5-kafka-producer-interceptor.html</id>
    <published>2018-01-25T13:35:23.000Z</published>
    <updated>2018-11-20T02:07:46.743Z</updated>
    
    <content type="html"><![CDATA[<h2 id="拦截器原理"><a href="#拦截器原理" class="headerlink" title="拦截器原理"></a>拦截器原理</h2><p>Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。</p><p>对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。</p><p>Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，</p><p>其定义的方法包括：</p><p>（1）configure(configs)</p><p>获取配置信息和初始化数据时调用。</p><p>（2）onSend(ProducerRecord)：</p><p>该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算</p><p>（3）onAcknowledgement(RecordMetadata, Exception)：</p><p>该方法会在消息被应答或消息发送失败时调用，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率</p><p>（4）close：</p><p>关闭interceptor，主要用于执行一些资源清理工作</p><p>如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</p><a id="more"></a><h2 id="拦截器案例"><a href="#拦截器案例" class="headerlink" title="拦截器案例"></a>拦截器案例</h2><p>1）需求：</p><p>实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</p><img src="/2018/01/25/kafka-5-kafka-producer-interceptor/kafka-1.png"><p>2）案例实操</p><p>（1）增加时间戳拦截器</p><pre><code class="java">package com.coderf.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; {    @Override    public void configure(Map&lt;String, ?&gt; configs) {    }    @Override    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {        // 创建一个新的record，把时间戳写入消息体的最前部        return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),                System.currentTimeMillis() + &quot;,&quot; + record.value().toString());    }    @Override    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {    }    @Override    public void close() {    }}</code></pre><p>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器</p><pre><code class="java">    package com.coderf.kafka.interceptor;    import java.util.Map;    import org.apache.kafka.clients.producer.ProducerInterceptor;    import org.apache.kafka.clients.producer.ProducerRecord;    import org.apache.kafka.clients.producer.RecordMetadata;    public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;{        private int errorCounter = 0;        private int successCounter = 0;        @Override        public void configure(Map&lt;String, ?&gt; configs) {        }        @Override        public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {             return record;        }        @Override        public void onAcknowledgement(RecordMetadata metadata, Exception exception) {            // 统计成功和失败的次数            if (exception == null) {                successCounter++;            } else {                errorCounter++;            }        }        @Override        public void close() {            // 保存结果            System.out.println(&quot;Successful sent: &quot; + successCounter);            System.out.println(&quot;Failed sent: &quot; + errorCounter);        }    }</code></pre><p>（3）producer主程序</p><pre><code class="java">package com.coderf.kafka.interceptor;import java.util.ArrayList;import java.util.List;import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;public class InterceptorProducer {    public static void main(String[] args) throws Exception {        // 1 设置配置信息        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);        props.put(&quot;acks&quot;, &quot;all&quot;);        props.put(&quot;retries&quot;, 0);        props.put(&quot;batch.size&quot;, 16384);        props.put(&quot;linger.ms&quot;, 1);        props.put(&quot;buffer.memory&quot;, 33554432);        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        // 2 构建拦截链        List&lt;String&gt; interceptors = new ArrayList&lt;&gt;();        interceptors.add(&quot;com.coderf.kafka.interceptor.TimeInterceptor&quot;);     interceptors.add(&quot;com.coderf.kafka.interceptor.CounterInterceptor&quot;);         props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);        String topic = &quot;first&quot;;        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);        // 3 发送消息        for (int i = 0; i &lt; 10; i++) {            ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i);            producer.send(record);        }        // 4 一定要关闭producer，这样才会调用interceptor的close方法        producer.close();    }}</code></pre><p>3）测试</p><p>（1）在kafka上启动消费者，然后运行客户端java程序。</p><pre><code>[root@hadoop001 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop001:2181 --from-beginning --topic first1501904047034,message01501904047225,message11501904047230,message21501904047234,message31501904047236,message41501904047240,message51501904047243,message61501904047246,message71501904047249,message81501904047252,message9</code></pre><p>（2）观察java平台控制台输出数据如下</p><pre><code>Successful sent: 10Failed sent: 0</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;拦截器原理&quot;&gt;&lt;a href=&quot;#拦截器原理&quot; class=&quot;headerlink&quot; title=&quot;拦截器原理&quot;&gt;&lt;/a&gt;拦截器原理&lt;/h2&gt;&lt;p&gt;Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。&lt;/p&gt;
&lt;p&gt;对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。&lt;/p&gt;
&lt;p&gt;Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，&lt;/p&gt;
&lt;p&gt;其定义的方法包括：&lt;/p&gt;
&lt;p&gt;（1）configure(configs)&lt;/p&gt;
&lt;p&gt;获取配置信息和初始化数据时调用。&lt;/p&gt;
&lt;p&gt;（2）onSend(ProducerRecord)：&lt;/p&gt;
&lt;p&gt;该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算&lt;/p&gt;
&lt;p&gt;（3）onAcknowledgement(RecordMetadata, Exception)：&lt;/p&gt;
&lt;p&gt;该方法会在消息被应答或消息发送失败时调用，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率&lt;/p&gt;
&lt;p&gt;（4）close：&lt;/p&gt;
&lt;p&gt;关闭interceptor，主要用于执行一些资源清理工作&lt;/p&gt;
&lt;p&gt;如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。&lt;/p&gt;
    
    </summary>
    
      <category term="Kafka" scheme="http://www.coderfei.com/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="http://www.coderfei.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka：4、Kafka API</title>
    <link href="http://www.coderfei.com/2018/01/23/kafka-4-kafka-api.html"/>
    <id>http://www.coderfei.com/2018/01/23/kafka-4-kafka-api.html</id>
    <published>2018-01-23T14:52:23.000Z</published>
    <updated>2018-11-20T02:07:46.742Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kafka生产过程分析"><a href="#Kafka生产过程分析" class="headerlink" title="Kafka生产过程分析"></a>Kafka生产过程分析</h2><h3 id="写入方式"><a href="#写入方式" class="headerlink" title="写入方式"></a>写入方式</h3><p>producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。</p><h3 id="分区（Partition）"><a href="#分区（Partition）" class="headerlink" title="分区（Partition）"></a>分区（Partition）</h3><p>Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。</p><p>Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。</p><p>消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：</p><p>下图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。</p><a id="more"></a><p>我们可以看到，<font color="red">每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。</font></p><p>发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。<font color="red">原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。</font>消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。</p><p>1）    分区的原因</p><p>（1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p><p>（2）可以提高并发，因为可以以Partition为单位读写了。</p><p>传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。<font color="red">但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。</font>虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。</p><p>Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。<font color="red">Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。</font>每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。</p><p>2）分区的原则</p><p>（1）指定了patition，则直接使用</p><p>（2）未指定patition但指定key，通过对key的value进行hash出一个patition</p><p>（3）patition和key都未指定，使用轮询选出一个patition</p><p>DefaultPartitioner类</p><pre><code class="scala">public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);        int numPartitions = partitions.size();        if (keyBytes == null) {            int nextValue = nextValue(topic);            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);            if (availablePartitions.size() &gt; 0) {                int part = Utils.toPositive(nextValue) % availablePartitions.size();                return availablePartitions.get(part).partition();            } else {                // no partitions are available, give a non-available partition                return Utils.toPositive(nextValue) % numPartitions;            }        } else {            // hash the keyBytes to choose a partition            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;        }    }</code></pre><h3 id="副本（Replication）"><a href="#副本（Replication）" class="headerlink" title="副本（Replication）"></a>副本（Replication）</h3><p>同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。</p><h3 id="写入流程"><a href="#写入流程" class="headerlink" title="写入流程"></a>写入流程</h3><p>producer写入消息流程如下</p><p>1）producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader</p><p>2）producer将消息发送给该leader</p><p>3）leader将消息写入本地log</p><p>4）followers从leader pull消息，写入本地log后向leader发送ACK</p><p>5）leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK</p><h2 id="Broker保存消息"><a href="#Broker保存消息" class="headerlink" title="Broker保存消息"></a>Broker保存消息</h2><h3 id="存储方式"><a href="#存储方式" class="headerlink" title="存储方式"></a>存储方式</h3><p>物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下：</p><pre><code>[root@hadoop001 logs]$ lldrwxrwxr-x. 2 root root  4096 8月   6 14:37 first-0drwxrwxr-x. 2 root root  4096 8月   6 14:35 first-1drwxrwxr-x. 2 root root  4096 8月   6 14:37 first-2[root@hadoop001 logs]$ cd first-0[root@hadoop001 first-0]$ ll-rw-rw-r--. 1 root root 10485760 8月   6 14:33 00000000000000000000.index-rw-rw-r--. 1 root root      219 8月   6 15:07 00000000000000000000.log-rw-rw-r--. 1 root root 10485756 8月   6 14:33 00000000000000000000.timeindex-rw-rw-r--. 1 root root        8 8月   6 14:37 leader-epoch-checkpoint</code></pre><h3 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h3><p>无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：</p><p>1）基于时间：log.retention.hours=168</p><p>2）基于大小：log.retention.bytes=1073741824</p><p>需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。</p><h3 id="Zookeeper存储结构"><a href="#Zookeeper存储结构" class="headerlink" title="Zookeeper存储结构"></a>Zookeeper存储结构</h3><font color="red">注意：producer不在zk中注册，消费者在zk中注册</font><h2 id="Kafka消费过程"><a href="#Kafka消费过程" class="headerlink" title="Kafka消费过程"></a>Kafka消费过程</h2><p>kafka提供了两套consumer API：高级Consumer API和低级API。</p><h3 id="消费模型"><a href="#消费模型" class="headerlink" title="消费模型"></a>消费模型</h3><p>消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。</p><p>基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。</p><p>Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。</p><p>在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉。</p><h3 id="高级API"><a href="#高级API" class="headerlink" title="高级API"></a>高级API</h3><p>1）高级API优点</p><p>高级API 写起来简单</p><p>不需要自行去管理offset，系统通过zookeeper自行管理。</p><p>不需要管理分区，副本等情况，.系统自动管理。</p><p>消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset）</p><p>可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）</p><p>2）高级API缺点</p><p>不能自行控制offset（对于某些特殊需求来说）</p><p>不能细化控制如分区、副本、zk等</p><h3 id="低级API"><a href="#低级API" class="headerlink" title="低级API"></a>低级API</h3><p>1）低级 API 优点</p><p>能够让开发者自己控制offset，想从哪里读取就从哪里读取。</p><p>自行控制连接分区，对分区自定义进行负载均衡</p><p>对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）</p><p>2）低级API缺点</p><p>太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。</p><h3 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h3><p>消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。</p><p>在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。</p><h3 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h3><p>consumer采用pull（拉）模式从broker中读取数据。</p><p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p><p>对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p><p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。</p><h3 id="消费者组案例"><a href="#消费者组案例" class="headerlink" title="消费者组案例"></a>消费者组案例</h3><p>1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。</p><p>2）案例实操</p><p>（1）在hadoop002、hadoop003上修改/opt/software/kafka/config/consumer.properties配置文件中的group.id属性为任意组名。</p><pre><code>[root@hadoop002 config]$ vi consumer.propertiesgroup.id=coderf</code></pre><p>（2）在hadoop002、hadoop003上分别启动消费者</p><pre><code>[root@ hadoop002 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop002:2181 --topic first --consumer.config config/consumer.properties[root@ hadoop003 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop002:2181 --topic first --consumer.config config/consumer.properties</code></pre><p>（3）在hadoop001上启动生产者</p><pre><code>[root@hadoop001 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop002:9092 --topic first&gt;hello world</code></pre><p>（4）查看hadoop002和hadoop003的接收者。</p><p>同一时刻只有一个消费者接收到消息。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kafka生产过程分析&quot;&gt;&lt;a href=&quot;#Kafka生产过程分析&quot; class=&quot;headerlink&quot; title=&quot;Kafka生产过程分析&quot;&gt;&lt;/a&gt;Kafka生产过程分析&lt;/h2&gt;&lt;h3 id=&quot;写入方式&quot;&gt;&lt;a href=&quot;#写入方式&quot; class=&quot;headerlink&quot; title=&quot;写入方式&quot;&gt;&lt;/a&gt;写入方式&lt;/h3&gt;&lt;p&gt;producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。&lt;/p&gt;
&lt;h3 id=&quot;分区（Partition）&quot;&gt;&lt;a href=&quot;#分区（Partition）&quot; class=&quot;headerlink&quot; title=&quot;分区（Partition）&quot;&gt;&lt;/a&gt;分区（Partition）&lt;/h3&gt;&lt;p&gt;Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。&lt;/p&gt;
&lt;p&gt;Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。&lt;/p&gt;
&lt;p&gt;消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：&lt;/p&gt;
&lt;p&gt;下图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。&lt;/p&gt;
    
    </summary>
    
      <category term="Kafka" scheme="http://www.coderfei.com/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="http://www.coderfei.com/tags/Kafka/"/>
    
  </entry>
  
</feed>
